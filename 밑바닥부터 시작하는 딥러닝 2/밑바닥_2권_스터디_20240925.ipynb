{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoKp/ptgcpmu6SzxN5uXPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratel0307/Deep-Learning/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5_2%EA%B6%8C_%EC%8A%A4%ED%84%B0%EB%94%94_20240925.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5장: 순환 신경망(RNN)"
      ],
      "metadata": {
        "id": "P4Ws89M56UH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 확률과 언어 모델"
      ],
      "metadata": {
        "id": "NgXcB193O2sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 언어 모델(Language Model): 특정한 단어의 시퀀스에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지(얼마나 자연스러운 단어 순서인지)를 확률로 평가하는 것; 즉 단어 나열에 확률을 부여\n",
        "- CBOW 모델의 한계: 맥락 안의 단어 순서가 무시됨 -> RNN의 등장"
      ],
      "metadata": {
        "id": "rlPo2f2HO8jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 RNN이란"
      ],
      "metadata": {
        "id": "IgOs6ufnQcJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BPTT(Backpropagation Through Time): 시간 방향으로 펼친 신경망의 오차역전파법\n",
        "- Truncated BPTT: 너무 길어진 신경망을 작은 신경망으로 잘라내서 오차역전파법을 수행\n",
        "  - 데이터를 순서대로 제공해야 한다\n",
        "  - 미니배치별로 데이터를 제공하는 시작 위치를 옮겨야 한다"
      ],
      "metadata": {
        "id": "ll3APAu-RUZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 RNN 구현"
      ],
      "metadata": {
        "id": "egyRe7P1SWze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "    h_next = np.tanh(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next\n",
        "\n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params\n",
        "    x, h_prev, h_next = self.cache\n",
        "\n",
        "    dt = dh_next * (1 - h_next ** 2)\n",
        "    db = np.sum(dt, axis=0)\n",
        "    dWh = np.dot(h_prev.T, dt)\n",
        "    dh_prev = np.dot(dt, Wh.T)\n",
        "    dWx = np.dot(x.T, dt)\n",
        "    dx = np.dot(dt, Wx.T)\n",
        "\n",
        "    self.grads[0][...] = dWx\n",
        "    self.grads[1][...] = dWh\n",
        "    self.grads[2][...] = db\n",
        "\n",
        "    return dx, dh_prev"
      ],
      "metadata": {
        "id": "kN1c6PKiO2ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeRNN:\n",
        "  def __init__(self, Wx, Wh, b, stateful=False):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.layers = None\n",
        "\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful\n",
        "\n",
        "  def set_state(self, h):\n",
        "    self.h = h\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.h = None\n",
        "\n",
        "  def forward(self, xs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, D = xs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    self.layers = []\n",
        "    hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "    if not self.stateful or self.h is None:\n",
        "      self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = RNN(*self.params)\n",
        "      self.h = layer.forward(xs[:, t, :], self.h)\n",
        "      hs[:, t, :] = self.h\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    return hs"
      ],
      "metadata": {
        "id": "SoKiJqz36TqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 시계열 데이터 처리 계층 구현\n",
        "- RNNLM: RNN을 사용한 언어 모델"
      ],
      "metadata": {
        "id": "3bHhZnkSV_dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPU = False"
      ],
      "metadata": {
        "id": "JByfNnEoXbRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "KvJmTPbiXgCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJGQvtSH6QPM"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None  # softmax의 출력\n",
        "        self.t = None  # 정답 레이블\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.loss = None\n",
        "        self.y = None  # sigmoid의 출력\n",
        "        self.t = None  # 정답 데이터\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "\n",
        "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = (self.y - self.t) * dout / batch_size\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    '''\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    '''\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
        "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
        "        b: 편향（4개분의 편향이 담겨 있음）\n",
        "        '''\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, H = h_prev.shape\n",
        "\n",
        "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
        "\n",
        "        f = A[:, :H]\n",
        "        g = A[:, H:2*H]\n",
        "        i = A[:, 2*H:3*H]\n",
        "        o = A[:, 3*H:]\n",
        "\n",
        "        f = sigmoid(f)\n",
        "        g = np.tanh(g)\n",
        "        i = sigmoid(i)\n",
        "        o = sigmoid(o)\n",
        "\n",
        "        c_next = f * c_prev + g * i\n",
        "        h_next = o * np.tanh(c_next)\n",
        "\n",
        "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def backward(self, dh_next, dc_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
        "\n",
        "        tanh_c_next = np.tanh(c_next)\n",
        "\n",
        "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
        "\n",
        "        dc_prev = ds * f\n",
        "\n",
        "        di = ds * g\n",
        "        df = ds * c_prev\n",
        "        do = dh_next * tanh_c_next\n",
        "        dg = ds * i\n",
        "\n",
        "        di *= i * (1 - i)\n",
        "        df *= f * (1 - f)\n",
        "        do *= o * (1 - o)\n",
        "        dg *= (1 - g ** 2)\n",
        "\n",
        "        dA = np.hstack((df, dg, di, do))\n",
        "\n",
        "        dWh = np.dot(h_prev.T, dA)\n",
        "        dWx = np.dot(x.T, dA)\n",
        "        db = dA.sum(axis=0)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        dx = np.dot(dA, Wx.T)\n",
        "        dh_prev = np.dot(dA, Wh.T)\n",
        "\n",
        "        return dx, dh_prev, dc_prev\n",
        "\n",
        "\n",
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None\n",
        "\n",
        "\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.layers = None\n",
        "        self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T = xs.shape\n",
        "        V, D = self.W.shape\n",
        "\n",
        "        out = np.empty((N, T, D), dtype='f')\n",
        "        self.layers = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Embedding(self.W)\n",
        "            out[:, t, :] = layer.forward(xs[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "\n",
        "        grad = 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(dout[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "\n",
        "\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        rx = x.reshape(N*T, -1)\n",
        "        out = np.dot(rx, W) + b\n",
        "        self.x = x\n",
        "        return out.reshape(N, T, -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.x\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        dout = dout.reshape(N*T, -1)\n",
        "        rx = x.reshape(N*T, -1)\n",
        "\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dW = np.dot(rx.T, dout)\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dx = dx.reshape(*x.shape)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "        self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "\n",
        "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
        "            ts = ts.argmax(axis=2)\n",
        "\n",
        "        mask = (ts != self.ignore_label)\n",
        "\n",
        "        # 배치용과 시계열용을 정리(reshape)\n",
        "        xs = xs.reshape(N * T, V)\n",
        "        ts = ts.reshape(N * T)\n",
        "        mask = mask.reshape(N * T)\n",
        "\n",
        "        ys = softmax(xs)\n",
        "        ls = np.log(ys[np.arange(N * T), ts])\n",
        "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
        "        loss = -np.sum(ls)\n",
        "        loss /= mask.sum()\n",
        "\n",
        "        self.cache = (ts, ys, mask, (N, T, V))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "        dx = ys\n",
        "        dx[np.arange(N * T), ts] -= 1\n",
        "        dx *= dout\n",
        "        dx /= mask.sum()\n",
        "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
        "\n",
        "        dx = dx.reshape((N, T, V))\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeDropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "        self.train_flg = True\n",
        "\n",
        "    def forward(self, xs):\n",
        "        if self.train_flg:\n",
        "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
        "            scale = 1 / (1.0 - self.dropout_ratio)\n",
        "            self.mask = flg.astype(np.float32) * scale\n",
        "\n",
        "            return xs * self.mask\n",
        "        else:\n",
        "            return xs\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class TimeBiLSTM:\n",
        "    def __init__(self, Wx1, Wh1, b1,\n",
        "                 Wx2, Wh2, b2, stateful=False):\n",
        "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
        "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
        "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
        "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
        "\n",
        "    def forward(self, xs):\n",
        "        o1 = self.forward_lstm.forward(xs)\n",
        "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
        "        o2 = o2[:, ::-1]\n",
        "\n",
        "        out = np.concatenate((o1, o2), axis=2)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        H = dhs.shape[2] // 2\n",
        "        do1 = dhs[:, :, :H]\n",
        "        do2 = dhs[:, :, H:]\n",
        "\n",
        "        dxs1 = self.forward_lstm.backward(do1)\n",
        "        do2 = do2[:, ::-1]\n",
        "        dxs2 = self.backward_lstm.backward(do2)\n",
        "        dxs2 = dxs2[:, ::-1]\n",
        "        dxs = dxs1 + dxs2\n",
        "        return dxs\n",
        "\n",
        "# ====================================================================== #\n",
        "# 이 아래의 계층들은 책에서 설명하지 않았거나\n",
        "# 처리 속도보다는 쉽게 이해할 수 있도록 구현했습니다.\n",
        "#\n",
        "# TimeSigmoidWithLoss: 시계열 데이터용 시그모이드 + 손실 계층\n",
        "# GRU: GRU 계층\n",
        "# TimeGRU: 시계열 데이터용 GRU 계층\n",
        "# BiTimeLSTM: 양방향 LSTM 계층\n",
        "# Simple_TimeSoftmaxWithLoss：간단한 TimeSoftmaxWithLoss 계층의 구현\n",
        "# Simple_TimeAffine: 간단한 TimeAffine 계층의 구현\n",
        "# ====================================================================== #\n",
        "\n",
        "\n",
        "class TimeSigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.xs_shape = None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T = xs.shape\n",
        "        self.xs_shape = xs.shape\n",
        "\n",
        "        self.layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SigmoidWithLoss()\n",
        "            loss += layer.forward(xs[:, t], ts[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return loss / T\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        N, T = self.xs_shape\n",
        "        dxs = np.empty(self.xs_shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class GRU:\n",
        "    def __init__(self, Wx, Wh):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: 입력 x에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
        "        Wh: 은닉 상태 h에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
        "        '''\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        self.dWx, self.dWh = None, None\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "\n",
        "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
        "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
        "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
        "        h_next = (1-z) * h_prev + z * h_hat\n",
        "\n",
        "        self.cache = (x, h_prev, z, r, h_hat)\n",
        "\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "        x, h_prev, z, r, h_hat = self.cache\n",
        "\n",
        "        dh_hat =dh_next * z\n",
        "        dh_prev = dh_next * (1-z)\n",
        "\n",
        "        # tanh\n",
        "        dt = dh_hat * (1 - h_hat ** 2)\n",
        "        dWh = np.dot((r * h_prev).T, dt)\n",
        "        dhr = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "        dh_prev += r * dhr\n",
        "\n",
        "        # update gate(z)\n",
        "        dz = dh_next * h_hat - dh_next * h_prev\n",
        "        dt = dz * z * (1-z)\n",
        "        dWhz = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whz.T)\n",
        "        dWxz = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxz.T)\n",
        "\n",
        "        # rest gate(r)\n",
        "        dr = dhr * h_prev\n",
        "        dt = dr * r * (1-r)\n",
        "        dWhr = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whr.T)\n",
        "        dWxr = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxr.T)\n",
        "\n",
        "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
        "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeGRU:\n",
        "    def __init__(self, Wx, Wh, stateful=False):\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        selfdWx, self.dWh = None, None\n",
        "        self.layers = None\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        H, H3 = self.Wh.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = GRU(self.Wx, self.Wh)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        N, T, H = dhs.shape\n",
        "        D = self.Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dWx, self.dWh = 0, 0\n",
        "\n",
        "        dh = 0\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "\n",
        "            dxs[:, t, :] = dx\n",
        "            self.dWx += layer.dWx\n",
        "            self.dWh += layer.dWh\n",
        "\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class Simple_TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "        layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SoftmaxWithLoss()\n",
        "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
        "            layers.append(layer)\n",
        "        loss /= T\n",
        "\n",
        "        self.cache = (layers, xs)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        layers, xs = self.cache\n",
        "        N, T, V = xs.shape\n",
        "        dxs = np.empty(xs.shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class Simple_TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W, self.b = W, b\n",
        "        self.dW, self.db = None, None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        self.layers = []\n",
        "        out = np.empty((N, T, M), dtype='f')\n",
        "        for t in range(T):\n",
        "            layer = Affine(self.W, self.b)\n",
        "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, M = dout.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dW, self.db = 0, 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
        "\n",
        "            self.dW += layer.dW\n",
        "            self.db += layer.db\n",
        "\n",
        "        return dxs"
      ],
      "metadata": {
        "id": "-qb7jb2-Xowt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()"
      ],
      "metadata": {
        "id": "15wttPgEX5tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- perplexity: 언어 모델의 예측 성능을 평가하는 척도\n",
        "  - 확률의 역수\n",
        "  - 작을수록 좋다\n",
        "  - 분기수: 다음에 취할 수 있는 선택사항의 수"
      ],
      "metadata": {
        "id": "w3qMnKdmYQLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    '''\n",
        "    확률적 경사하강법(Stochastic Gradient Descent)\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "    '''\n",
        "    모멘텀 SGG(Momentum SGD)\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
        "            params[i] += self.v[i]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "    '''\n",
        "    네스테로프 가속 경사(NAG; Nesterov's Accelerated Gradient) (http://arxiv.org/abs/1212.0901)\n",
        "    '네스테로프 모멘텀 최적화'라고도 한다.\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] *= self.momentum\n",
        "            self.v[i] -= self.lr * grads[i]\n",
        "            params[i] += self.momentum * self.momentum * self.v[i]\n",
        "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    '''\n",
        "    AdaGrad\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] += grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "    '''\n",
        "    RMSprop\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] *= self.decay_rate\n",
        "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "\n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
      ],
      "metadata": {
        "id": "K3w1eHs2ZkVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "import os\n",
        "import urllib.request\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# 코랩에서 작업 디렉토리로 설정\n",
        "dataset_dir = os.path.join(os.getcwd(), 'ptb_dataset')\n",
        "os.makedirs(dataset_dir, exist_ok=True)  # 데이터셋 디렉토리 생성\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train': 'ptb.train.txt',\n",
        "    'test': 'ptb.test.txt',\n",
        "    'valid': 'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train': 'ptb.train.npy',\n",
        "    'test': 'ptb.test.npy',\n",
        "    'valid': 'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = os.path.join(dataset_dir, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = os.path.join(dataset_dir, vocab_file)\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = os.path.join(dataset_dir, file_name)\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def load_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: 데이터 유형: 'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val':\n",
        "        data_type = 'valid'\n",
        "    save_path = os.path.join(dataset_dir, save_file[data_type])\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = os.path.join(dataset_dir, file_name)\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for data_type in ('train', 'val', 'test'):\n",
        "        load_data(data_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTPuudfmcC-p",
        "outputId": "7f08bca0-7acc-4170-a2d7-8d69dc007128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "Downloading ptb.valid.txt ... \n",
            "Done\n",
            "Downloading ptb.test.txt ... \n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNLM의 학습 코드\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수\n",
        "time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습 데이터 읽기(전체 중 1000개만)\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:-1]  # 입력\n",
        "ts = corpus[1:]   # 출력(정답 레이블)\n",
        "data_size = len(xs)\n",
        "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
        "\n",
        "# 학습 시 사용하는 변수\n",
        "max_iters = data_size // (batch_size * time_size)\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []\n",
        "\n",
        "# 모델 생성 (SimpleRnnlm과 SGD 정의 필요)\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
        "jump = (corpus_size - 1) // batch_size\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "# 학습 루프 시작\n",
        "for epoch in range(max_epoch):\n",
        "    for iter in range(max_iters):\n",
        "        # 미니배치 취득\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "            time_idx += 1\n",
        "\n",
        "        # 기울기를 구하여 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "        total_loss += loss  # placeholder\n",
        "        loss_count += 1\n",
        "\n",
        "    # 에폭마다 퍼플렉서티 평가\n",
        "    ppl = np.exp(total_loss / loss_count)\n",
        "    print('| 에폭 %d | 퍼플렉서티 %.2f'\n",
        "          % (epoch+1, ppl))\n",
        "    ppl_list.append(float(ppl))\n",
        "    total_loss, loss_count = 0, 0\n",
        "\n",
        "# 그래프 그리기\n",
        "x = np.arange(len(ppl_list))\n",
        "plt.plot(x, ppl_list, label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('perplexity')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W7zJ46HjcyvC",
        "outputId": "059d7fd4-b6e7-4c1e-bcd6-b6b69cd8bc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "말뭉치 크기: 1000, 어휘 수: 418\n",
            "| 에폭 1 | 퍼플렉서티 394.70\n",
            "| 에폭 2 | 퍼플렉서티 270.96\n",
            "| 에폭 3 | 퍼플렉서티 227.44\n",
            "| 에폭 4 | 퍼플렉서티 217.78\n",
            "| 에폭 5 | 퍼플렉서티 206.14\n",
            "| 에폭 6 | 퍼플렉서티 202.73\n",
            "| 에폭 7 | 퍼플렉서티 200.00\n",
            "| 에폭 8 | 퍼플렉서티 196.42\n",
            "| 에폭 9 | 퍼플렉서티 190.13\n",
            "| 에폭 10 | 퍼플렉서티 191.89\n",
            "| 에폭 11 | 퍼플렉서티 187.36\n",
            "| 에폭 12 | 퍼플렉서티 192.41\n",
            "| 에폭 13 | 퍼플렉서티 189.16\n",
            "| 에폭 14 | 퍼플렉서티 189.05\n",
            "| 에폭 15 | 퍼플렉서티 187.92\n",
            "| 에폭 16 | 퍼플렉서티 184.63\n",
            "| 에폭 17 | 퍼플렉서티 182.49\n",
            "| 에폭 18 | 퍼플렉서티 180.28\n",
            "| 에폭 19 | 퍼플렉서티 181.35\n",
            "| 에폭 20 | 퍼플렉서티 184.35\n",
            "| 에폭 21 | 퍼플렉서티 178.85\n",
            "| 에폭 22 | 퍼플렉서티 176.74\n",
            "| 에폭 23 | 퍼플렉서티 173.92\n",
            "| 에폭 24 | 퍼플렉서티 176.83\n",
            "| 에폭 25 | 퍼플렉서티 173.26\n",
            "| 에폭 26 | 퍼플렉서티 172.94\n",
            "| 에폭 27 | 퍼플렉서티 165.47\n",
            "| 에폭 28 | 퍼플렉서티 165.31\n",
            "| 에폭 29 | 퍼플렉서티 160.56\n",
            "| 에폭 30 | 퍼플렉서티 157.70\n",
            "| 에폭 31 | 퍼플렉서티 158.01\n",
            "| 에폭 32 | 퍼플렉서티 151.90\n",
            "| 에폭 33 | 퍼플렉서티 153.01\n",
            "| 에폭 34 | 퍼플렉서티 148.85\n",
            "| 에폭 35 | 퍼플렉서티 149.39\n",
            "| 에폭 36 | 퍼플렉서티 140.34\n",
            "| 에폭 37 | 퍼플렉서티 139.01\n",
            "| 에폭 38 | 퍼플렉서티 134.85\n",
            "| 에폭 39 | 퍼플렉서티 129.20\n",
            "| 에폭 40 | 퍼플렉서티 123.92\n",
            "| 에폭 41 | 퍼플렉서티 124.45\n",
            "| 에폭 42 | 퍼플렉서티 117.22\n",
            "| 에폭 43 | 퍼플렉서티 114.19\n",
            "| 에폭 44 | 퍼플렉서티 109.25\n",
            "| 에폭 45 | 퍼플렉서티 103.94\n",
            "| 에폭 46 | 퍼플렉서티 103.65\n",
            "| 에폭 47 | 퍼플렉서티 97.99\n",
            "| 에폭 48 | 퍼플렉서티 92.34\n",
            "| 에폭 49 | 퍼플렉서티 89.07\n",
            "| 에폭 50 | 퍼플렉서티 86.88\n",
            "| 에폭 51 | 퍼플렉서티 81.64\n",
            "| 에폭 52 | 퍼플렉서티 77.90\n",
            "| 에폭 53 | 퍼플렉서티 73.63\n",
            "| 에폭 54 | 퍼플렉서티 70.99\n",
            "| 에폭 55 | 퍼플렉서티 67.69\n",
            "| 에폭 56 | 퍼플렉서티 61.98\n",
            "| 에폭 57 | 퍼플렉서티 60.75\n",
            "| 에폭 58 | 퍼플렉서티 56.62\n",
            "| 에폭 59 | 퍼플렉서티 52.34\n",
            "| 에폭 60 | 퍼플렉서티 49.98\n",
            "| 에폭 61 | 퍼플렉서티 48.06\n",
            "| 에폭 62 | 퍼플렉서티 45.37\n",
            "| 에폭 63 | 퍼플렉서티 42.04\n",
            "| 에폭 64 | 퍼플렉서티 39.87\n",
            "| 에폭 65 | 퍼플렉서티 38.81\n",
            "| 에폭 66 | 퍼플렉서티 35.69\n",
            "| 에폭 67 | 퍼플렉서티 34.42\n",
            "| 에폭 68 | 퍼플렉서티 30.65\n",
            "| 에폭 69 | 퍼플렉서티 30.30\n",
            "| 에폭 70 | 퍼플렉서티 28.43\n",
            "| 에폭 71 | 퍼플렉서티 26.99\n",
            "| 에폭 72 | 퍼플렉서티 25.28\n",
            "| 에폭 73 | 퍼플렉서티 23.38\n",
            "| 에폭 74 | 퍼플렉서티 22.29\n",
            "| 에폭 75 | 퍼플렉서티 21.36\n",
            "| 에폭 76 | 퍼플렉서티 19.62\n",
            "| 에폭 77 | 퍼플렉서티 18.80\n",
            "| 에폭 78 | 퍼플렉서티 18.29\n",
            "| 에폭 79 | 퍼플렉서티 17.13\n",
            "| 에폭 80 | 퍼플렉서티 16.19\n",
            "| 에폭 81 | 퍼플렉서티 14.86\n",
            "| 에폭 82 | 퍼플렉서티 14.51\n",
            "| 에폭 83 | 퍼플렉서티 13.09\n",
            "| 에폭 84 | 퍼플렉서티 13.08\n",
            "| 에폭 85 | 퍼플렉서티 11.90\n",
            "| 에폭 86 | 퍼플렉서티 11.40\n",
            "| 에폭 87 | 퍼플렉서티 11.20\n",
            "| 에폭 88 | 퍼플렉서티 10.92\n",
            "| 에폭 89 | 퍼플렉서티 10.39\n",
            "| 에폭 90 | 퍼플렉서티 9.14\n",
            "| 에폭 91 | 퍼플렉서티 8.86\n",
            "| 에폭 92 | 퍼플렉서티 8.57\n",
            "| 에폭 93 | 퍼플렉서티 8.11\n",
            "| 에폭 94 | 퍼플렉서티 7.67\n",
            "| 에폭 95 | 퍼플렉서티 7.22\n",
            "| 에폭 96 | 퍼플렉서티 6.75\n",
            "| 에폭 97 | 퍼플렉서티 6.43\n",
            "| 에폭 98 | 퍼플렉서티 6.04\n",
            "| 에폭 99 | 퍼플렉서티 5.88\n",
            "| 에폭 100 | 퍼플렉서티 5.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBdI3pVDda1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
