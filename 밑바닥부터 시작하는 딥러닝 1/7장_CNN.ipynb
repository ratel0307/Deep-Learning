{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18fSUHmGa74yM_OprmIkUsh1etFzy7Pkv",
      "authorship_tag": "ABX9TyPbNVMpk/+ZURObwRCGf0j6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratel0307/Deep-Learning/blob/main/7%EC%9E%A5_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.4 합성곱/풀링 계층 구현하기"
      ],
      "metadata": {
        "id": "HxCefIo_ZuNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4IjfVXysZltm"
      },
      "outputs": [],
      "source": [
        "# 7.4.1 4차원 배열"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0rBQEShZ0LS",
        "outputId": "60936fc4-f57b-4cc2-dcbc-08e994685b28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].shape\n",
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5dY8mLJ0Nq6",
        "outputId": "b4ffb7b7-b67d-492f-8225-40852c2d8626"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iucAK3v0Sc2",
        "outputId": "1e50259b-eed0-42ff-90a9-a72748f0dd16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04975246, 0.24032338, 0.64817774, 0.89608315, 0.49709807,\n",
              "        0.303584  , 0.43530586, 0.56798937, 0.22049847, 0.45295315,\n",
              "        0.79733347, 0.41535515, 0.3995392 , 0.83958985, 0.84453896,\n",
              "        0.08865211, 0.93570724, 0.51302994, 0.52811991, 0.28399491,\n",
              "        0.58595655, 0.82944581, 0.2088317 , 0.50973843, 0.31381656,\n",
              "        0.16337765, 0.23321871, 0.81496387],\n",
              "       [0.37529646, 0.17186967, 0.10647343, 0.90613573, 0.77425376,\n",
              "        0.28294601, 0.88595742, 0.6050474 , 0.26056978, 0.38473828,\n",
              "        0.93266977, 0.65087436, 0.92126326, 0.57796149, 0.41716072,\n",
              "        0.81876092, 0.3166147 , 0.2982416 , 0.31247145, 0.95570248,\n",
              "        0.70983001, 0.52103809, 0.62803033, 0.31628769, 0.05043231,\n",
              "        0.52416899, 0.40107955, 0.19038348],\n",
              "       [0.07538759, 0.9963049 , 0.22076633, 0.52035609, 0.73397386,\n",
              "        0.07874782, 0.2787149 , 0.24702323, 0.84432766, 0.5153932 ,\n",
              "        0.39860965, 0.13965456, 0.81112117, 0.41920173, 0.25820483,\n",
              "        0.45796246, 0.82719129, 0.86798682, 0.8030521 , 0.18203196,\n",
              "        0.67253463, 0.30429759, 0.62113738, 0.99943145, 0.93975202,\n",
              "        0.74660193, 0.78273501, 0.9449614 ],\n",
              "       [0.51922561, 0.41590929, 0.04213221, 0.46389699, 0.74761577,\n",
              "        0.2767111 , 0.70882318, 0.60006344, 0.49584125, 0.01933053,\n",
              "        0.21724471, 0.36523899, 0.74065145, 0.93214638, 0.15762501,\n",
              "        0.38485001, 0.41153376, 0.17813797, 0.32436672, 0.85517666,\n",
              "        0.86459824, 0.13983057, 0.22074208, 0.87563855, 0.77824369,\n",
              "        0.36279962, 0.81139675, 0.87121545],\n",
              "       [0.51840908, 0.55745176, 0.4775347 , 0.29944349, 0.92176857,\n",
              "        0.36956741, 0.02242865, 0.26808993, 0.20318569, 0.83028349,\n",
              "        0.23354577, 0.24190636, 0.48362118, 0.1101407 , 0.6729821 ,\n",
              "        0.88642319, 0.82068462, 0.64923377, 0.43986233, 0.89162475,\n",
              "        0.49827551, 0.8664273 , 0.5316046 , 0.2926906 , 0.36986971,\n",
              "        0.95340247, 0.33588577, 0.47160418],\n",
              "       [0.15012722, 0.57144298, 0.81997726, 0.28890318, 0.40847753,\n",
              "        0.4578617 , 0.2897075 , 0.63634949, 0.64769579, 0.11834433,\n",
              "        0.02822135, 0.05144117, 0.82148722, 0.52244658, 0.10526012,\n",
              "        0.11170653, 0.96481085, 0.69598778, 0.62373197, 0.43460928,\n",
              "        0.00737988, 0.76458855, 0.00848485, 0.4202913 , 0.15694496,\n",
              "        0.09709512, 0.98756797, 0.25288752],\n",
              "       [0.13787815, 0.39190921, 0.04614188, 0.489647  , 0.6914438 ,\n",
              "        0.37707836, 0.58348673, 0.81155423, 0.48705967, 0.83538449,\n",
              "        0.24983972, 0.87987582, 0.58896286, 0.66634635, 0.53276261,\n",
              "        0.88614655, 0.17058418, 0.63056859, 0.16142837, 0.2668163 ,\n",
              "        0.6568976 , 0.5963823 , 0.48329644, 0.65837515, 0.95517132,\n",
              "        0.11107344, 0.1668597 , 0.8513749 ],\n",
              "       [0.00207485, 0.15660651, 0.05151601, 0.87826988, 0.29350628,\n",
              "        0.65613952, 0.52907445, 0.97368888, 0.36068877, 0.0525544 ,\n",
              "        0.39436853, 0.46724576, 0.80137926, 0.85270788, 0.50570755,\n",
              "        0.51537236, 0.83247618, 0.95078072, 0.61860397, 0.0417879 ,\n",
              "        0.49327944, 0.8473484 , 0.01828588, 0.08483991, 0.02593289,\n",
              "        0.1958062 , 0.24218851, 0.49286139],\n",
              "       [0.41416988, 0.40117124, 0.67315039, 0.69155967, 0.691581  ,\n",
              "        0.43196372, 0.7802077 , 0.96352785, 0.73483151, 0.62034355,\n",
              "        0.35552259, 0.9589675 , 0.55861614, 0.21237799, 0.09544217,\n",
              "        0.72349549, 0.82335974, 0.75673111, 0.02094374, 0.50376714,\n",
              "        0.68845641, 0.96458877, 0.01086224, 0.89741355, 0.02593022,\n",
              "        0.8963233 , 0.17181912, 0.99863792],\n",
              "       [0.85990116, 0.82562921, 0.73454483, 0.61432906, 0.89478087,\n",
              "        0.30911102, 0.64161276, 0.9841935 , 0.85773163, 0.24135418,\n",
              "        0.75186112, 0.58607595, 0.04027882, 0.85342551, 0.58512261,\n",
              "        0.24770835, 0.45373814, 0.61945478, 0.3176589 , 0.40092944,\n",
              "        0.18279423, 0.21600074, 0.3236167 , 0.34793056, 0.5556907 ,\n",
              "        0.48708687, 0.40317092, 0.38260463],\n",
              "       [0.02748896, 0.08738913, 0.27107434, 0.11590351, 0.8954049 ,\n",
              "        0.0034298 , 0.78884749, 0.92569285, 0.08736289, 0.91889352,\n",
              "        0.87581269, 0.21051823, 0.75106076, 0.75551847, 0.62436435,\n",
              "        0.82077753, 0.45621884, 0.32875538, 0.03058463, 0.69008799,\n",
              "        0.92839379, 0.53307167, 0.97403959, 0.67289581, 0.53852878,\n",
              "        0.99118938, 0.40573051, 0.51711236],\n",
              "       [0.36231965, 0.95482307, 0.05038421, 0.74410239, 0.85431802,\n",
              "        0.46500426, 0.04507719, 0.81146747, 0.44285714, 0.94836286,\n",
              "        0.92421108, 0.97075129, 0.38901182, 0.90596418, 0.65910237,\n",
              "        0.39119761, 0.35988726, 0.97090817, 0.21246121, 0.42373678,\n",
              "        0.5199073 , 0.75836888, 0.22764937, 0.16423816, 0.46319587,\n",
              "        0.78645508, 0.3604804 , 0.6119173 ],\n",
              "       [0.72339928, 0.85942494, 0.54992758, 0.21242367, 0.75382832,\n",
              "        0.53907515, 0.0411273 , 0.97680634, 0.43637563, 0.39472082,\n",
              "        0.91968875, 0.44180066, 0.88081597, 0.079302  , 0.8943781 ,\n",
              "        0.6153988 , 0.45152628, 0.65597409, 0.67805425, 0.98001251,\n",
              "        0.89429459, 0.01505737, 0.01059301, 0.80320539, 0.5313757 ,\n",
              "        0.92163458, 0.61634117, 0.62895149],\n",
              "       [0.68687336, 0.38764343, 0.73410989, 0.22463362, 0.26385487,\n",
              "        0.1086045 , 0.11488674, 0.70251207, 0.05153578, 0.55128031,\n",
              "        0.68323176, 0.30862036, 0.20288795, 0.07208842, 0.53046734,\n",
              "        0.67946642, 0.59999592, 0.35257223, 0.59388291, 0.04397237,\n",
              "        0.42022043, 0.43130744, 0.63891856, 0.61342089, 0.28395532,\n",
              "        0.28091613, 0.62047745, 0.70492306],\n",
              "       [0.95973028, 0.71554003, 0.21241541, 0.34538395, 0.48481038,\n",
              "        0.93111669, 0.88591555, 0.6229495 , 0.70787395, 0.7937637 ,\n",
              "        0.78064654, 0.90263949, 0.1122704 , 0.98157465, 0.0168491 ,\n",
              "        0.34805195, 0.26867806, 0.87336558, 0.65941987, 0.7189971 ,\n",
              "        0.87680536, 0.76602205, 0.46957986, 0.79554445, 0.73783399,\n",
              "        0.49822943, 0.96170415, 0.49327563],\n",
              "       [0.21181961, 0.42163466, 0.83530376, 0.9009292 , 0.76773624,\n",
              "        0.82158442, 0.55926933, 0.48553768, 0.91349806, 0.64388302,\n",
              "        0.45343155, 0.38577486, 0.61148135, 0.66280438, 0.51391609,\n",
              "        0.95056721, 0.41643308, 0.25368083, 0.32406912, 0.52241646,\n",
              "        0.62215664, 0.9080482 , 0.30298232, 0.23754016, 0.14116522,\n",
              "        0.7990545 , 0.16567299, 0.93756524],\n",
              "       [0.69383085, 0.85726325, 0.4001882 , 0.42461988, 0.56960781,\n",
              "        0.67617922, 0.10989173, 0.24344747, 0.23318671, 0.08792668,\n",
              "        0.88505784, 0.94187027, 0.82016908, 0.83341897, 0.87486103,\n",
              "        0.8150079 , 0.86575461, 0.58374765, 0.42718177, 0.03785896,\n",
              "        0.23335341, 0.92899548, 0.02398219, 0.72285796, 0.77678216,\n",
              "        0.18856604, 0.99243899, 0.14916623],\n",
              "       [0.6583635 , 0.84884941, 0.06363287, 0.24718361, 0.38467487,\n",
              "        0.19581205, 0.45047097, 0.26582306, 0.40335918, 0.73690694,\n",
              "        0.51161445, 0.17328714, 0.8122732 , 0.8232956 , 0.90122615,\n",
              "        0.4701717 , 0.59277931, 0.32277571, 0.43567808, 0.27376328,\n",
              "        0.37651164, 0.78950777, 0.12097851, 0.02576041, 0.15759311,\n",
              "        0.02337742, 0.20433763, 0.1586436 ],\n",
              "       [0.67235001, 0.06586278, 0.17649412, 0.1976917 , 0.60183835,\n",
              "        0.01932334, 0.90476129, 0.18025553, 0.63527025, 0.14107243,\n",
              "        0.72878524, 0.57830713, 0.99220702, 0.37344973, 0.20611479,\n",
              "        0.4398023 , 0.98499253, 0.75012101, 0.76999621, 0.09821661,\n",
              "        0.99373534, 0.72940621, 0.28319339, 0.3604772 , 0.15636204,\n",
              "        0.69266127, 0.7540884 , 0.12309503],\n",
              "       [0.6601054 , 0.67167562, 0.57477935, 0.19570349, 0.14573748,\n",
              "        0.66418597, 0.52153876, 0.66231363, 0.44219224, 0.46777392,\n",
              "        0.90732178, 0.9018443 , 0.68449659, 0.41964868, 0.30904613,\n",
              "        0.71649537, 0.0076225 , 0.74238105, 0.83197   , 0.58160485,\n",
              "        0.54703063, 0.27824571, 0.52868794, 0.17698381, 0.22711651,\n",
              "        0.94890604, 0.22878184, 0.63616883],\n",
              "       [0.31628547, 0.10104094, 0.17627896, 0.29397039, 0.34548241,\n",
              "        0.82593546, 0.38991312, 0.25259644, 0.63058742, 0.01513528,\n",
              "        0.52732672, 0.92502858, 0.49239819, 0.8269436 , 0.5148529 ,\n",
              "        0.43627796, 0.61075116, 0.4693523 , 0.38346765, 0.32323872,\n",
              "        0.33980883, 0.30344245, 0.29747791, 0.82437398, 0.08060309,\n",
              "        0.24521925, 0.18539891, 0.03504693],\n",
              "       [0.11336794, 0.3256122 , 0.56139058, 0.7370328 , 0.31261715,\n",
              "        0.44826477, 0.44748933, 0.46486824, 0.40705549, 0.26700945,\n",
              "        0.93149036, 0.70219381, 0.04313937, 0.98508998, 0.03093541,\n",
              "        0.84864241, 0.4642682 , 0.75356538, 0.60316459, 0.35187185,\n",
              "        0.1983266 , 0.53242556, 0.46978736, 0.99327169, 0.79695858,\n",
              "        0.79587994, 0.73942984, 0.10924453],\n",
              "       [0.86473291, 0.91994685, 0.84760501, 0.16879145, 0.3354534 ,\n",
              "        0.72700639, 0.19465072, 0.92118484, 0.24623867, 0.165123  ,\n",
              "        0.97322386, 0.59244697, 0.42076901, 0.46054099, 0.37277928,\n",
              "        0.66027553, 0.80213574, 0.64062236, 0.75322677, 0.95976137,\n",
              "        0.06856189, 0.23100177, 0.86803117, 0.4044079 , 0.96669347,\n",
              "        0.81711024, 0.98358377, 0.51311381],\n",
              "       [0.06206582, 0.22444445, 0.37966245, 0.41379241, 0.39822657,\n",
              "        0.4281662 , 0.68437254, 0.61398084, 0.89706339, 0.51841657,\n",
              "        0.503074  , 0.82869453, 0.23017395, 0.54095772, 0.15236544,\n",
              "        0.82401902, 0.01332784, 0.86563673, 0.52904762, 0.64646265,\n",
              "        0.74804712, 0.12435828, 0.29401386, 0.04260972, 0.71201313,\n",
              "        0.58154856, 0.13782941, 0.51599606],\n",
              "       [0.28867416, 0.35103112, 0.30026905, 0.63426467, 0.28836094,\n",
              "        0.3378979 , 0.70495062, 0.148824  , 0.26643517, 0.26044992,\n",
              "        0.95023966, 0.04167765, 0.65143632, 0.94311577, 0.30699467,\n",
              "        0.93364655, 0.38555908, 0.25618894, 0.74826125, 0.38708164,\n",
              "        0.77845868, 0.84748998, 0.91181468, 0.74114031, 0.27197799,\n",
              "        0.51125575, 0.220576  , 0.80656022],\n",
              "       [0.94008231, 0.18909391, 0.85420647, 0.89471699, 0.94542666,\n",
              "        0.45773943, 0.62557918, 0.02326478, 0.29605709, 0.66895496,\n",
              "        0.44866577, 0.22426586, 0.46886346, 0.09008471, 0.0821824 ,\n",
              "        0.53809277, 0.62907669, 0.20428441, 0.52253779, 0.18559361,\n",
              "        0.54646739, 0.02455556, 0.81747562, 0.86248695, 0.05087353,\n",
              "        0.87962218, 0.15932907, 0.68742697],\n",
              "       [0.79467822, 0.19268127, 0.16512586, 0.28889458, 0.78783215,\n",
              "        0.3584492 , 0.07949528, 0.20124674, 0.51754993, 0.81403149,\n",
              "        0.22560066, 0.31434189, 0.78372289, 0.19988469, 0.96970847,\n",
              "        0.34035169, 0.75255755, 0.73889817, 0.5384893 , 0.43433999,\n",
              "        0.96056224, 0.75848866, 0.33015201, 0.66674024, 0.86351563,\n",
              "        0.23686454, 0.14746547, 0.65770723],\n",
              "       [0.0741549 , 0.78939036, 0.73504977, 0.89519655, 0.41748133,\n",
              "        0.26340468, 0.68238814, 0.37216093, 0.9171874 , 0.58562859,\n",
              "        0.87577563, 0.6855393 , 0.08193281, 0.09776845, 0.09999592,\n",
              "        0.68991345, 0.82509356, 0.73902098, 0.51190054, 0.17339654,\n",
              "        0.92307961, 0.15554972, 0.07787379, 0.59696967, 0.96539997,\n",
              "        0.00612052, 0.43353863, 0.57935479]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# im2col로 데이터 전개하기\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "TYrWBibF0VAG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 계층 구현하기\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널 수, 높이, 너비)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF8IsZJ_1CL3",
        "outputId": "992ca79f-ea53-433e-df0f-e5f4694cc20a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syirT3IE1PQZ",
        "outputId": "40b53652-4e4c-4c25-e1b5-390851bf102e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Convulution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    col_W = self.W.reshape(FN, -1).T # 필터 전개\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "0MpeLNgD1VK4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파 포함"
      ],
      "metadata": {
        "id": "XsHXEn0124BN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "EY_w-HG93Imm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "uNfHZhkp3CDW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀링 계층 구현하기\n",
        "\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    # 전개 (1)\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "    # 최댓값 (2)\n",
        "    out = np.max(col, axis=1)\n",
        "\n",
        "    # 성형 (3)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ReMxkIh43C37"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파 포함 pooling 계층\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "-dSH50GA4ySw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "6FyfGQ1f7qx-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim=(1, 28, 28),\n",
        "               conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "               hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * \\\n",
        "      np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * \\\n",
        "      np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * \\\n",
        "      np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "    self.layers['Relu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "      for layer in self.layers.values():\n",
        "        x = layer.forward(x)\n",
        "      return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "      y = self.predict(x)\n",
        "      return self.last_layer.forward(y, t)\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "      # forward\n",
        "      self.loss(x, t)\n",
        "\n",
        "      # backward\n",
        "      dout = 1\n",
        "      dout = self.last_layer.backward(dout)\n",
        "\n",
        "      layers = list(self.layers.values())\n",
        "      layers.reverse()\n",
        "      for layer in layers:\n",
        "        dout = layer.backward(dout)\n",
        "\n",
        "      # 결과 저장\n",
        "      grads = {}\n",
        "      grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "      grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "      grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "      return grads"
      ],
      "metadata": {
        "id": "ez8wMVTL6Rb-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "metadata": {
        "id": "xtuD6DlA91Dd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "metadata": {
        "id": "gN00AM-Y9HMB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "h1wCgz-x-JXv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ],
      "metadata": {
        "id": "4IiBkjb_-Ntl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mnist 실습"
      ],
      "metadata": {
        "id": "PCFq0LmT9_R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST로 실험\n",
        "\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import sys, os\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch/dataset') #구글 드라이브와 연결합니다.\n",
        "from mnist import load_mnist #dataset.mnist가 아닌 그냥 mnist입니다!\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0TtEOp4n7pBv",
        "outputId": "0d49bbad-106b-4ae9-e153-49f6fcb48396"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.003520774885374663\n",
            "train loss:0.005562641681232626\n",
            "train loss:0.002305484357290267\n",
            "train loss:0.0015481184465435454\n",
            "train loss:0.0049296711307947085\n",
            "train loss:0.003894858921474325\n",
            "train loss:0.0018567489024977582\n",
            "train loss:0.0051391731780121306\n",
            "train loss:0.001652998212257948\n",
            "train loss:0.003932541814776644\n",
            "train loss:0.004957008491661193\n",
            "train loss:0.006262896374273478\n",
            "train loss:0.009331943777936298\n",
            "train loss:0.01670929362948118\n",
            "train loss:0.0034956809672626486\n",
            "train loss:0.004566108697628394\n",
            "train loss:0.0020228675357871735\n",
            "train loss:0.004135403115420355\n",
            "train loss:0.015149212156978923\n",
            "train loss:0.005068684684154147\n",
            "train loss:0.0015912791482220327\n",
            "train loss:0.0005481927842385621\n",
            "train loss:0.0027091141277463464\n",
            "train loss:0.002238086042777569\n",
            "train loss:0.0013177826614344252\n",
            "train loss:0.004125385614299102\n",
            "train loss:0.011736882491386318\n",
            "train loss:0.003366940963474388\n",
            "train loss:0.0014644233974547135\n",
            "train loss:0.014478135703004049\n",
            "train loss:0.012019195110649644\n",
            "train loss:0.0030761535508757832\n",
            "train loss:0.00374011367866702\n",
            "train loss:0.0022326446859897084\n",
            "train loss:0.0042505909062168536\n",
            "train loss:0.0016058234398099102\n",
            "train loss:0.005455013679105706\n",
            "train loss:0.0023148346481577293\n",
            "train loss:0.008374173801495228\n",
            "train loss:0.0264662112289643\n",
            "train loss:0.013413746827485602\n",
            "train loss:0.0057104909444230835\n",
            "train loss:0.003872731483327399\n",
            "train loss:0.01147910570520922\n",
            "train loss:0.0029083743017901936\n",
            "train loss:0.001303529454064043\n",
            "train loss:0.0039296642377992025\n",
            "train loss:0.0054042575649974455\n",
            "train loss:0.00451668146700593\n",
            "train loss:0.0031427616888165437\n",
            "train loss:0.0040043982582429645\n",
            "train loss:0.005806508813464609\n",
            "train loss:0.012350716623675075\n",
            "train loss:0.0023460453783952463\n",
            "train loss:0.0031872161791555516\n",
            "train loss:0.002118784610438565\n",
            "train loss:0.001350689661729313\n",
            "train loss:0.01869834608460417\n",
            "train loss:0.0008035813450595566\n",
            "train loss:0.002749619901442576\n",
            "train loss:0.004408958061995892\n",
            "train loss:0.003677765439204855\n",
            "train loss:0.004012210052201484\n",
            "train loss:0.002068949499248463\n",
            "train loss:0.008421222911905486\n",
            "train loss:0.002213762239624755\n",
            "train loss:0.001592328702720782\n",
            "train loss:0.0009839115216645658\n",
            "train loss:0.0005548082247850492\n",
            "train loss:0.003476307683477863\n",
            "train loss:0.003665609524600908\n",
            "train loss:0.0006862107139361005\n",
            "train loss:0.0020529892198526147\n",
            "train loss:0.002440571598242838\n",
            "train loss:0.001715262825357318\n",
            "train loss:0.004940539564662704\n",
            "train loss:0.004413215136713257\n",
            "train loss:0.0031029147645659392\n",
            "train loss:0.0014463129257613844\n",
            "train loss:0.0017239834115650287\n",
            "train loss:0.001384098694044341\n",
            "train loss:0.0008102243342138904\n",
            "train loss:0.0028124187618726714\n",
            "train loss:0.01227506661252569\n",
            "train loss:0.009231996716728773\n",
            "train loss:0.00042666863552242\n",
            "train loss:0.0013231816486431903\n",
            "train loss:0.0008534615581326217\n",
            "train loss:0.0030607758792688894\n",
            "train loss:0.0004946853002198964\n",
            "train loss:0.006230085840905032\n",
            "train loss:0.009118552883724894\n",
            "train loss:0.02366176353042644\n",
            "train loss:0.021248518813093403\n",
            "train loss:0.0013034124865470176\n",
            "train loss:0.005475148888495046\n",
            "train loss:0.003940524924545627\n",
            "train loss:0.021134807603282676\n",
            "train loss:0.011444308061611745\n",
            "train loss:0.0370453690436104\n",
            "train loss:0.024314124635527228\n",
            "train loss:0.005000312067449361\n",
            "train loss:0.0031358865310654837\n",
            "train loss:0.005005467867699742\n",
            "train loss:0.0014626048141198138\n",
            "train loss:0.005095896514001445\n",
            "train loss:0.0029077389362599814\n",
            "train loss:0.0003317593241354396\n",
            "train loss:0.0013324107131622756\n",
            "train loss:0.000918256331462429\n",
            "train loss:0.005788719219124626\n",
            "train loss:0.0019382470140919567\n",
            "train loss:0.0020383708233930853\n",
            "train loss:0.004679145479170609\n",
            "train loss:0.023690281633353898\n",
            "train loss:0.003630915823483129\n",
            "train loss:0.0004700984844539794\n",
            "train loss:0.0008268492568727319\n",
            "train loss:0.02987107861939672\n",
            "train loss:0.004589537474423089\n",
            "train loss:0.009319639855035079\n",
            "train loss:0.000976162696235286\n",
            "train loss:0.019836028211374493\n",
            "train loss:0.0007388498196901866\n",
            "train loss:0.0004743759841617381\n",
            "train loss:0.007180069880940198\n",
            "train loss:0.0016440775387042954\n",
            "train loss:0.0026570447866161788\n",
            "train loss:0.0069027022142518326\n",
            "train loss:0.001960291659296594\n",
            "train loss:0.0029488500492701073\n",
            "train loss:0.0019686368871062467\n",
            "train loss:0.0030109245720059768\n",
            "train loss:0.0017520812351187454\n",
            "train loss:0.015624914655720697\n",
            "train loss:0.03785808635848282\n",
            "train loss:0.003224291879338435\n",
            "train loss:0.007352476931085433\n",
            "train loss:0.0016723010133351344\n",
            "train loss:0.002064587485414564\n",
            "train loss:0.00026226543332745483\n",
            "train loss:0.0007217943457829092\n",
            "train loss:0.002437283249611701\n",
            "train loss:0.0033345086553270755\n",
            "train loss:0.0039751989289924915\n",
            "train loss:0.0012462525398107848\n",
            "train loss:0.0073164243809802085\n",
            "train loss:0.0022403439360955397\n",
            "train loss:0.004362162770384501\n",
            "train loss:0.004027743591866627\n",
            "train loss:0.0033043998162301975\n",
            "train loss:0.00021329582792784448\n",
            "train loss:0.003460276706607542\n",
            "train loss:0.0009910954399282503\n",
            "train loss:0.004586009811340203\n",
            "train loss:0.007293135859544128\n",
            "train loss:0.006648891032344803\n",
            "train loss:0.0065455381151633555\n",
            "train loss:0.008310310448121196\n",
            "train loss:0.0039294834348353215\n",
            "train loss:0.03128611307635361\n",
            "train loss:0.0018136964842180145\n",
            "train loss:0.000681183546313088\n",
            "train loss:0.007042665847710486\n",
            "train loss:0.0009363319461437483\n",
            "train loss:0.004317757967863452\n",
            "train loss:0.000960645164585714\n",
            "train loss:0.0052772603165341095\n",
            "train loss:0.006474669616404062\n",
            "train loss:0.015648902775241683\n",
            "train loss:0.0030108208990863895\n",
            "train loss:0.005917295879981394\n",
            "train loss:0.001578949354950145\n",
            "train loss:0.001188298952911909\n",
            "train loss:0.00039628933676856846\n",
            "train loss:0.0010047007900286784\n",
            "train loss:0.02083683277398095\n",
            "train loss:0.001838476098173492\n",
            "train loss:0.0033294475111686427\n",
            "train loss:0.01948410610255995\n",
            "train loss:0.0010430739998104578\n",
            "train loss:0.001324833032372053\n",
            "train loss:0.010766253114161694\n",
            "train loss:0.0019779021115899457\n",
            "train loss:0.008263780990625275\n",
            "train loss:0.0002535356335280952\n",
            "train loss:0.0025815558354569433\n",
            "train loss:0.0009593719562485437\n",
            "train loss:0.0027085060394140444\n",
            "train loss:0.004114314584688762\n",
            "=== epoch:13, train acc:0.999, test acc:0.988 ===\n",
            "train loss:0.0027026752929377356\n",
            "train loss:0.0024799254279169094\n",
            "train loss:0.048448649098708454\n",
            "train loss:0.0011561846817325517\n",
            "train loss:0.023346055009249092\n",
            "train loss:0.005512790028266682\n",
            "train loss:0.0007386861551737235\n",
            "train loss:0.000670904831120529\n",
            "train loss:0.013365550300311996\n",
            "train loss:0.002465603344880934\n",
            "train loss:0.001989452763301385\n",
            "train loss:0.010549036315808415\n",
            "train loss:0.0017256111830413043\n",
            "train loss:0.00353221100470039\n",
            "train loss:0.003555233235790592\n",
            "train loss:0.0012176332788491017\n",
            "train loss:0.0010192180995068536\n",
            "train loss:0.0029728472066005685\n",
            "train loss:0.0007711016812228076\n",
            "train loss:0.0026246688710698986\n",
            "train loss:0.007598537730744771\n",
            "train loss:0.003243970776632012\n",
            "train loss:0.004006591209606465\n",
            "train loss:0.030973393003656504\n",
            "train loss:0.0017144557759714615\n",
            "train loss:0.0009506100747166742\n",
            "train loss:0.0027719724122732383\n",
            "train loss:0.0008157620583954906\n",
            "train loss:0.0038843658998610193\n",
            "train loss:0.0005651240764868051\n",
            "train loss:0.006131523818523078\n",
            "train loss:0.005779184927171047\n",
            "train loss:0.0011841571769341122\n",
            "train loss:0.0024266933584931073\n",
            "train loss:0.01069833201316406\n",
            "train loss:0.00335557620828398\n",
            "train loss:0.0028661738626542897\n",
            "train loss:0.0008234468709183669\n",
            "train loss:0.007221143284951914\n",
            "train loss:0.0009041629375557544\n",
            "train loss:0.0068150795510452085\n",
            "train loss:0.0008571184193555483\n",
            "train loss:0.0034188253237513026\n",
            "train loss:0.002919016082296378\n",
            "train loss:0.0032282572819991567\n",
            "train loss:0.0019976023318197143\n",
            "train loss:0.001972774081025291\n",
            "train loss:0.009082027253825035\n",
            "train loss:0.0016117386081198706\n",
            "train loss:0.0015379570062042202\n",
            "train loss:0.004087690473207452\n",
            "train loss:0.003283461850252122\n",
            "train loss:0.004603031038819959\n",
            "train loss:0.005860971349728749\n",
            "train loss:0.004041404298817355\n",
            "train loss:0.0009164643703533869\n",
            "train loss:0.002462272341126504\n",
            "train loss:0.002000006391820153\n",
            "train loss:0.004666043857020011\n",
            "train loss:0.01909441887671143\n",
            "train loss:0.002201839253423049\n",
            "train loss:0.00610430970456537\n",
            "train loss:0.0011447180599786413\n",
            "train loss:0.002055587830063685\n",
            "train loss:0.0035966024938215145\n",
            "train loss:0.0008273824577779628\n",
            "train loss:0.00213960382308105\n",
            "train loss:0.0010500536746779333\n",
            "train loss:0.0031792002420563585\n",
            "train loss:0.0004293960328154808\n",
            "train loss:0.0018322925058182902\n",
            "train loss:0.0009950329960054945\n",
            "train loss:0.0021674762789277973\n",
            "train loss:0.0005314248921605867\n",
            "train loss:0.022188856089152224\n",
            "train loss:0.001605905010713766\n",
            "train loss:0.0012087792961668605\n",
            "train loss:0.011873481757993182\n",
            "train loss:0.0020035044798391542\n",
            "train loss:0.0038999337703920856\n",
            "train loss:0.004469563420565378\n",
            "train loss:0.007078897550209589\n",
            "train loss:0.003830396086501918\n",
            "train loss:0.00326012932047396\n",
            "train loss:0.003002523300821825\n",
            "train loss:0.004767031662028257\n",
            "train loss:0.003314218229906218\n",
            "train loss:0.004692122665354127\n",
            "train loss:0.0026904152239697605\n",
            "train loss:0.0025669343745165166\n",
            "train loss:0.00124474175319386\n",
            "train loss:0.003883323805192085\n",
            "train loss:0.004615959486733578\n",
            "train loss:0.0005337041525253584\n",
            "train loss:0.000499230207398669\n",
            "train loss:0.00017395427968981865\n",
            "train loss:0.030816828611212946\n",
            "train loss:0.008565940733343363\n",
            "train loss:0.004954958112207823\n",
            "train loss:0.0018416281445022466\n",
            "train loss:0.005161032484403816\n",
            "train loss:0.0044920498823387564\n",
            "train loss:0.0026251175814892935\n",
            "train loss:0.0026241054734749986\n",
            "train loss:0.0022534282863450115\n",
            "train loss:0.0032388429236703525\n",
            "train loss:0.0022616400609239357\n",
            "train loss:0.0016925491515311967\n",
            "train loss:0.003236660141414214\n",
            "train loss:0.0020380813750457707\n",
            "train loss:0.004250503991888279\n",
            "train loss:0.0008725752688545749\n",
            "train loss:0.01767719682482291\n",
            "train loss:0.003395594796774505\n",
            "train loss:0.0018894920365029042\n",
            "train loss:0.00019496571770811797\n",
            "train loss:0.006005445735918373\n",
            "train loss:0.0034336524960049105\n",
            "train loss:0.001586496883918016\n",
            "train loss:0.004649746228692975\n",
            "train loss:0.008005460779691083\n",
            "train loss:0.00553241411052262\n",
            "train loss:0.002179515517242513\n",
            "train loss:0.0013613235050126593\n",
            "train loss:0.006335079587421667\n",
            "train loss:0.0008727007233424888\n",
            "train loss:0.00144142123389271\n",
            "train loss:0.005307602032951875\n",
            "train loss:0.0025834229997908\n",
            "train loss:0.0003469105368920459\n",
            "train loss:0.0017048102344237475\n",
            "train loss:0.0035537524793136173\n",
            "train loss:0.004064431776693769\n",
            "train loss:0.0015969398532723856\n",
            "train loss:0.008254680626901255\n",
            "train loss:0.0005520148313774796\n",
            "train loss:0.003732834049631113\n",
            "train loss:0.0011023152843123432\n",
            "train loss:0.004301279426256915\n",
            "train loss:0.000784123400198555\n",
            "train loss:0.0006971493059506604\n",
            "train loss:0.020225983394135953\n",
            "train loss:0.00022485616188561486\n",
            "train loss:0.0015442182083480439\n",
            "train loss:0.0015066137361491934\n",
            "train loss:0.0075555419805971745\n",
            "train loss:0.001216676020830243\n",
            "train loss:0.003170068632664413\n",
            "train loss:0.004033920445449966\n",
            "train loss:0.002401517367948609\n",
            "train loss:0.0002917795327471472\n",
            "train loss:0.0016342281656117875\n",
            "train loss:0.001924954185073245\n",
            "train loss:0.0017277897547668716\n",
            "train loss:0.003946156149112799\n",
            "train loss:0.00020553724219154183\n",
            "train loss:0.0021462737296059613\n",
            "train loss:0.0010249313422447375\n",
            "train loss:0.002242426283876475\n",
            "train loss:0.0003939763547540916\n",
            "train loss:0.0006885401079852755\n",
            "train loss:0.002755389511897346\n",
            "train loss:0.007626774277009799\n",
            "train loss:0.0020426254081656113\n",
            "train loss:0.007459966910278975\n",
            "train loss:0.00047705580055000687\n",
            "train loss:0.000649086278141455\n",
            "train loss:0.002231863671713995\n",
            "train loss:0.002344218917753833\n",
            "train loss:0.0008719186032627877\n",
            "train loss:0.003175064068753436\n",
            "train loss:0.055587284607862164\n",
            "train loss:0.00044117979249995074\n",
            "train loss:0.000664313334837241\n",
            "train loss:0.0048812586521635535\n",
            "train loss:0.004273626883874409\n",
            "train loss:0.003607720375304189\n",
            "train loss:0.0032570040584083896\n",
            "train loss:0.010244443742950929\n",
            "train loss:0.0051130266512254404\n",
            "train loss:0.00042737570783344926\n",
            "train loss:0.007245199327163759\n",
            "train loss:0.0004229297174106098\n",
            "train loss:0.01089820740789229\n",
            "train loss:0.0029396275905230803\n",
            "train loss:0.006108782823168115\n",
            "train loss:0.005410384953420127\n",
            "train loss:0.00011518192452553411\n",
            "train loss:0.0034374959130433784\n",
            "train loss:0.0006071886695500698\n",
            "train loss:0.009165246191801502\n",
            "train loss:0.01233159818604599\n",
            "train loss:0.0015056315944630089\n",
            "train loss:0.00014174118957870966\n",
            "train loss:0.001862366041796962\n",
            "train loss:0.00012741051478813847\n",
            "train loss:0.0004164130581313775\n",
            "train loss:0.0013400846643618228\n",
            "train loss:0.000888920264697425\n",
            "train loss:0.0023606676235763523\n",
            "train loss:0.0007279456293773893\n",
            "train loss:0.00208629296894574\n",
            "train loss:0.005870836940587118\n",
            "train loss:0.0025785281581076312\n",
            "train loss:0.0009536428129054719\n",
            "train loss:0.007764324115066244\n",
            "train loss:0.00037042229138974173\n",
            "train loss:0.007435163583798209\n",
            "train loss:0.0041238908331852646\n",
            "train loss:0.000668183820750272\n",
            "train loss:0.0045674651096089605\n",
            "train loss:8.629601911973966e-05\n",
            "train loss:0.012197691580903048\n",
            "train loss:0.0004548996090442795\n",
            "train loss:0.0001425799822043263\n",
            "train loss:0.0018799431952104737\n",
            "train loss:0.0006229837072248581\n",
            "train loss:0.004345794825301989\n",
            "train loss:0.001400837980792345\n",
            "train loss:0.003879621480730709\n",
            "train loss:0.001453311502949736\n",
            "train loss:0.001693249687954749\n",
            "train loss:0.05833215460512909\n",
            "train loss:0.001196770399855053\n",
            "train loss:0.006685735532066031\n",
            "train loss:0.000899068858336112\n",
            "train loss:0.0035888297522697495\n",
            "train loss:0.001446308275512993\n",
            "train loss:0.002757734988617258\n",
            "train loss:0.005461947669504108\n",
            "train loss:0.012259963006604748\n",
            "train loss:0.0016544279065094283\n",
            "train loss:0.00018119216782252684\n",
            "train loss:0.0026412313818273457\n",
            "train loss:0.0005430074701767259\n",
            "train loss:0.0030488311033312846\n",
            "train loss:0.0008523885492115629\n",
            "train loss:0.00044712858892273293\n",
            "train loss:0.00032667377879287126\n",
            "train loss:0.0012938038292867437\n",
            "train loss:0.005359480048530945\n",
            "train loss:0.004565922231532368\n",
            "train loss:0.0020214292443166944\n",
            "train loss:0.0036032193519785895\n",
            "train loss:0.006279906063479359\n",
            "train loss:0.004144438403308737\n",
            "train loss:0.0031908354177470925\n",
            "train loss:0.0061097607099932375\n",
            "train loss:0.0007564103756835827\n",
            "train loss:0.0005627638391771385\n",
            "train loss:0.0037061198053152345\n",
            "train loss:0.011458990917976173\n",
            "train loss:0.003372233285577347\n",
            "train loss:0.0008611439550554303\n",
            "train loss:0.0020724110173184736\n",
            "train loss:0.017284523775248778\n",
            "train loss:0.00503745260464676\n",
            "train loss:0.003328510498524078\n",
            "train loss:0.0003750472595751336\n",
            "train loss:0.0020310513454225147\n",
            "train loss:0.0064570045770832776\n",
            "train loss:0.002022306768501023\n",
            "train loss:0.0042817476327900415\n",
            "train loss:0.0036597548500394727\n",
            "train loss:0.007593456289136549\n",
            "train loss:0.002695779890254732\n",
            "train loss:0.003560511505767776\n",
            "train loss:0.0015754067835704695\n",
            "train loss:0.005540318214186681\n",
            "train loss:0.0017636348127145329\n",
            "train loss:0.002232336582852662\n",
            "train loss:0.025337116483409913\n",
            "train loss:0.015213437767829128\n",
            "train loss:0.0014002604563743105\n",
            "train loss:0.008211534077046372\n",
            "train loss:0.0019825663166701604\n",
            "train loss:0.0013538577483536481\n",
            "train loss:0.0006178802290629764\n",
            "train loss:0.0015063260013956148\n",
            "train loss:0.005324890454364389\n",
            "train loss:0.0004648308661939931\n",
            "train loss:0.005409250195021138\n",
            "train loss:0.0016768097616723298\n",
            "train loss:0.005025828804170868\n",
            "train loss:0.0007631445565434763\n",
            "train loss:0.00038225185495805743\n",
            "train loss:0.0018779856581476518\n",
            "train loss:0.001340564060744936\n",
            "train loss:0.005471259321340145\n",
            "train loss:0.0009913063359168635\n",
            "train loss:0.005199140590672392\n",
            "train loss:0.008347260054715026\n",
            "train loss:0.003884750865119897\n",
            "train loss:0.006009798217054975\n",
            "train loss:0.023477178351963772\n",
            "train loss:0.003139441591381857\n",
            "train loss:0.008962249832569363\n",
            "train loss:0.0027581604225080545\n",
            "train loss:0.0011319207855037994\n",
            "train loss:0.0011426368796413806\n",
            "train loss:0.003410680708177092\n",
            "train loss:0.0011704928976902897\n",
            "train loss:0.004629418679657988\n",
            "train loss:0.00931947008501546\n",
            "train loss:0.0034345390791084896\n",
            "train loss:0.0006461328382816\n",
            "train loss:0.005303342885221748\n",
            "train loss:0.0046299268037027445\n",
            "train loss:0.005004613993423147\n",
            "train loss:0.004868386861886767\n",
            "train loss:0.004159080178600185\n",
            "train loss:0.002901640772500631\n",
            "train loss:0.0013231442127240869\n",
            "train loss:0.005278286425615081\n",
            "train loss:0.0010754210492459131\n",
            "train loss:0.00238886930302784\n",
            "train loss:0.0018101816795958446\n",
            "train loss:0.0013786384603039657\n",
            "train loss:0.00938085939023399\n",
            "train loss:0.0027228694123379218\n",
            "train loss:0.016741307657072903\n",
            "train loss:0.007416597454599436\n",
            "train loss:0.018578737158534336\n",
            "train loss:0.01879626053070434\n",
            "train loss:0.005216727248967267\n",
            "train loss:0.0016297515078690296\n",
            "train loss:0.002810031857326195\n",
            "train loss:0.0016925499289480576\n",
            "train loss:0.005030841933194059\n",
            "train loss:0.0012351054438057808\n",
            "train loss:0.0004159442840984851\n",
            "train loss:0.004194163901517684\n",
            "train loss:0.0033314399714101978\n",
            "train loss:0.0014830287625171867\n",
            "train loss:0.0036665776281774197\n",
            "train loss:0.004295627388320655\n",
            "train loss:0.0029063538662284305\n",
            "train loss:0.0006092986512833854\n",
            "train loss:0.0012015231589974783\n",
            "train loss:0.00012424652040026363\n",
            "train loss:0.0038963549181841523\n",
            "train loss:0.0018021439437693693\n",
            "train loss:0.0001706328483493876\n",
            "train loss:0.0051309053590484184\n",
            "train loss:0.003101748621220198\n",
            "train loss:0.0038664395358155\n",
            "train loss:0.000268248177075123\n",
            "train loss:0.002419417843497235\n",
            "train loss:0.0015500834953506059\n",
            "train loss:0.0008515117783946673\n",
            "train loss:0.0013440835748263096\n",
            "train loss:0.00017710446847477658\n",
            "train loss:0.0068567928358607465\n",
            "train loss:0.002484469052805881\n",
            "train loss:0.0016018924994231569\n",
            "train loss:0.00015242189182218026\n",
            "train loss:0.001397112653726333\n",
            "train loss:0.0019671989833859645\n",
            "train loss:0.0019637688894975965\n",
            "train loss:0.0005591104133374292\n",
            "train loss:3.0409470478471858e-05\n",
            "train loss:0.004551135056341593\n",
            "train loss:0.00020502451608607066\n",
            "train loss:0.0011353878099018702\n",
            "train loss:0.0015159530610455484\n",
            "train loss:0.004537141431762179\n",
            "train loss:0.002314650107408476\n",
            "train loss:0.006121862733664313\n",
            "train loss:0.00027969684304487904\n",
            "train loss:0.0032956733315038576\n",
            "train loss:0.0010490763461690263\n",
            "train loss:0.0016735448434507178\n",
            "train loss:0.0012889828418901515\n",
            "train loss:0.003391162152312884\n",
            "train loss:0.00044750196947794654\n",
            "train loss:0.026227846278957367\n",
            "train loss:0.02455053464699619\n",
            "train loss:0.0032093725299997262\n",
            "train loss:0.004757936511288433\n",
            "train loss:0.007872926070712391\n",
            "train loss:0.0009758709741448295\n",
            "train loss:0.0018567943925113747\n",
            "train loss:0.002279653633419381\n",
            "train loss:0.04559197939727141\n",
            "train loss:0.002554860760800195\n",
            "train loss:0.004759055315174594\n",
            "train loss:0.008370037182527988\n",
            "train loss:0.0031636130952505987\n",
            "train loss:0.0016339999355363868\n",
            "train loss:0.000839094189382653\n",
            "train loss:0.003948598399689018\n",
            "train loss:0.002262102609860778\n",
            "train loss:0.0004787816860291252\n",
            "train loss:0.005918261986554856\n",
            "train loss:0.0013892539100412678\n",
            "train loss:0.00030524045763132517\n",
            "train loss:0.001114573769647059\n",
            "train loss:0.0022573479735081546\n",
            "train loss:0.004367781263918119\n",
            "train loss:0.0012359606331953216\n",
            "train loss:0.0008737980542615535\n",
            "train loss:0.0014021836347012542\n",
            "train loss:0.017459251810998902\n",
            "train loss:0.005099475523889594\n",
            "train loss:0.005841817480879418\n",
            "train loss:0.0021269576384946166\n",
            "train loss:0.002279706067356196\n",
            "train loss:0.00018979813676371773\n",
            "train loss:0.0027958939328119304\n",
            "train loss:0.005438160775183124\n",
            "train loss:0.004529549675608497\n",
            "train loss:0.006215908377825672\n",
            "train loss:0.002106947435073656\n",
            "train loss:0.0007226371747903926\n",
            "train loss:0.0105511785074478\n",
            "train loss:0.01623239686854908\n",
            "train loss:0.001131757320141481\n",
            "train loss:0.0013476112171781705\n",
            "train loss:0.005326479185868339\n",
            "train loss:0.0017447616013468554\n",
            "train loss:0.009792722225684341\n",
            "train loss:0.004935894444595579\n",
            "train loss:0.006993138833758488\n",
            "train loss:0.0003666788942023538\n",
            "train loss:0.007123294562234753\n",
            "train loss:0.01733061929344766\n",
            "train loss:0.009207358407837101\n",
            "train loss:0.0009092839345559932\n",
            "train loss:0.0010602121594209266\n",
            "train loss:0.0014549950721742045\n",
            "train loss:0.0015940174477598798\n",
            "train loss:0.00013208714745539503\n",
            "train loss:0.005071811483305289\n",
            "train loss:0.0015652198545481355\n",
            "train loss:0.013651711044799477\n",
            "train loss:0.00037108397611669385\n",
            "train loss:0.004289756091690225\n",
            "train loss:0.0022440902208668914\n",
            "train loss:0.003695000186483836\n",
            "train loss:0.001947550922295799\n",
            "train loss:0.0015646575391650055\n",
            "train loss:0.004731914455363518\n",
            "train loss:0.002116612556233381\n",
            "train loss:0.0024239026759743537\n",
            "train loss:0.0006182836771360814\n",
            "train loss:0.0014162274337029572\n",
            "train loss:0.005443278929307964\n",
            "train loss:0.0034112107580045875\n",
            "train loss:0.004271553307078203\n",
            "train loss:0.0008848664501144661\n",
            "train loss:0.002707903182232127\n",
            "train loss:0.0020534685782357346\n",
            "train loss:0.002119117421895488\n",
            "train loss:0.006393939705904663\n",
            "train loss:0.00027806305034201603\n",
            "train loss:0.0015853487546964179\n",
            "train loss:0.006652877815283974\n",
            "train loss:0.026526743624883435\n",
            "train loss:0.0024998381486995014\n",
            "train loss:0.003509808803704608\n",
            "train loss:0.0004921570712735726\n",
            "train loss:0.002269157950389918\n",
            "train loss:0.0009647822365572502\n",
            "train loss:0.0007709978808721854\n",
            "train loss:0.015026132066573323\n",
            "train loss:0.0009817815826173313\n",
            "train loss:0.0005378243854072552\n",
            "train loss:0.000971792153010418\n",
            "train loss:0.0018866240026782334\n",
            "train loss:0.006324763200961145\n",
            "train loss:0.007147588978299112\n",
            "train loss:0.006323610380983658\n",
            "train loss:0.0019601017391010288\n",
            "train loss:0.028363619450678768\n",
            "train loss:0.0008748153768598299\n",
            "train loss:0.004559611867950745\n",
            "train loss:0.019692743547730467\n",
            "train loss:0.007744729877604891\n",
            "train loss:0.0008485579085691898\n",
            "train loss:0.00427584268108348\n",
            "train loss:0.005073957607402012\n",
            "train loss:0.0004108074714794642\n",
            "train loss:0.0032019686002738106\n",
            "train loss:0.016714366108888833\n",
            "train loss:0.00048750349040164645\n",
            "train loss:0.002875373597515918\n",
            "train loss:0.002905896591461993\n",
            "train loss:0.0023813037798504537\n",
            "train loss:0.0034971852941472316\n",
            "train loss:0.0023012519014180403\n",
            "train loss:0.006079101676091542\n",
            "train loss:0.001459790627311148\n",
            "train loss:0.00333599356250373\n",
            "train loss:0.0005386182310110107\n",
            "train loss:0.0007417020138136665\n",
            "train loss:0.0013931830579162318\n",
            "train loss:0.0005252487647282886\n",
            "train loss:0.0050657552381653806\n",
            "train loss:0.002562359122906408\n",
            "train loss:0.0050491841738868745\n",
            "train loss:0.01104447897481468\n",
            "train loss:0.006654542199839437\n",
            "train loss:0.008560401933961303\n",
            "train loss:0.002692416766409903\n",
            "train loss:0.002395867864483873\n",
            "train loss:0.001285872835837002\n",
            "train loss:0.0026143765162830314\n",
            "train loss:0.006497127208680976\n",
            "train loss:0.00750346964408562\n",
            "train loss:0.0005366258466339791\n",
            "train loss:0.0030720232780378663\n",
            "train loss:0.0007939330725530762\n",
            "train loss:0.0005093392947933754\n",
            "train loss:0.0016833741281093509\n",
            "train loss:0.007778877302614325\n",
            "train loss:0.007901608035290025\n",
            "train loss:0.00139757180553723\n",
            "train loss:9.680914114091802e-05\n",
            "train loss:0.002129391418516467\n",
            "train loss:0.0018173421078290283\n",
            "train loss:0.0009243205789539639\n",
            "train loss:0.006924301368215843\n",
            "train loss:0.010992070188865997\n",
            "train loss:0.0009609258918020834\n",
            "train loss:0.0008494684205189832\n",
            "train loss:0.0010551232558375307\n",
            "train loss:0.004064790194769926\n",
            "train loss:0.011246876592870002\n",
            "train loss:0.0003152070514366401\n",
            "train loss:0.00020087658326630378\n",
            "train loss:0.0029149453927626535\n",
            "train loss:0.0003789671238692895\n",
            "train loss:0.0006260481588632213\n",
            "train loss:0.0016290711169248\n",
            "train loss:0.0025041421518505248\n",
            "train loss:0.0001511709401352342\n",
            "train loss:0.003520056628535968\n",
            "train loss:0.0002426896665601997\n",
            "train loss:0.0011326713384116048\n",
            "train loss:0.003770190896318589\n",
            "train loss:0.00044864232340372607\n",
            "train loss:0.002548533916461446\n",
            "train loss:0.0022946980991734763\n",
            "train loss:0.00022856813753678975\n",
            "train loss:0.002988973592984233\n",
            "train loss:0.003946354287298477\n",
            "train loss:0.0012512386712229843\n",
            "train loss:0.0023328117884205486\n",
            "train loss:0.0017831864925127716\n",
            "train loss:0.0018383211407983274\n",
            "train loss:0.00040620297921604195\n",
            "train loss:0.0039530953566140285\n",
            "train loss:0.0009083333906686075\n",
            "train loss:0.0028190722175269107\n",
            "train loss:0.004190575921920769\n",
            "train loss:0.000338967423851423\n",
            "train loss:0.000938504205708793\n",
            "train loss:0.013039748794461\n",
            "train loss:0.004253377909654781\n",
            "train loss:0.0006406063714109733\n",
            "train loss:0.0029271072724717326\n",
            "train loss:0.0005064863714154954\n",
            "train loss:0.0015451891211219464\n",
            "train loss:0.01368449900256081\n",
            "train loss:0.0015247661159079923\n",
            "train loss:0.007481343709387205\n",
            "train loss:0.0005624383496888139\n",
            "train loss:0.007369221965692316\n",
            "train loss:0.0023460430309631404\n",
            "train loss:0.0018580189731204303\n",
            "train loss:0.0034166944187488963\n",
            "train loss:0.00036210486475338075\n",
            "train loss:0.000985008073446685\n",
            "train loss:0.0003394854668693962\n",
            "train loss:0.001831458215700858\n",
            "train loss:0.007417136471833813\n",
            "train loss:0.0024893303694972797\n",
            "train loss:0.0111425626985512\n",
            "train loss:4.985864592729552e-05\n",
            "train loss:0.00160946216919002\n",
            "train loss:0.0020048741869047523\n",
            "train loss:0.0024139087631435113\n",
            "train loss:0.00264644180306095\n",
            "train loss:0.004298473991948185\n",
            "train loss:0.0026491250356518847\n",
            "train loss:0.0020826124395781466\n",
            "train loss:0.0018921575802697136\n",
            "train loss:0.0015501925302514505\n",
            "train loss:0.00260882931777067\n",
            "train loss:0.002242988512175605\n",
            "train loss:0.002454538213753094\n",
            "train loss:0.04700198285798551\n",
            "train loss:0.00020928976869072122\n",
            "train loss:0.005871774229329655\n",
            "train loss:0.002817590445245005\n",
            "train loss:0.000380461726588125\n",
            "train loss:0.003852146071181979\n",
            "train loss:0.0030557921065818296\n",
            "train loss:0.005525129992112867\n",
            "train loss:0.001301542825745573\n",
            "=== epoch:14, train acc:0.997, test acc:0.991 ===\n",
            "train loss:0.00263548421694879\n",
            "train loss:0.0011117528854591484\n",
            "train loss:0.003385897971613969\n",
            "train loss:0.006631529045984912\n",
            "train loss:0.0005700818309376583\n",
            "train loss:0.0025274043038155925\n",
            "train loss:0.0008346082792019152\n",
            "train loss:0.002236866136204782\n",
            "train loss:0.003196472726519035\n",
            "train loss:0.0006373837111804279\n",
            "train loss:0.0005645327190062172\n",
            "train loss:0.0001735770667953658\n",
            "train loss:0.0007486168643492184\n",
            "train loss:0.0002023124689576164\n",
            "train loss:0.0046134918578025486\n",
            "train loss:0.0007525002654705566\n",
            "train loss:0.0037333006784926316\n",
            "train loss:0.004097083852824224\n",
            "train loss:0.0030663904666464723\n",
            "train loss:0.002009666381280305\n",
            "train loss:0.00044486319095524677\n",
            "train loss:0.0015715983816404426\n",
            "train loss:0.0035468219720339515\n",
            "train loss:0.0014449787179686125\n",
            "train loss:0.00013327060420885578\n",
            "train loss:0.001114122376762867\n",
            "train loss:0.00909411702443503\n",
            "train loss:0.0034089276550603297\n",
            "train loss:0.0015999120769572463\n",
            "train loss:0.00202153203056142\n",
            "train loss:0.00062340481419548\n",
            "train loss:0.007698789429268601\n",
            "train loss:0.0015010602951694163\n",
            "train loss:0.00029381368619461835\n",
            "train loss:0.0005707840999011418\n",
            "train loss:0.000978160078424553\n",
            "train loss:0.0002041013560304613\n",
            "train loss:0.00012086332065405699\n",
            "train loss:0.002406479424101438\n",
            "train loss:0.0001404751155413659\n",
            "train loss:0.0013345635244803626\n",
            "train loss:0.0059770173043870375\n",
            "train loss:0.013545922025150769\n",
            "train loss:0.0149973007243767\n",
            "train loss:0.001915409392673347\n",
            "train loss:0.001331301240320781\n",
            "train loss:0.007419118605179712\n",
            "train loss:0.021696888973930562\n",
            "train loss:0.01764417393943762\n",
            "train loss:0.008714662159333223\n",
            "train loss:0.0019314829922860582\n",
            "train loss:0.00010808101550104561\n",
            "train loss:0.0019375500207777444\n",
            "train loss:0.022511943369140197\n",
            "train loss:0.00041192653296731526\n",
            "train loss:0.0035605268379256903\n",
            "train loss:0.0022498335987873822\n",
            "train loss:0.002732909992053814\n",
            "train loss:0.0026578784633295867\n",
            "train loss:0.007288009586133598\n",
            "train loss:0.028069465185388342\n",
            "train loss:0.0013195067282506692\n",
            "train loss:0.0008947015724320745\n",
            "train loss:0.00261698545282002\n",
            "train loss:0.004885851936351142\n",
            "train loss:0.025793185549121595\n",
            "train loss:0.003873657879969356\n",
            "train loss:0.009725928917559256\n",
            "train loss:0.0011552469338337633\n",
            "train loss:0.0009222693714356644\n",
            "train loss:0.0011082120640359702\n",
            "train loss:0.0003128032509335476\n",
            "train loss:0.001638543239759949\n",
            "train loss:0.0019882520301188066\n",
            "train loss:0.0007828494495368789\n",
            "train loss:0.0023908547718812616\n",
            "train loss:0.0015872493987651277\n",
            "train loss:0.0012165123369429658\n",
            "train loss:0.024965943088653332\n",
            "train loss:0.0015993792105638933\n",
            "train loss:0.001454730770555073\n",
            "train loss:0.011386683394284553\n",
            "train loss:0.0010552582598209168\n",
            "train loss:0.0016983514188861357\n",
            "train loss:0.003779354304183207\n",
            "train loss:0.006522241198238471\n",
            "train loss:0.0014708662670679877\n",
            "train loss:0.006542133845847547\n",
            "train loss:0.005643612188190106\n",
            "train loss:0.005017144846589812\n",
            "train loss:0.008375128038613079\n",
            "train loss:0.008735837122540035\n",
            "train loss:0.0019657928589922754\n",
            "train loss:0.0055473811266021035\n",
            "train loss:0.0011074503577743226\n",
            "train loss:0.001520103317298129\n",
            "train loss:0.001310261407802106\n",
            "train loss:0.006593115486641939\n",
            "train loss:0.0019613539156541748\n",
            "train loss:0.0008423725030622241\n",
            "train loss:0.0006199655436570452\n",
            "train loss:0.00204473958708038\n",
            "train loss:0.0012684924636849181\n",
            "train loss:0.0015962014883339967\n",
            "train loss:0.002355493652772651\n",
            "train loss:0.008252604715846105\n",
            "train loss:0.00038006577699722694\n",
            "train loss:0.011486131232230506\n",
            "train loss:0.0020673020695171486\n",
            "train loss:0.0002850714385979271\n",
            "train loss:0.0013995511425871296\n",
            "train loss:0.016448268498111606\n",
            "train loss:0.0035549947663966186\n",
            "train loss:0.001646507385545515\n",
            "train loss:0.0011940444689678838\n",
            "train loss:0.002612068763814075\n",
            "train loss:0.013119729773857908\n",
            "train loss:0.008443691156194123\n",
            "train loss:0.0007483885975608119\n",
            "train loss:0.0055261517238644805\n",
            "train loss:0.009027331432792511\n",
            "train loss:0.0003876149716054218\n",
            "train loss:0.004074066220648051\n",
            "train loss:0.001519111763927712\n",
            "train loss:0.00014268941038829864\n",
            "train loss:0.0021922766554898837\n",
            "train loss:0.011227523477976582\n",
            "train loss:0.0009347931889891977\n",
            "train loss:0.006397726604732257\n",
            "train loss:0.0051327506050108836\n",
            "train loss:0.0011737978716787635\n",
            "train loss:0.0068404047165697215\n",
            "train loss:0.0026758472026364383\n",
            "train loss:0.002894278890760977\n",
            "train loss:0.00263390730280565\n",
            "train loss:0.003300009222594657\n",
            "train loss:0.002585485435659588\n",
            "train loss:0.0022144126546333725\n",
            "train loss:0.0026903834107131048\n",
            "train loss:0.0008488411876066984\n",
            "train loss:0.0014238514540521698\n",
            "train loss:0.005078039302419143\n",
            "train loss:0.000880174172454925\n",
            "train loss:0.004372412904032208\n",
            "train loss:0.0036843752698269717\n",
            "train loss:0.0010417552297442036\n",
            "train loss:0.0025799266570998656\n",
            "train loss:0.0026736939785449564\n",
            "train loss:0.0019833563337601874\n",
            "train loss:0.003854974216257547\n",
            "train loss:0.004702857385462265\n",
            "train loss:0.0008011230946360707\n",
            "train loss:0.0016193451964109811\n",
            "train loss:0.0024382604106884345\n",
            "train loss:0.008543245665574706\n",
            "train loss:0.030081911697631077\n",
            "train loss:0.0017450738662265148\n",
            "train loss:0.0037337508732007653\n",
            "train loss:0.0018845882693242404\n",
            "train loss:0.00306663603281987\n",
            "train loss:0.00046440096043147666\n",
            "train loss:0.004154818601599486\n",
            "train loss:0.004431527478029881\n",
            "train loss:0.0018252168375361571\n",
            "train loss:0.001934656083878162\n",
            "train loss:0.000384108227310916\n",
            "train loss:0.0038245231420817264\n",
            "train loss:0.00471250471698574\n",
            "train loss:0.013497759873912907\n",
            "train loss:0.14510338256724284\n",
            "train loss:0.001961864890284857\n",
            "train loss:0.007196092573147576\n",
            "train loss:0.0031979670450724136\n",
            "train loss:0.0006855480612199661\n",
            "train loss:0.0009064659935958838\n",
            "train loss:0.014202757474562573\n",
            "train loss:0.0033085490898904207\n",
            "train loss:0.0021567646305804085\n",
            "train loss:0.0012775226713024257\n",
            "train loss:0.009526899617446679\n",
            "train loss:0.004327538334269558\n",
            "train loss:0.026554398586571555\n",
            "train loss:0.013203671760576977\n",
            "train loss:0.004999910408843427\n",
            "train loss:0.0038275468218533807\n",
            "train loss:0.002070842789426464\n",
            "train loss:0.03240229601448668\n",
            "train loss:0.0011789775257202953\n",
            "train loss:0.014746799394552139\n",
            "train loss:0.002231366866089851\n",
            "train loss:0.01171880789521517\n",
            "train loss:0.001034757825813661\n",
            "train loss:0.0056524290388139855\n",
            "train loss:0.005423415271606087\n",
            "train loss:0.008128510868118504\n",
            "train loss:0.002519132370258303\n",
            "train loss:0.007177294433170013\n",
            "train loss:0.008313077328034716\n",
            "train loss:0.009463973532502046\n",
            "train loss:0.006537524007574136\n",
            "train loss:0.0073543024169457245\n",
            "train loss:0.004187553309237258\n",
            "train loss:0.003619594559290776\n",
            "train loss:0.0012071335867091227\n",
            "train loss:0.0060181035439463145\n",
            "train loss:0.0008908315395877701\n",
            "train loss:0.00024033048669521505\n",
            "train loss:0.0014553048346968398\n",
            "train loss:0.002907590083780196\n",
            "train loss:0.016227167501663392\n",
            "train loss:0.0030002574502048206\n",
            "train loss:0.003611846334592554\n",
            "train loss:0.011372703567625264\n",
            "train loss:0.02265501463208413\n",
            "train loss:0.007263381222614899\n",
            "train loss:0.025140160668862516\n",
            "train loss:0.001327786954352198\n",
            "train loss:0.003678268763289052\n",
            "train loss:0.002831164808711472\n",
            "train loss:0.017494069055029034\n",
            "train loss:0.0030195441679103053\n",
            "train loss:0.002777371444105367\n",
            "train loss:0.059181486820664826\n",
            "train loss:0.0014277131986465221\n",
            "train loss:0.00023772515431699952\n",
            "train loss:0.028585033086257813\n",
            "train loss:0.0015224152075422642\n",
            "train loss:0.02695374181318453\n",
            "train loss:0.0008356600573534314\n",
            "train loss:0.003946117065287133\n",
            "train loss:6.606156879656837e-05\n",
            "train loss:0.0010171135824849009\n",
            "train loss:0.0010109882716590366\n",
            "train loss:0.002033022003488364\n",
            "train loss:0.006046019571257424\n",
            "train loss:0.007135925177206079\n",
            "train loss:0.004264808010205537\n",
            "train loss:0.003615937210662642\n",
            "train loss:0.0004932759653117117\n",
            "train loss:0.0042177486066574685\n",
            "train loss:0.00619945251514991\n",
            "train loss:0.0008137732351240246\n",
            "train loss:0.0017654930582514073\n",
            "train loss:0.0007122578930351928\n",
            "train loss:0.0015943562639343782\n",
            "train loss:0.002719616917290415\n",
            "train loss:0.0004924812794248058\n",
            "train loss:0.003981015381580944\n",
            "train loss:0.012538889562343272\n",
            "train loss:0.0031292860417507424\n",
            "train loss:0.021605413042753582\n",
            "train loss:0.002813713632849566\n",
            "train loss:0.000819918117926128\n",
            "train loss:0.00494024980909518\n",
            "train loss:0.005409466707387337\n",
            "train loss:0.0044032514288327015\n",
            "train loss:0.012096783368710135\n",
            "train loss:0.001813645642276473\n",
            "train loss:0.0001788061127869858\n",
            "train loss:0.0012303370863208526\n",
            "train loss:0.004182340679358631\n",
            "train loss:0.015133855320856853\n",
            "train loss:0.0016744321174368218\n",
            "train loss:0.0009400985076239092\n",
            "train loss:0.0011575178906345891\n",
            "train loss:0.00036325838988247405\n",
            "train loss:0.0018582776134255647\n",
            "train loss:0.0027152447649006687\n",
            "train loss:0.013311944598304466\n",
            "train loss:0.004707060428763744\n",
            "train loss:0.03614287075680316\n",
            "train loss:0.0006493493105716042\n",
            "train loss:0.014757009076511984\n",
            "train loss:0.0013233908943295228\n",
            "train loss:0.00745699812939279\n",
            "train loss:0.0013825811094508463\n",
            "train loss:0.0015713894330336173\n",
            "train loss:0.00788429066870751\n",
            "train loss:0.004649983500922053\n",
            "train loss:0.0005307505851730389\n",
            "train loss:0.004143665699774252\n",
            "train loss:0.0007454842698239576\n",
            "train loss:0.00011190106991120767\n",
            "train loss:0.005639371132169043\n",
            "train loss:0.004860139756804297\n",
            "train loss:0.0007391308752651263\n",
            "train loss:0.0010442363317258348\n",
            "train loss:0.0018019891168471727\n",
            "train loss:0.0021560936928073375\n",
            "train loss:0.0006754871156138375\n",
            "train loss:0.002624781365820861\n",
            "train loss:0.0063591804055691935\n",
            "train loss:0.0011661153727051323\n",
            "train loss:0.0015756653629842362\n",
            "train loss:0.0011666102945403333\n",
            "train loss:0.007931313447037758\n",
            "train loss:0.001172473975679434\n",
            "train loss:0.0015533936665125569\n",
            "train loss:0.0008982544914937303\n",
            "train loss:0.00022272269711133875\n",
            "train loss:0.012959798923655765\n",
            "train loss:0.002328867187378017\n",
            "train loss:0.0008896557769132886\n",
            "train loss:0.0016481695438370489\n",
            "train loss:0.00258778196869941\n",
            "train loss:0.0019546722159499968\n",
            "train loss:0.0018392536197101086\n",
            "train loss:0.0005862659424197671\n",
            "train loss:0.004242462435819727\n",
            "train loss:0.0010746038699700686\n",
            "train loss:0.0006916404643772607\n",
            "train loss:0.01592162045952134\n",
            "train loss:0.007355750522077035\n",
            "train loss:0.009851202878637645\n",
            "train loss:0.000675043672634381\n",
            "train loss:0.00038536308364721286\n",
            "train loss:0.0019730985460062332\n",
            "train loss:0.035359488462076136\n",
            "train loss:0.0032838169272459894\n",
            "train loss:0.03175862898038083\n",
            "train loss:0.015905285332536278\n",
            "train loss:0.0011126623836696988\n",
            "train loss:0.0022868514779627908\n",
            "train loss:0.0009050704835597402\n",
            "train loss:0.0016919899266351368\n",
            "train loss:0.001092813088388253\n",
            "train loss:0.013836221661174843\n",
            "train loss:0.0020038500825091078\n",
            "train loss:0.0049529125074682\n",
            "train loss:0.008200927045721186\n",
            "train loss:0.0023040072394106136\n",
            "train loss:0.0001780593997513135\n",
            "train loss:0.0035670576748027157\n",
            "train loss:0.0006548062308236166\n",
            "train loss:0.0021969467778848364\n",
            "train loss:0.0007119998725479981\n",
            "train loss:0.009252851035289217\n",
            "train loss:0.002724891233143055\n",
            "train loss:0.006399818658595169\n",
            "train loss:0.006253185739098219\n",
            "train loss:0.0020333552250965427\n",
            "train loss:0.02008991098689718\n",
            "train loss:0.0033606438082471705\n",
            "train loss:0.011595085679061692\n",
            "train loss:0.012603098022602375\n",
            "train loss:0.0009224610131319369\n",
            "train loss:0.0005467159901535054\n",
            "train loss:0.0014754153995978742\n",
            "train loss:0.0038873659009621433\n",
            "train loss:0.0035551417177082946\n",
            "train loss:0.0022375293304477584\n",
            "train loss:0.0019507026109066164\n",
            "train loss:0.0014472643498349264\n",
            "train loss:0.0034105594859949596\n",
            "train loss:0.0016009265291912428\n",
            "train loss:0.003166696625700059\n",
            "train loss:0.0036920599036416317\n",
            "train loss:0.00251749281045255\n",
            "train loss:0.002613990014269667\n",
            "train loss:0.005370269866422267\n",
            "train loss:0.010221251349381157\n",
            "train loss:0.0027657137450187784\n",
            "train loss:0.004633925040747889\n",
            "train loss:0.003737343862561207\n",
            "train loss:0.0010755471193734682\n",
            "train loss:0.0033355899093944884\n",
            "train loss:0.0012911188250913945\n",
            "train loss:0.001660439428181861\n",
            "train loss:0.00018916417747479348\n",
            "train loss:0.01114582357702277\n",
            "train loss:0.0035289838003583224\n",
            "train loss:0.012415906859082625\n",
            "train loss:0.006371290836175469\n",
            "train loss:0.004467063274628175\n",
            "train loss:0.004588429669663978\n",
            "train loss:0.003433887243248686\n",
            "train loss:0.0001802921682954637\n",
            "train loss:0.0016571086015800011\n",
            "train loss:0.0019361077854558625\n",
            "train loss:0.0019006238006503877\n",
            "train loss:0.0015421674010313623\n",
            "train loss:0.005300698172883951\n",
            "train loss:0.0018033403123152683\n",
            "train loss:0.018949502266082247\n",
            "train loss:0.000560467928874168\n",
            "train loss:0.00857377493816851\n",
            "train loss:0.0027145929988535347\n",
            "train loss:0.0018447435161689952\n",
            "train loss:0.0017763943614786826\n",
            "train loss:0.0007374329623812305\n",
            "train loss:0.0029818212198667836\n",
            "train loss:5.0681410670545216e-05\n",
            "train loss:0.0035019074480490842\n",
            "train loss:0.0016710294308922432\n",
            "train loss:0.0007667809059151623\n",
            "train loss:0.003863813403770138\n",
            "train loss:0.0055560213949055505\n",
            "train loss:0.003832938354761086\n",
            "train loss:0.0008417269397857038\n",
            "train loss:0.017411089682619448\n",
            "train loss:0.0087935107295964\n",
            "train loss:0.00019075581732670076\n",
            "train loss:0.0001798721882671405\n",
            "train loss:0.0013666793081861862\n",
            "train loss:0.002088177141112136\n",
            "train loss:0.002011849848702774\n",
            "train loss:0.0029665772005465535\n",
            "train loss:0.0003388353655691637\n",
            "train loss:0.0005368839773447208\n",
            "train loss:0.0010269315322450042\n",
            "train loss:0.0009162485347216003\n",
            "train loss:0.002899593023476818\n",
            "train loss:0.0010291825819977772\n",
            "train loss:0.0020031246889951008\n",
            "train loss:0.003567313040101464\n",
            "train loss:0.002119494256649867\n",
            "train loss:0.0021963398234932712\n",
            "train loss:0.0030130662792252653\n",
            "train loss:0.0012820087285904567\n",
            "train loss:0.0015322719180888974\n",
            "train loss:0.004042598662411294\n",
            "train loss:0.0022894571946129446\n",
            "train loss:0.0005606487560340818\n",
            "train loss:0.0009142779765358222\n",
            "train loss:0.00010510420476999884\n",
            "train loss:0.01912290060058729\n",
            "train loss:0.0007153269658980665\n",
            "train loss:0.004307119293798156\n",
            "train loss:0.005936311365149055\n",
            "train loss:0.0006669798199202438\n",
            "train loss:0.000521689721978298\n",
            "train loss:0.03997862991849053\n",
            "train loss:0.006216764859904625\n",
            "train loss:0.000918251534322782\n",
            "train loss:0.002669570336251509\n",
            "train loss:0.0009697406451762856\n",
            "train loss:0.0017983200793233053\n",
            "train loss:0.005465193896005575\n",
            "train loss:0.0025633339479650863\n",
            "train loss:0.005738086308818164\n",
            "train loss:0.011014590928640544\n",
            "train loss:0.0006124201657712494\n",
            "train loss:0.0026163768090333096\n",
            "train loss:0.008321290421215426\n",
            "train loss:0.017948669657213746\n",
            "train loss:0.007127904460640203\n",
            "train loss:0.024005159833049197\n",
            "train loss:0.004181269213730613\n",
            "train loss:0.00035658411952894925\n",
            "train loss:0.0014961221275891043\n",
            "train loss:0.001942137680126959\n",
            "train loss:0.0017746243503785242\n",
            "train loss:0.0056748636826948196\n",
            "train loss:0.014660480290685276\n",
            "train loss:0.003120861724864138\n",
            "train loss:0.008162588945070721\n",
            "train loss:0.0008084595862221437\n",
            "train loss:0.005890434827878166\n",
            "train loss:0.002022835068139289\n",
            "train loss:0.0006616990871064697\n",
            "train loss:0.015764272364784525\n",
            "train loss:0.0004299412905448498\n",
            "train loss:0.0005385112872730159\n",
            "train loss:0.0012992753859078611\n",
            "train loss:0.006079438255413008\n",
            "train loss:0.016122395300722892\n",
            "train loss:0.0005627391759496697\n",
            "train loss:0.0013573180091545059\n",
            "train loss:0.0005019137732020181\n",
            "train loss:0.0007310422812914376\n",
            "train loss:0.002711186207000015\n",
            "train loss:0.035199225975372166\n",
            "train loss:0.05933078629484109\n",
            "train loss:0.003746573114437913\n",
            "train loss:0.0030345221893377473\n",
            "train loss:0.0013323299777926088\n",
            "train loss:0.001322636999906156\n",
            "train loss:0.003333633450009732\n",
            "train loss:0.0021536661997406436\n",
            "train loss:0.004483490526725305\n",
            "train loss:0.0008370026621306497\n",
            "train loss:0.0017625002449922428\n",
            "train loss:0.0017486251838833408\n",
            "train loss:0.006894609065532676\n",
            "train loss:0.003430790672543716\n",
            "train loss:0.0009510343639702796\n",
            "train loss:0.0007944807057994925\n",
            "train loss:0.0007923482033539568\n",
            "train loss:0.008322773054812875\n",
            "train loss:0.003382368345278194\n",
            "train loss:0.0037314082229761235\n",
            "train loss:0.0010958692193234836\n",
            "train loss:0.007720800876516773\n",
            "train loss:0.0015917034353129618\n",
            "train loss:0.0008190975995602402\n",
            "train loss:0.0180705923145789\n",
            "train loss:0.003403071330614064\n",
            "train loss:0.004489005038317632\n",
            "train loss:0.015474125466990755\n",
            "train loss:0.010388358547455472\n",
            "train loss:0.006480125825317915\n",
            "train loss:0.017330565247658806\n",
            "train loss:0.0020060806627645662\n",
            "train loss:0.0006389654461854524\n",
            "train loss:0.003279270868154204\n",
            "train loss:0.0015182760696296335\n",
            "train loss:0.003531900568770656\n",
            "train loss:0.0008185702703679396\n",
            "train loss:0.002849119565319092\n",
            "train loss:0.0048698753440456245\n",
            "train loss:0.00022616998821775347\n",
            "train loss:0.003469522352340864\n",
            "train loss:0.004612837301282341\n",
            "train loss:0.00222548933561283\n",
            "train loss:0.001780899428063241\n",
            "train loss:0.0008407578011373806\n",
            "train loss:0.0024232281686853594\n",
            "train loss:0.006461311638198932\n",
            "train loss:0.002373898561090961\n",
            "train loss:0.00603582755998315\n",
            "train loss:0.004229063345978455\n",
            "train loss:0.0001490643249198601\n",
            "train loss:0.003982840354430315\n",
            "train loss:0.0008931672391938406\n",
            "train loss:0.0037105974465370716\n",
            "train loss:0.005938743409246136\n",
            "train loss:0.00019320030742165674\n",
            "train loss:0.0014100938034880914\n",
            "train loss:0.0006342875749055588\n",
            "train loss:0.0017339555063217702\n",
            "train loss:0.004813409478972119\n",
            "train loss:0.0059927757203002445\n",
            "train loss:0.005073470446744238\n",
            "train loss:0.001204507335584438\n",
            "train loss:0.0009485236021080903\n",
            "train loss:0.0005517687737823247\n",
            "train loss:0.0005187239347667085\n",
            "train loss:0.0014522061819154518\n",
            "train loss:0.004680125623341999\n",
            "train loss:0.0010097915270833252\n",
            "train loss:0.007842889133323\n",
            "train loss:0.0014261854555336012\n",
            "train loss:0.0013968436017742988\n",
            "train loss:0.004901517031986308\n",
            "train loss:0.0022168097132915505\n",
            "train loss:0.00481866841772964\n",
            "train loss:0.0035210079769025333\n",
            "train loss:0.007570081521598124\n",
            "train loss:0.0038114542326641403\n",
            "train loss:0.006796509529433836\n",
            "train loss:0.0005744684660237587\n",
            "train loss:0.005187865013431479\n",
            "train loss:0.0009731793612324811\n",
            "train loss:0.0027939801959010906\n",
            "train loss:0.003954350183552267\n",
            "train loss:0.002089961539170297\n",
            "train loss:0.005375936106269139\n",
            "train loss:0.005089481963659901\n",
            "train loss:0.0021719002707798175\n",
            "train loss:0.0008246295169746027\n",
            "train loss:0.0004916459826252715\n",
            "train loss:0.0015961763720083039\n",
            "train loss:0.001137259835814524\n",
            "train loss:0.002654331783664123\n",
            "train loss:0.00492946863270831\n",
            "train loss:0.0076684596839069604\n",
            "train loss:0.0018761484507143094\n",
            "train loss:0.0026153895249069487\n",
            "train loss:0.00048715689122238835\n",
            "train loss:0.002157309744419651\n",
            "train loss:0.12017375741665315\n",
            "train loss:0.00741757220153689\n",
            "train loss:0.005324680573087111\n",
            "train loss:0.00025926695384681926\n",
            "train loss:0.004319052405594412\n",
            "train loss:0.00478692401458724\n",
            "train loss:0.009327405623647525\n",
            "train loss:0.014284847763589013\n",
            "train loss:0.013433396806710539\n",
            "train loss:0.021586082644937533\n",
            "train loss:0.003786455482170054\n",
            "train loss:0.01544223975784599\n",
            "train loss:0.0015443025715386899\n",
            "train loss:0.003877626419676901\n",
            "train loss:0.0014345142475155484\n",
            "train loss:0.029821124556780498\n",
            "train loss:0.006463087259134653\n",
            "train loss:0.006955979361843688\n",
            "train loss:0.02003494966954808\n",
            "train loss:0.006070643588818795\n",
            "train loss:0.0018711699895612997\n",
            "train loss:0.0029604478702877175\n",
            "train loss:0.0015819284994925311\n",
            "train loss:0.0034917786335593374\n",
            "train loss:0.0022788523094093905\n",
            "train loss:0.005082839828609475\n",
            "train loss:0.0010739575497536575\n",
            "train loss:0.0007268399118776487\n",
            "train loss:0.0034418085249860913\n",
            "train loss:0.0004115151116599096\n",
            "=== epoch:15, train acc:0.999, test acc:0.988 ===\n",
            "train loss:0.006187839999404747\n",
            "train loss:0.004804982419247256\n",
            "train loss:0.006702654170810246\n",
            "train loss:0.0024468928703437966\n",
            "train loss:0.0044004926530829104\n",
            "train loss:0.004669091675756232\n",
            "train loss:0.007091821662509289\n",
            "train loss:0.00023447286473462202\n",
            "train loss:0.004514252344862935\n",
            "train loss:0.0019842375968421692\n",
            "train loss:0.00458944201873001\n",
            "train loss:0.0025889709456310196\n",
            "train loss:0.00030599360569078313\n",
            "train loss:0.002687903150885563\n",
            "train loss:0.0002499270333795918\n",
            "train loss:0.0011830213077701596\n",
            "train loss:0.0014944556172429833\n",
            "train loss:0.008706524570886045\n",
            "train loss:0.0021376587078904283\n",
            "train loss:0.0006171580916218839\n",
            "train loss:0.0010579736137422344\n",
            "train loss:0.0022731366713952404\n",
            "train loss:0.0009470015879707092\n",
            "train loss:0.00021674356836500806\n",
            "train loss:0.0022101440846270383\n",
            "train loss:0.008918260120922493\n",
            "train loss:0.0016263953766110027\n",
            "train loss:0.0013331662396056335\n",
            "train loss:0.0023542464997661963\n",
            "train loss:0.004538725972383339\n",
            "train loss:0.003929028537708426\n",
            "train loss:0.004066208918631547\n",
            "train loss:0.012061704101205566\n",
            "train loss:0.0004348156074818521\n",
            "train loss:0.002309189416150737\n",
            "train loss:0.008534367331985064\n",
            "train loss:0.0010666244741729999\n",
            "train loss:0.0013493450450826174\n",
            "train loss:0.001981720194093114\n",
            "train loss:0.03471145253612692\n",
            "train loss:0.007473902306638568\n",
            "train loss:0.003452067222045741\n",
            "train loss:0.0007072937062607253\n",
            "train loss:0.001007498460444603\n",
            "train loss:0.007850170391274204\n",
            "train loss:0.0007523963177770232\n",
            "train loss:0.0008288634918244171\n",
            "train loss:0.0017381370040753697\n",
            "train loss:0.0008369121199207148\n",
            "train loss:0.00026245497224210954\n",
            "train loss:0.004127178959698567\n",
            "train loss:0.0001777327296641172\n",
            "train loss:0.0006103307499551087\n",
            "train loss:0.0008326137922605281\n",
            "train loss:0.0005577735318620444\n",
            "train loss:0.004802943177899935\n",
            "train loss:0.007609935187960874\n",
            "train loss:0.000518388447248733\n",
            "train loss:0.003313707307045725\n",
            "train loss:0.0009087540307533342\n",
            "train loss:0.0029380615206696103\n",
            "train loss:0.002637330154174281\n",
            "train loss:0.006013761409914992\n",
            "train loss:0.0005616540905295596\n",
            "train loss:0.0026336617905088964\n",
            "train loss:0.0011717478537585525\n",
            "train loss:0.001113293398508108\n",
            "train loss:0.0013372199937277745\n",
            "train loss:0.011686775054019875\n",
            "train loss:0.0014078807733400964\n",
            "train loss:0.0012578253333346496\n",
            "train loss:0.00040316035115749973\n",
            "train loss:0.0015389148401950233\n",
            "train loss:0.0024466812601874554\n",
            "train loss:0.002498906431580024\n",
            "train loss:0.007233113927057048\n",
            "train loss:0.0018563670966262502\n",
            "train loss:0.0021354206390875106\n",
            "train loss:0.006900466863249454\n",
            "train loss:0.00011886375489280829\n",
            "train loss:0.0004986542736415999\n",
            "train loss:0.0014440398337335297\n",
            "train loss:0.0036799266772215788\n",
            "train loss:0.0011916191369723808\n",
            "train loss:0.005449082461728652\n",
            "train loss:0.008838438519448254\n",
            "train loss:0.00032435166244232936\n",
            "train loss:0.001285626478150573\n",
            "train loss:0.00016572832954381611\n",
            "train loss:0.002963867246479784\n",
            "train loss:0.0001645292369372199\n",
            "train loss:0.0006698701570317905\n",
            "train loss:0.006568805419880645\n",
            "train loss:0.0006777232495407434\n",
            "train loss:0.013508308473154556\n",
            "train loss:0.0006754012422786827\n",
            "train loss:0.0002753401772083986\n",
            "train loss:0.003954791952734954\n",
            "train loss:0.002369731497894071\n",
            "train loss:0.0011310704363713603\n",
            "train loss:0.0007726253630246367\n",
            "train loss:0.000596861237339808\n",
            "train loss:0.0008504553630548353\n",
            "train loss:0.0013067386509742467\n",
            "train loss:0.004818486309265091\n",
            "train loss:0.016381148677716444\n",
            "train loss:0.003151000160797371\n",
            "train loss:0.004846472700411481\n",
            "train loss:0.00011719987342788984\n",
            "train loss:0.0012793199894873383\n",
            "train loss:0.0019945468732491417\n",
            "train loss:0.00028852482468755974\n",
            "train loss:0.0017282182261084472\n",
            "train loss:0.0011189914490727672\n",
            "train loss:0.006889797475698392\n",
            "train loss:0.008428335556388618\n",
            "train loss:0.000773514231007472\n",
            "train loss:0.0008738605586017071\n",
            "train loss:0.00039670633511811005\n",
            "train loss:0.008709515565584344\n",
            "train loss:0.007261158139973235\n",
            "train loss:0.0014237991841279318\n",
            "train loss:0.00021572612897947992\n",
            "train loss:0.0036297302877219967\n",
            "train loss:0.0004965071780371794\n",
            "train loss:0.0011658265739434607\n",
            "train loss:0.013609941817293943\n",
            "train loss:0.0011062743023861283\n",
            "train loss:0.0007253200740652465\n",
            "train loss:0.002131873320617686\n",
            "train loss:0.0007527604708469558\n",
            "train loss:0.002413583413609516\n",
            "train loss:0.001877960327609379\n",
            "train loss:0.004637941296105422\n",
            "train loss:0.0029052787432920597\n",
            "train loss:0.00789498151721783\n",
            "train loss:0.0024136901395054432\n",
            "train loss:0.0027182459164145807\n",
            "train loss:0.00034260982105043496\n",
            "train loss:0.011850731506105279\n",
            "train loss:0.00022372207513816835\n",
            "train loss:0.0001166743254745436\n",
            "train loss:0.004208980267384811\n",
            "train loss:0.0008797912308855218\n",
            "train loss:0.0003466507135565493\n",
            "train loss:0.0020654464258168786\n",
            "train loss:0.0007094640765145182\n",
            "train loss:0.007297191761839024\n",
            "train loss:7.992512080333167e-05\n",
            "train loss:0.0009334095681349141\n",
            "train loss:0.0006699910560889629\n",
            "train loss:0.00035880776832117567\n",
            "train loss:0.0006151471566147571\n",
            "train loss:0.001546275875473588\n",
            "train loss:0.0018030412333952714\n",
            "train loss:0.003349591088885692\n",
            "train loss:0.0011122860642896707\n",
            "train loss:0.0011228420396792282\n",
            "train loss:0.0007295488235491757\n",
            "train loss:0.00014814324728358043\n",
            "train loss:0.000855002331427055\n",
            "train loss:0.0006307355018153194\n",
            "train loss:0.00018048199658468364\n",
            "train loss:0.0015463642929844027\n",
            "train loss:0.0019635136701813298\n",
            "train loss:0.0011515654196093432\n",
            "train loss:0.004488114927835709\n",
            "train loss:0.0004289930722350811\n",
            "train loss:0.00435693789878007\n",
            "train loss:0.005284368221481685\n",
            "train loss:0.0016333327072703102\n",
            "train loss:0.01629859284334219\n",
            "train loss:0.001830545716842564\n",
            "train loss:0.002296730735699659\n",
            "train loss:0.0028693514809442298\n",
            "train loss:0.0018215902128535786\n",
            "train loss:0.0016634394096100567\n",
            "train loss:0.001579821173468075\n",
            "train loss:0.00022364183410646598\n",
            "train loss:0.0009820432125999205\n",
            "train loss:0.0041321794166497075\n",
            "train loss:0.0034198451327054923\n",
            "train loss:0.00046596936466245897\n",
            "train loss:0.0016271382966013207\n",
            "train loss:0.0006126009046850355\n",
            "train loss:0.00015386063515200079\n",
            "train loss:0.0012709082115395414\n",
            "train loss:0.0013504710154904648\n",
            "train loss:0.002054055286731116\n",
            "train loss:0.0011495140716606819\n",
            "train loss:0.0050540177116835975\n",
            "train loss:0.0026868155611857636\n",
            "train loss:0.000830036703766878\n",
            "train loss:0.0037162385918073208\n",
            "train loss:0.0004618021911328388\n",
            "train loss:0.00499603974660317\n",
            "train loss:0.0010554861773632896\n",
            "train loss:0.0005402319272447089\n",
            "train loss:0.0008646257074078905\n",
            "train loss:0.0004579117778784934\n",
            "train loss:0.0006814648144741475\n",
            "train loss:0.004106234802344341\n",
            "train loss:3.0819581131260576e-05\n",
            "train loss:0.0013715513129022626\n",
            "train loss:0.00023499349552890403\n",
            "train loss:0.004159903664517275\n",
            "train loss:0.0047676300589608025\n",
            "train loss:9.901835248332709e-05\n",
            "train loss:0.00018989813745502032\n",
            "train loss:0.0011568809694108674\n",
            "train loss:0.0016010131441331604\n",
            "train loss:0.0006527531646271631\n",
            "train loss:0.0003451242485723438\n",
            "train loss:0.0026138035324036364\n",
            "train loss:0.00020984686271865798\n",
            "train loss:0.0024566699776314162\n",
            "train loss:0.00018717502737821857\n",
            "train loss:0.00062511233277013\n",
            "train loss:0.0018400795170289625\n",
            "train loss:0.0005408572424803383\n",
            "train loss:0.0012516033879646077\n",
            "train loss:0.00025930600331747737\n",
            "train loss:0.00020648789182667682\n",
            "train loss:0.001186226692310609\n",
            "train loss:0.001746146141507778\n",
            "train loss:0.000730713491254876\n",
            "train loss:0.0009927846999464726\n",
            "train loss:0.0024883834351794847\n",
            "train loss:2.279120382609089e-05\n",
            "train loss:0.003059018000078172\n",
            "train loss:0.0017388429842463264\n",
            "train loss:0.0009054759170976204\n",
            "train loss:0.0013133997779877682\n",
            "train loss:0.0036518593108503007\n",
            "train loss:0.00041156096439049004\n",
            "train loss:0.002430805734906438\n",
            "train loss:0.0002270652356939554\n",
            "train loss:0.0009452513751067528\n",
            "train loss:0.004364085139090767\n",
            "train loss:0.00013612519349300538\n",
            "train loss:0.0004347135196278293\n",
            "train loss:0.0013944320848903395\n",
            "train loss:0.0012853645449262174\n",
            "train loss:0.0008292185405417636\n",
            "train loss:0.0007472845707071559\n",
            "train loss:0.001664922226262028\n",
            "train loss:0.0015285266636815892\n",
            "train loss:0.0035357967631485594\n",
            "train loss:0.0009165813548364163\n",
            "train loss:0.0005894997672305714\n",
            "train loss:0.00019082674985061103\n",
            "train loss:5.1971331339734256e-05\n",
            "train loss:0.0004823314072697594\n",
            "train loss:0.00047639507420764975\n",
            "train loss:0.0014856927293862277\n",
            "train loss:0.0007682987051810878\n",
            "train loss:0.0023797919625453408\n",
            "train loss:0.004215338864843822\n",
            "train loss:0.002421815391560721\n",
            "train loss:0.0025835055368769494\n",
            "train loss:0.0011110329143961808\n",
            "train loss:0.001102137399429574\n",
            "train loss:0.00020165162284979828\n",
            "train loss:0.0018177872140220743\n",
            "train loss:0.002381617549996531\n",
            "train loss:0.003377911429047476\n",
            "train loss:0.0037990152068499085\n",
            "train loss:0.0009745156067360093\n",
            "train loss:0.000168358053820376\n",
            "train loss:0.0002646320378109427\n",
            "train loss:0.0009158658606243018\n",
            "train loss:0.00032109145598219224\n",
            "train loss:0.001488182450168429\n",
            "train loss:0.0004462453470135178\n",
            "train loss:0.0008124132492016698\n",
            "train loss:0.0033636189234077293\n",
            "train loss:0.0012157188332856958\n",
            "train loss:0.00033150802680387585\n",
            "train loss:0.0014477211004426352\n",
            "train loss:0.0005238099921078833\n",
            "train loss:0.0008088252517877297\n",
            "train loss:0.0008163800445856118\n",
            "train loss:0.00029197386932799816\n",
            "train loss:0.0013083899345897021\n",
            "train loss:0.00014772611545787437\n",
            "train loss:0.003872085060349845\n",
            "train loss:0.0005272465147360889\n",
            "train loss:0.0002399533117793468\n",
            "train loss:0.0005714734440814351\n",
            "train loss:0.001005783960094643\n",
            "train loss:0.00015581167112205713\n",
            "train loss:0.0014560799260583293\n",
            "train loss:0.00017205795531501588\n",
            "train loss:0.002233810443771982\n",
            "train loss:0.0003244942631992681\n",
            "train loss:0.0005354976662109365\n",
            "train loss:0.002230344557483114\n",
            "train loss:0.0008346239433939948\n",
            "train loss:2.427779547550829e-05\n",
            "train loss:0.0010453130859790036\n",
            "train loss:0.0017280404300386134\n",
            "train loss:0.0008838676852632714\n",
            "train loss:0.0007390999657775533\n",
            "train loss:0.0009925804382776814\n",
            "train loss:0.00011054757691141722\n",
            "train loss:0.00040188969841553987\n",
            "train loss:0.0006730459471224992\n",
            "train loss:0.0010752827482317028\n",
            "train loss:0.00011523865793236745\n",
            "train loss:0.0006883620924537629\n",
            "train loss:0.00017250531210241907\n",
            "train loss:0.0008501893498501737\n",
            "train loss:0.0004510386042994795\n",
            "train loss:0.004238424802093639\n",
            "train loss:0.0014794822966410163\n",
            "train loss:5.796364872329984e-05\n",
            "train loss:4.82110947833792e-05\n",
            "train loss:0.00042539483952696374\n",
            "train loss:0.0006385172022261199\n",
            "train loss:0.000642651443563315\n",
            "train loss:0.00012163750303231645\n",
            "train loss:0.007288841395364533\n",
            "train loss:0.0003553310824130344\n",
            "train loss:0.01708127443312958\n",
            "train loss:0.0003807494084634178\n",
            "train loss:0.00019697918768473062\n",
            "train loss:0.0001229819880202496\n",
            "train loss:0.005565842732977917\n",
            "train loss:0.0005738168789899958\n",
            "train loss:0.009006459145074407\n",
            "train loss:8.060387488741803e-05\n",
            "train loss:0.0007247576542216435\n",
            "train loss:0.05069031567911386\n",
            "train loss:0.0015319155239612884\n",
            "train loss:0.00022599637991194753\n",
            "train loss:0.0033691829936851212\n",
            "train loss:0.024033628156387897\n",
            "train loss:0.002542811389275891\n",
            "train loss:0.000572289028915975\n",
            "train loss:0.0013461359948704085\n",
            "train loss:0.0008820129422314415\n",
            "train loss:0.0017966883235379478\n",
            "train loss:0.00010372556083933373\n",
            "train loss:0.0009594880998887082\n",
            "train loss:0.00024081124712831457\n",
            "train loss:0.0002274115038922547\n",
            "train loss:0.0002737158230357235\n",
            "train loss:6.353943259829962e-05\n",
            "train loss:0.0007705291368441735\n",
            "train loss:0.0031237844171231655\n",
            "train loss:0.00010190173698672483\n",
            "train loss:0.0003391466794108264\n",
            "train loss:0.0013978947903241954\n",
            "train loss:0.00011402354111869449\n",
            "train loss:0.014567762959280663\n",
            "train loss:0.0006738471590424294\n",
            "train loss:0.0002826281432093701\n",
            "train loss:0.0008152556257110645\n",
            "train loss:0.0019815497212902007\n",
            "train loss:0.0005235888401209647\n",
            "train loss:0.0010327561932262064\n",
            "train loss:0.0008918852832040212\n",
            "train loss:0.001852690026405398\n",
            "train loss:0.000359836767687258\n",
            "train loss:0.0012450365560600172\n",
            "train loss:0.0010423578516834905\n",
            "train loss:0.0010525273405810696\n",
            "train loss:0.0006151798179040195\n",
            "train loss:0.001761251549397422\n",
            "train loss:0.0030151967718777878\n",
            "train loss:0.0005709627242287074\n",
            "train loss:0.00184560751051521\n",
            "train loss:0.003242459632070438\n",
            "train loss:0.001901883631645858\n",
            "train loss:5.408682591329324e-05\n",
            "train loss:0.0003152204019611814\n",
            "train loss:0.01069011155540588\n",
            "train loss:0.006920629288131819\n",
            "train loss:0.0004613763794661983\n",
            "train loss:0.004334319416654533\n",
            "train loss:0.01898352223197329\n",
            "train loss:0.00044741006299160296\n",
            "train loss:0.00013894427592580673\n",
            "train loss:0.0004498820272366288\n",
            "train loss:0.0008937873356656596\n",
            "train loss:0.003358964264682736\n",
            "train loss:0.004020164116369152\n",
            "train loss:0.0056503460275736305\n",
            "train loss:0.00011404100073529403\n",
            "train loss:0.0021703834496964993\n",
            "train loss:0.006351993646952035\n",
            "train loss:0.001487963572684729\n",
            "train loss:0.0006982152306307505\n",
            "train loss:0.00532095521384026\n",
            "train loss:0.0015277967424959487\n",
            "train loss:0.0010865904631800608\n",
            "train loss:0.002740652928552217\n",
            "train loss:0.00040275281379415065\n",
            "train loss:0.002831806003026681\n",
            "train loss:0.0013021145522931449\n",
            "train loss:0.017551888839340515\n",
            "train loss:0.001997292467319621\n",
            "train loss:0.001461623586168736\n",
            "train loss:0.0002784921227917311\n",
            "train loss:0.00023415091598761662\n",
            "train loss:0.0008712590190337184\n",
            "train loss:0.0022414681843289444\n",
            "train loss:0.0018344544482735224\n",
            "train loss:0.0013647938870666048\n",
            "train loss:0.00012789615731627165\n",
            "train loss:0.00011944792716367937\n",
            "train loss:0.0009836121196103735\n",
            "train loss:0.0020103781608519877\n",
            "train loss:0.00017934566313343263\n",
            "train loss:0.0001954734605151976\n",
            "train loss:0.00010180253401873269\n",
            "train loss:0.0006523256762544286\n",
            "train loss:0.0014882977179930359\n",
            "train loss:0.0015408716616184298\n",
            "train loss:0.0006136054726355618\n",
            "train loss:0.0002719278538393623\n",
            "train loss:0.0011129116587360625\n",
            "train loss:0.0029834451867998095\n",
            "train loss:0.020113083529729278\n",
            "train loss:0.0003303335326308721\n",
            "train loss:0.012361328533210312\n",
            "train loss:0.001008576654042816\n",
            "train loss:0.000776571267091305\n",
            "train loss:0.002716879895352565\n",
            "train loss:0.0010472801578325234\n",
            "train loss:0.0006161735156531855\n",
            "train loss:0.0034748498820387002\n",
            "train loss:0.001774745556901097\n",
            "train loss:0.0014489902926948996\n",
            "train loss:0.002372291753689805\n",
            "train loss:0.000411672702576651\n",
            "train loss:0.0023767151232161874\n",
            "train loss:0.002511805496715472\n",
            "train loss:0.0012182243493965993\n",
            "train loss:0.0014801122749874534\n",
            "train loss:0.0009326463290832026\n",
            "train loss:0.0043811105990939365\n",
            "train loss:0.0004494505163862619\n",
            "train loss:0.0006284369924223647\n",
            "train loss:0.00039614148368533693\n",
            "train loss:0.001113434953018195\n",
            "train loss:0.001400298401725506\n",
            "train loss:0.0011920139190566231\n",
            "train loss:0.0005604335194393615\n",
            "train loss:0.006919603271281688\n",
            "train loss:0.0008039728835452953\n",
            "train loss:0.0004323473753336742\n",
            "train loss:0.00047482776392293596\n",
            "train loss:0.0018337875072604452\n",
            "train loss:0.00020206635344211535\n",
            "train loss:0.0005695768079184367\n",
            "train loss:0.005796373966484455\n",
            "train loss:0.0026044889713172244\n",
            "train loss:0.00452483670412227\n",
            "train loss:0.0055019016595469475\n",
            "train loss:0.0006405961018569827\n",
            "train loss:0.000986815313331815\n",
            "train loss:0.003882832065267151\n",
            "train loss:7.60276631376582e-05\n",
            "train loss:0.00012093757962058067\n",
            "train loss:0.002900258335222299\n",
            "train loss:0.002609238426377675\n",
            "train loss:0.01220350127583691\n",
            "train loss:0.006205032706595572\n",
            "train loss:0.0006045058242640286\n",
            "train loss:0.006332731442490853\n",
            "train loss:0.0012700811736249126\n",
            "train loss:0.0008423188367619842\n",
            "train loss:0.0020091776727439877\n",
            "train loss:0.001381651559697781\n",
            "train loss:0.00014893438783801892\n",
            "train loss:0.0006880582200223554\n",
            "train loss:0.0012903461638759947\n",
            "train loss:0.0006091139675729431\n",
            "train loss:0.001246902121351298\n",
            "train loss:0.001976890703137668\n",
            "train loss:0.0012074727980682428\n",
            "train loss:0.004774522927910236\n",
            "train loss:0.00048758743022258937\n",
            "train loss:0.0008451460955999124\n",
            "train loss:7.817429440886292e-05\n",
            "train loss:0.001642434706746369\n",
            "train loss:0.0025903643924659377\n",
            "train loss:0.0026549775977643152\n",
            "train loss:0.0005226149094200973\n",
            "train loss:0.0014038308127375574\n",
            "train loss:0.002622407864921838\n",
            "train loss:0.0012124130776359236\n",
            "train loss:0.0038557701107620984\n",
            "train loss:0.0004214868623415253\n",
            "train loss:0.0015719189735254521\n",
            "train loss:0.0036588902264263245\n",
            "train loss:0.004099509793782115\n",
            "train loss:0.019714305912489752\n",
            "train loss:0.00300676434637375\n",
            "train loss:0.0014086819423678445\n",
            "train loss:0.001600310873009701\n",
            "train loss:0.0018164464082956672\n",
            "train loss:0.0011477518556166691\n",
            "train loss:0.0011685395066219322\n",
            "train loss:0.0023945225975901914\n",
            "train loss:0.0002746476534644432\n",
            "train loss:0.0028105968626361798\n",
            "train loss:0.0007247123046690744\n",
            "train loss:0.008862316892891\n",
            "train loss:0.0006283038952032813\n",
            "train loss:0.004019608424616124\n",
            "train loss:0.0004480386036741874\n",
            "train loss:0.02962348468177461\n",
            "train loss:0.0029937263489422813\n",
            "train loss:0.0006058007735007592\n",
            "train loss:0.0011337455399328589\n",
            "train loss:0.02291327627881718\n",
            "train loss:0.001838295362840105\n",
            "train loss:0.0052516199557388924\n",
            "train loss:0.006009401627074448\n",
            "train loss:0.020609225624108097\n",
            "train loss:0.0014598616633251212\n",
            "train loss:0.0037685231388833672\n",
            "train loss:0.0009584755390895317\n",
            "train loss:0.00017350125336275377\n",
            "train loss:0.0014282527924347626\n",
            "train loss:0.011560257811716277\n",
            "train loss:0.0030891001675786467\n",
            "train loss:0.003821012409523638\n",
            "train loss:0.0001490610622361434\n",
            "train loss:0.0005273365917714705\n",
            "train loss:0.0006857124764222614\n",
            "train loss:0.00150584552443747\n",
            "train loss:0.0002363730195072799\n",
            "train loss:0.00029138997082988634\n",
            "train loss:0.001755397144497271\n",
            "train loss:0.0035431071061219786\n",
            "train loss:0.03894598718827547\n",
            "train loss:0.0015568174346378598\n",
            "train loss:0.004536382715205828\n",
            "train loss:0.003175119761016215\n",
            "train loss:0.0031160047074332764\n",
            "train loss:0.0005435926971922332\n",
            "train loss:0.00141787037354823\n",
            "train loss:0.00026840242380709873\n",
            "train loss:0.0035500736548514634\n",
            "train loss:0.037483354686784424\n",
            "train loss:0.0034087840425873423\n",
            "train loss:0.0025583711246925382\n",
            "train loss:0.0016514127298494807\n",
            "train loss:0.0007740239907928036\n",
            "train loss:0.001164955215299447\n",
            "train loss:0.002613072633045185\n",
            "train loss:0.0002642228611645059\n",
            "train loss:0.013810259168822132\n",
            "train loss:0.006733166753796102\n",
            "train loss:0.005653144971916502\n",
            "train loss:0.008208111071557769\n",
            "train loss:0.00038838606303050275\n",
            "train loss:0.003275638069981471\n",
            "train loss:0.004490020035626907\n",
            "train loss:0.003856156740492205\n",
            "train loss:0.0017505036355177392\n",
            "train loss:0.0005987597022123219\n",
            "train loss:0.0015945978182743366\n",
            "train loss:0.009594829798084464\n",
            "train loss:0.0031276186181768495\n",
            "train loss:0.004666360014950573\n",
            "train loss:0.0018752058002001865\n",
            "train loss:0.0011886336029898756\n",
            "train loss:0.014768602080527903\n",
            "train loss:0.00029781141146031383\n",
            "train loss:0.0022709844952099032\n",
            "train loss:0.007861100135046333\n",
            "train loss:0.0002656591138670304\n",
            "train loss:0.03414662279492246\n",
            "train loss:0.005530720784925492\n",
            "train loss:0.002241861189217813\n",
            "train loss:0.014495315424766328\n",
            "train loss:0.004170987848875495\n",
            "train loss:0.0017253787841006896\n",
            "train loss:0.0023243995093688826\n",
            "train loss:0.007535678372849694\n",
            "train loss:0.0008199883507708259\n",
            "train loss:0.00043615525369855904\n",
            "train loss:0.0013736525770112556\n",
            "train loss:0.012152660706560934\n",
            "train loss:0.0020208333174309458\n",
            "train loss:0.00039934170689997275\n",
            "train loss:0.001188259977291846\n",
            "train loss:0.033343210808423014\n",
            "train loss:0.00757373868218716\n",
            "train loss:0.0007283206209436527\n",
            "train loss:0.003281591644440555\n",
            "train loss:0.0012008792769968087\n",
            "train loss:0.0013577017526354585\n",
            "train loss:8.54045684941155e-05\n",
            "train loss:0.01639134179299807\n",
            "train loss:0.004891384261323724\n",
            "=== epoch:16, train acc:1.0, test acc:0.991 ===\n",
            "train loss:0.003155833908610162\n",
            "train loss:0.0017266810089296488\n",
            "train loss:0.01400190796488898\n",
            "train loss:7.946018698287715e-05\n",
            "train loss:0.010002450814116309\n",
            "train loss:0.002729518030418963\n",
            "train loss:0.012881526617115023\n",
            "train loss:0.004276100119623075\n",
            "train loss:0.0014226957659575661\n",
            "train loss:0.0021678481529343505\n",
            "train loss:0.003843336894337194\n",
            "train loss:0.0024854014044962624\n",
            "train loss:0.0008428479896505109\n",
            "train loss:0.00044768277894131827\n",
            "train loss:0.0009204785739479976\n",
            "train loss:0.0013526541596240785\n",
            "train loss:0.0012953850965748894\n",
            "train loss:0.0009356749715220325\n",
            "train loss:0.025517259180200865\n",
            "train loss:0.0012415448287237188\n",
            "train loss:0.0012468626133311038\n",
            "train loss:0.0015744277105374687\n",
            "train loss:0.0013444278349809158\n",
            "train loss:0.0010774714114563538\n",
            "train loss:0.0016704299182092174\n",
            "train loss:0.0065807162683188805\n",
            "train loss:0.0007889642649595649\n",
            "train loss:0.0013376560931260325\n",
            "train loss:0.0015936178774217072\n",
            "train loss:0.0013160426829611585\n",
            "train loss:0.003021857348286872\n",
            "train loss:0.000562031038039684\n",
            "train loss:0.0001102469400383037\n",
            "train loss:0.00033860897277755285\n",
            "train loss:0.0024991996954454006\n",
            "train loss:0.0006688283510558723\n",
            "train loss:0.00017224837878223933\n",
            "train loss:0.0009712311397131027\n",
            "train loss:0.00028091706963845575\n",
            "train loss:0.002344457311781279\n",
            "train loss:0.0025611393928042002\n",
            "train loss:0.0014726202150362792\n",
            "train loss:0.003973392722205624\n",
            "train loss:0.008321487534744853\n",
            "train loss:0.003592122222926359\n",
            "train loss:0.00022225251291876626\n",
            "train loss:0.0019360169429379044\n",
            "train loss:0.0001565793421610912\n",
            "train loss:0.0009705531084719607\n",
            "train loss:0.0007165342880554701\n",
            "train loss:0.005825088492129727\n",
            "train loss:0.004891468772652795\n",
            "train loss:0.0007786194724678351\n",
            "train loss:0.007491921187921854\n",
            "train loss:0.000656558520035818\n",
            "train loss:0.0008119419631156309\n",
            "train loss:0.002224122335367401\n",
            "train loss:0.00014208294291230184\n",
            "train loss:0.0023674961795130743\n",
            "train loss:0.0003285498503146619\n",
            "train loss:0.009248543873502511\n",
            "train loss:0.01846648656898015\n",
            "train loss:0.004791920545262358\n",
            "train loss:0.05768548674022335\n",
            "train loss:0.0017380854236482995\n",
            "train loss:0.005623951591047035\n",
            "train loss:0.005187382093152634\n",
            "train loss:0.0006584191002099736\n",
            "train loss:0.0008281147472467287\n",
            "train loss:0.003901173428353043\n",
            "train loss:0.005542495993104226\n",
            "train loss:0.005323898669678747\n",
            "train loss:0.01130529189057572\n",
            "train loss:0.001080994627538713\n",
            "train loss:0.002168525950916865\n",
            "train loss:0.0060993741141811865\n",
            "train loss:6.497517350026948e-05\n",
            "train loss:0.0027095600981959407\n",
            "train loss:0.006839425456688158\n",
            "train loss:0.0005767469079428657\n",
            "train loss:0.0019051976087575526\n",
            "train loss:0.0026701616948115697\n",
            "train loss:0.0004374829079081637\n",
            "train loss:0.0005879012981504935\n",
            "train loss:0.002038944404589428\n",
            "train loss:0.0007717683346653907\n",
            "train loss:0.007245593868752706\n",
            "train loss:0.0009488096024857056\n",
            "train loss:0.0026166371455899327\n",
            "train loss:0.004377142045696136\n",
            "train loss:0.003745820589245701\n",
            "train loss:0.022737951623411874\n",
            "train loss:0.00033062207401742395\n",
            "train loss:0.0026900895095340743\n",
            "train loss:0.030631451730082943\n",
            "train loss:0.0009645141175077674\n",
            "train loss:0.0005941643052255223\n",
            "train loss:0.001642358162240696\n",
            "train loss:0.00014411552171319977\n",
            "train loss:0.00018204974978717258\n",
            "train loss:0.005337122860550768\n",
            "train loss:0.005138144624869301\n",
            "train loss:0.004405010928664433\n",
            "train loss:0.0019884333379531407\n",
            "train loss:0.0012484053191556877\n",
            "train loss:0.005417110379452393\n",
            "train loss:0.0033954805772240123\n",
            "train loss:0.010570526760468334\n",
            "train loss:0.0025693677532169007\n",
            "train loss:0.007626274788923756\n",
            "train loss:0.0017756083248023496\n",
            "train loss:0.001317890815504288\n",
            "train loss:0.006515566518742273\n",
            "train loss:0.0019706425352468823\n",
            "train loss:0.0026509008807709513\n",
            "train loss:0.00526007791206945\n",
            "train loss:0.0065528877793949005\n",
            "train loss:0.000774367289639807\n",
            "train loss:0.000860353427153872\n",
            "train loss:0.005912121013094022\n",
            "train loss:0.0017201636469813871\n",
            "train loss:0.007965018867086665\n",
            "train loss:0.00524313620396551\n",
            "train loss:0.003926877546282412\n",
            "train loss:0.0019060972239596406\n",
            "train loss:0.0017806285875260838\n",
            "train loss:0.001642608397214084\n",
            "train loss:0.012450668043020632\n",
            "train loss:0.00876409743696329\n",
            "train loss:0.00878280495629469\n",
            "train loss:0.0005128798458351745\n",
            "train loss:0.003815872076454759\n",
            "train loss:0.001319723777166664\n",
            "train loss:0.014210639138298002\n",
            "train loss:0.00039903647888137446\n",
            "train loss:0.005713755815530974\n",
            "train loss:0.016631565431384596\n",
            "train loss:0.06774394565808041\n",
            "train loss:0.0015556011835622038\n",
            "train loss:0.004908568389680586\n",
            "train loss:0.0014429593167770696\n",
            "train loss:0.009262710623194951\n",
            "train loss:0.006351992555660684\n",
            "train loss:0.0003070141371551172\n",
            "train loss:0.0015380023076264795\n",
            "train loss:0.004900811932443222\n",
            "train loss:0.0015682354833901565\n",
            "train loss:0.017371078797068975\n",
            "train loss:0.0020207150335508663\n",
            "train loss:0.0035441074274932386\n",
            "train loss:0.008299721344980616\n",
            "train loss:0.021263055401555913\n",
            "train loss:0.004373474915292747\n",
            "train loss:0.011255119914731486\n",
            "train loss:0.0025004950916193403\n",
            "train loss:0.0029563696782507237\n",
            "train loss:0.0011937715081875473\n",
            "train loss:0.0005522970577323948\n",
            "train loss:0.004623946769107604\n",
            "train loss:0.0005401493648789277\n",
            "train loss:0.0004609412807107887\n",
            "train loss:0.0012640345053924233\n",
            "train loss:0.0063515627627716955\n",
            "train loss:0.00035981424002174057\n",
            "train loss:0.0006663001858651697\n",
            "train loss:0.010557429600277628\n",
            "train loss:0.002061346661623309\n",
            "train loss:0.009715527034494213\n",
            "train loss:0.0003738888970350218\n",
            "train loss:0.0034622130206757432\n",
            "train loss:0.0016444445812779519\n",
            "train loss:0.0027365741237967496\n",
            "train loss:0.0006323662282061765\n",
            "train loss:0.0003617740917743549\n",
            "train loss:0.0005296428771131484\n",
            "train loss:0.001805143157196303\n",
            "train loss:0.0024317269058701713\n",
            "train loss:0.0010091623265754137\n",
            "train loss:0.0026460775475854715\n",
            "train loss:0.0003431183155471983\n",
            "train loss:0.0019315902161423024\n",
            "train loss:0.001123238939839924\n",
            "train loss:0.0062703640616188884\n",
            "train loss:0.002516078386969593\n",
            "train loss:0.023657549003081785\n",
            "train loss:0.0015253498291091965\n",
            "train loss:0.001170734078138171\n",
            "train loss:0.0008082494820729035\n",
            "train loss:0.002029432544430655\n",
            "train loss:0.00073401184147041\n",
            "train loss:0.0009129720559058723\n",
            "train loss:0.006828901445442581\n",
            "train loss:0.0014600237217378277\n",
            "train loss:0.0015319102568548638\n",
            "train loss:0.0036430391672439334\n",
            "train loss:0.00046286154882831836\n",
            "train loss:0.0006925153720032623\n",
            "train loss:0.004347502233022068\n",
            "train loss:0.0009793063909941746\n",
            "train loss:0.0006100755417119513\n",
            "train loss:0.032152037202821976\n",
            "train loss:0.002822871676750622\n",
            "train loss:0.003574447842100324\n",
            "train loss:0.0014371439542921686\n",
            "train loss:0.0011262807569960686\n",
            "train loss:0.0017608398028169834\n",
            "train loss:0.0017814993492452425\n",
            "train loss:0.00016941317132622153\n",
            "train loss:0.007193502951714428\n",
            "train loss:0.0014666533644503643\n",
            "train loss:0.00023218460314048807\n",
            "train loss:0.0012632686726614698\n",
            "train loss:0.006449118534391889\n",
            "train loss:0.002173394428471617\n",
            "train loss:0.0017583478758041362\n",
            "train loss:0.0021533407885354215\n",
            "train loss:0.0011958970034107862\n",
            "train loss:0.007372932885180839\n",
            "train loss:3.9362974192449834e-05\n",
            "train loss:0.000375607224168875\n",
            "train loss:0.001391047424891354\n",
            "train loss:0.0005107597484501524\n",
            "train loss:0.0002672063465116583\n",
            "train loss:0.0006790740713077829\n",
            "train loss:0.000801634311690209\n",
            "train loss:0.0010787512951989176\n",
            "train loss:0.00047099383946163314\n",
            "train loss:0.003054758691809065\n",
            "train loss:0.0005604048618479691\n",
            "train loss:0.001418048908919054\n",
            "train loss:0.0005724010148309148\n",
            "train loss:0.004594084725324405\n",
            "train loss:0.0010071927847482834\n",
            "train loss:0.0015273190754314008\n",
            "train loss:0.001199777851438175\n",
            "train loss:0.003167725984090505\n",
            "train loss:0.00038035526727637337\n",
            "train loss:0.0033355290683154677\n",
            "train loss:0.002288652297336404\n",
            "train loss:0.00040856686293203075\n",
            "train loss:0.001603733365374911\n",
            "train loss:0.0008224589892811238\n",
            "train loss:0.0004933457895042149\n",
            "train loss:0.0017937997043501596\n",
            "train loss:9.736058573099453e-05\n",
            "train loss:0.0019768896710919727\n",
            "train loss:0.000762865682714028\n",
            "train loss:0.0013189432702310686\n",
            "train loss:0.00023961924132062329\n",
            "train loss:0.0016656237861798124\n",
            "train loss:0.0007028367633182116\n",
            "train loss:0.05555444506068463\n",
            "train loss:0.0005461857161715345\n",
            "train loss:0.004796692383874716\n",
            "train loss:0.0007481620103873565\n",
            "train loss:0.00022502892504170125\n",
            "train loss:0.00022455389020968315\n",
            "train loss:0.0010975524824865081\n",
            "train loss:0.0011900112922786422\n",
            "train loss:0.0001789263455576965\n",
            "train loss:0.012380599504238093\n",
            "train loss:0.0029652816444497116\n",
            "train loss:0.0001680835478070676\n",
            "train loss:0.0003734173049795497\n",
            "train loss:0.0025286831581408987\n",
            "train loss:0.005135769092428718\n",
            "train loss:0.00561093210901391\n",
            "train loss:0.0008235900187694781\n",
            "train loss:0.0067294895881823006\n",
            "train loss:0.0007436246519087316\n",
            "train loss:0.0029507556552460833\n",
            "train loss:0.0008284581373368414\n",
            "train loss:0.0013922957701673951\n",
            "train loss:0.00017472657591202677\n",
            "train loss:0.0027616994832395337\n",
            "train loss:0.0018136948437282466\n",
            "train loss:0.0003426804519776155\n",
            "train loss:0.0008216877050901665\n",
            "train loss:0.0008238330577862817\n",
            "train loss:0.0022744171901121755\n",
            "train loss:0.010793076805771356\n",
            "train loss:0.0018529918414778874\n",
            "train loss:0.0010977819028843649\n",
            "train loss:0.001250038449142248\n",
            "train loss:0.00042931550486457885\n",
            "train loss:0.06003220809622522\n",
            "train loss:0.0022918949160847172\n",
            "train loss:0.005041166004048466\n",
            "train loss:0.002030784590260186\n",
            "train loss:0.022271511528075844\n",
            "train loss:0.00018647481599689625\n",
            "train loss:0.0013677502537183155\n",
            "train loss:0.0010411166931050314\n",
            "train loss:0.002699703877125806\n",
            "train loss:9.512387323415334e-05\n",
            "train loss:0.00025635411493435693\n",
            "train loss:0.002139944718518989\n",
            "train loss:0.003905198607471143\n",
            "train loss:0.01094597324322401\n",
            "train loss:0.0005794765991014257\n",
            "train loss:0.007130561688601901\n",
            "train loss:0.000817063077102098\n",
            "train loss:0.0031883794650849136\n",
            "train loss:0.0011764082152039713\n",
            "train loss:0.002617849645163825\n",
            "train loss:0.005910081986181438\n",
            "train loss:0.0004283592070992375\n",
            "train loss:0.00016966370245576664\n",
            "train loss:0.0012896756666013667\n",
            "train loss:0.0016787243590380147\n",
            "train loss:0.006385304793772131\n",
            "train loss:0.005537909488602605\n",
            "train loss:0.0014833461232841723\n",
            "train loss:0.00026363147106822247\n",
            "train loss:0.0003115835686792939\n",
            "train loss:0.000646828866578329\n",
            "train loss:0.0009340716805202286\n",
            "train loss:0.0022974672694301775\n",
            "train loss:0.0011111924132514459\n",
            "train loss:0.005836170667968324\n",
            "train loss:0.008618390002164016\n",
            "train loss:0.0005302507664412439\n",
            "train loss:0.0017496035543891342\n",
            "train loss:0.0009886525704636052\n",
            "train loss:0.00182289637285427\n",
            "train loss:0.004251902931678829\n",
            "train loss:0.005101439399841166\n",
            "train loss:0.003304965155753598\n",
            "train loss:0.002161759023614665\n",
            "train loss:0.009660983460863594\n",
            "train loss:0.00026640221281841247\n",
            "train loss:0.0017039901060192447\n",
            "train loss:0.0020396190160710155\n",
            "train loss:0.001156701830308666\n",
            "train loss:0.004741944574178975\n",
            "train loss:0.0008787345390046579\n",
            "train loss:0.0014497185976748228\n",
            "train loss:0.00039402748890735945\n",
            "train loss:0.00045369522593513676\n",
            "train loss:0.0001481514236150764\n",
            "train loss:0.003291956679585117\n",
            "train loss:0.00020271174286772998\n",
            "train loss:0.0033047390379893605\n",
            "train loss:0.0004417064148351747\n",
            "train loss:0.00041292296493270343\n",
            "train loss:0.0006629643741650334\n",
            "train loss:0.00028852012610015214\n",
            "train loss:0.0005960945234277184\n",
            "train loss:0.0025090507608472746\n",
            "train loss:0.00429697047500305\n",
            "train loss:0.00011502981688899297\n",
            "train loss:0.0004271841841245787\n",
            "train loss:0.0011213180013478143\n",
            "train loss:4.1645881407580655e-05\n",
            "train loss:0.0021528695252301783\n",
            "train loss:0.0013025338435526507\n",
            "train loss:0.0016870041058717295\n",
            "train loss:0.0026443201075364454\n",
            "train loss:0.0004870592471593922\n",
            "train loss:0.0025632948430579757\n",
            "train loss:0.0024688537294157422\n",
            "train loss:0.0003144302057482059\n",
            "train loss:0.002380883232395357\n",
            "train loss:0.00044148754947248206\n",
            "train loss:0.0017567781080556643\n",
            "train loss:0.00022302093173010373\n",
            "train loss:0.0009107325629501773\n",
            "train loss:0.0006471983941371694\n",
            "train loss:0.0010141933987738566\n",
            "train loss:0.007695648237972374\n",
            "train loss:0.0005768606185846238\n",
            "train loss:0.0007828606292992302\n",
            "train loss:0.00024931358573264845\n",
            "train loss:0.0023904019968254823\n",
            "train loss:0.0005145008751556531\n",
            "train loss:0.0002243209487786818\n",
            "train loss:0.00012863512236669118\n",
            "train loss:0.0032789307376400053\n",
            "train loss:0.009531961954251386\n",
            "train loss:0.0026637217602098313\n",
            "train loss:0.002433069461264984\n",
            "train loss:0.0012008128936461613\n",
            "train loss:0.000640716086259658\n",
            "train loss:0.001107626647848902\n",
            "train loss:0.00167200942420656\n",
            "train loss:0.0002519003073255622\n",
            "train loss:0.0035766853621799263\n",
            "train loss:0.0024616094530468162\n",
            "train loss:0.0015293465532771767\n",
            "train loss:0.0007171309666557192\n",
            "train loss:0.0025859658988979357\n",
            "train loss:0.0005433107472556841\n",
            "train loss:0.000488223613683937\n",
            "train loss:0.00041554478826051877\n",
            "train loss:9.56127480769512e-05\n",
            "train loss:0.009613068743461453\n",
            "train loss:0.00018619723959273162\n",
            "train loss:0.0011841674359038172\n",
            "train loss:0.0002585337344020141\n",
            "train loss:0.004456833616919955\n",
            "train loss:0.000626920636735065\n",
            "train loss:0.0038898956461447614\n",
            "train loss:0.0009917513803900334\n",
            "train loss:0.00028800559671263707\n",
            "train loss:0.0033876898398264325\n",
            "train loss:0.0026143636731536953\n",
            "train loss:0.00025183525779313607\n",
            "train loss:0.0049515433394601936\n",
            "train loss:0.019155253399190178\n",
            "train loss:0.000486109487503028\n",
            "train loss:0.00021192752120429478\n",
            "train loss:0.0006272822381141756\n",
            "train loss:0.003406856600564847\n",
            "train loss:0.0012649981708327432\n",
            "train loss:0.001962700660657854\n",
            "train loss:0.0011633253066350602\n",
            "train loss:0.00027909994251843243\n",
            "train loss:0.0022071644368754613\n",
            "train loss:0.0005111928018477082\n",
            "train loss:0.0010655059317706478\n",
            "train loss:0.0017252674932641207\n",
            "train loss:0.0006963675833229607\n",
            "train loss:0.0033372105573554996\n",
            "train loss:0.0007498560438336639\n",
            "train loss:6.646744452031169e-05\n",
            "train loss:0.00317328171476654\n",
            "train loss:0.0008001157552663524\n",
            "train loss:0.004963440855743546\n",
            "train loss:0.000838686222505134\n",
            "train loss:0.0008249902507489397\n",
            "train loss:0.000300006679546762\n",
            "train loss:0.00042746075106511446\n",
            "train loss:0.000786284225729542\n",
            "train loss:0.00015226865025777487\n",
            "train loss:0.0002753423865256097\n",
            "train loss:0.0011222859148304716\n",
            "train loss:0.0005009168877969734\n",
            "train loss:0.004238063027014321\n",
            "train loss:0.0018654880153423628\n",
            "train loss:0.0003405616663488319\n",
            "train loss:0.0019081786912626786\n",
            "train loss:0.0007816283038916915\n",
            "train loss:0.0005384258950454269\n",
            "train loss:4.712604790523566e-05\n",
            "train loss:0.00409593937451107\n",
            "train loss:0.005950986013344218\n",
            "train loss:0.00034901528367187526\n",
            "train loss:0.002199844413239295\n",
            "train loss:0.0017297998610955373\n",
            "train loss:0.0014835787025487246\n",
            "train loss:0.0034772273798696395\n",
            "train loss:0.0006018219443159772\n",
            "train loss:0.0006146741848084375\n",
            "train loss:0.0011235035355017832\n",
            "train loss:0.001322555453618587\n",
            "train loss:0.0004239052198704091\n",
            "train loss:0.0009456263769621737\n",
            "train loss:0.0020786164262901145\n",
            "train loss:0.0018016670570825102\n",
            "train loss:0.002838125867491549\n",
            "train loss:0.0027831446359640796\n",
            "train loss:0.0009863296360971827\n",
            "train loss:0.00018662237929575927\n",
            "train loss:0.0009300571574068528\n",
            "train loss:0.0023360335586380503\n",
            "train loss:3.506124545904768e-05\n",
            "train loss:0.0004980624428573461\n",
            "train loss:0.001137068800809884\n",
            "train loss:0.005170354150373897\n",
            "train loss:0.0012912519553279964\n",
            "train loss:0.004612371694194568\n",
            "train loss:0.0002889348903472135\n",
            "train loss:0.0010905370420853256\n",
            "train loss:0.0004933239209985457\n",
            "train loss:0.006204042573643749\n",
            "train loss:0.00420648797597008\n",
            "train loss:0.0035978617415509816\n",
            "train loss:0.00020913310323484552\n",
            "train loss:0.002688812244263856\n",
            "train loss:0.0001963676881893433\n",
            "train loss:0.0005885799798098546\n",
            "train loss:2.891766851924742e-05\n",
            "train loss:0.006935767656442956\n",
            "train loss:0.00113940191274273\n",
            "train loss:0.0026117243656084226\n",
            "train loss:7.301882432386923e-05\n",
            "train loss:0.00012508642854384665\n",
            "train loss:0.0007502380952046064\n",
            "train loss:0.002016112576514878\n",
            "train loss:0.01842050862070087\n",
            "train loss:0.0008947723649514014\n",
            "train loss:0.00038200603462278516\n",
            "train loss:0.004315114240132083\n",
            "train loss:0.00025187096287345004\n",
            "train loss:0.0029269781973486606\n",
            "train loss:0.00012210907464329067\n",
            "train loss:0.0011900644276101708\n",
            "train loss:0.0003071747757536524\n",
            "train loss:0.00019464364411329272\n",
            "train loss:0.0004891810865486865\n",
            "train loss:0.00013569338930599208\n",
            "train loss:0.012586877809155062\n",
            "train loss:0.002734570676388612\n",
            "train loss:0.0005135073178500317\n",
            "train loss:0.00012230511551576358\n",
            "train loss:0.0008060937747612367\n",
            "train loss:0.001580591486430297\n",
            "train loss:0.0033950215976298785\n",
            "train loss:0.007487853769485567\n",
            "train loss:0.00863569246444746\n",
            "train loss:0.0012865747093417316\n",
            "train loss:0.002608825261275431\n",
            "train loss:0.005126754697033188\n",
            "train loss:0.003474189404553975\n",
            "train loss:0.0016763256277694462\n",
            "train loss:0.00042021873040083653\n",
            "train loss:0.002735396187020284\n",
            "train loss:0.002004255842365622\n",
            "train loss:0.011543600889470495\n",
            "train loss:0.021118185800033582\n",
            "train loss:0.0026473220302396087\n",
            "train loss:0.00015866477268091985\n",
            "train loss:0.0034767634856794965\n",
            "train loss:0.0030905885975889198\n",
            "train loss:0.004258376229057609\n",
            "train loss:0.0077442563337194225\n",
            "train loss:0.0009105665516905653\n",
            "train loss:0.008226425941237766\n",
            "train loss:0.0016201141453166745\n",
            "train loss:0.0011272056192772378\n",
            "train loss:0.0013237803069182663\n",
            "train loss:0.0007314688655390314\n",
            "train loss:0.001768876324823799\n",
            "train loss:0.005577798440767862\n",
            "train loss:0.0023572868706551527\n",
            "train loss:0.0004289735289549789\n",
            "train loss:0.002572368136063051\n",
            "train loss:0.00047142103691154394\n",
            "train loss:0.001084886166630157\n",
            "train loss:0.00041601044971933753\n",
            "train loss:0.0039219532829341186\n",
            "train loss:0.016010083066110143\n",
            "train loss:0.0010156727924036295\n",
            "train loss:0.0010131421669696077\n",
            "train loss:0.001934629460528016\n",
            "train loss:0.0025592687566604088\n",
            "train loss:0.003261603498366621\n",
            "train loss:0.0010767514564433006\n",
            "train loss:0.11368047618985982\n",
            "train loss:0.0017696601321604225\n",
            "train loss:0.0007331603236246652\n",
            "train loss:0.0038067378176771588\n",
            "train loss:0.000558235952443049\n",
            "train loss:0.0007226163450764564\n",
            "train loss:0.005469049144924635\n",
            "train loss:0.000379672500639227\n",
            "train loss:0.00049700642562115\n",
            "train loss:0.0014996156124215945\n",
            "train loss:0.000980210506768677\n",
            "train loss:0.007221499508701798\n",
            "train loss:0.00036328115371456806\n",
            "train loss:0.002874025767117117\n",
            "train loss:0.0004924664349140131\n",
            "train loss:0.0022090459575577173\n",
            "train loss:0.0011888947880952182\n",
            "train loss:0.0003162239628064417\n",
            "train loss:0.0008143691181815206\n",
            "train loss:0.00015231320406623077\n",
            "train loss:0.0027542213835544394\n",
            "train loss:0.00028412327239002314\n",
            "train loss:0.00046978081542142796\n",
            "train loss:0.003736497675598457\n",
            "train loss:0.002071739596152843\n",
            "train loss:0.0004851715871265378\n",
            "train loss:0.0005132157095709287\n",
            "train loss:0.0003492800429461166\n",
            "train loss:0.005763030435939448\n",
            "train loss:0.0012959849876469363\n",
            "train loss:0.002039499707908082\n",
            "train loss:0.0011659901138302512\n",
            "train loss:0.005023771141264662\n",
            "train loss:0.0026792832842050026\n",
            "train loss:0.0016293363138281983\n",
            "train loss:0.003879580360380041\n",
            "train loss:0.0008329020450368928\n",
            "train loss:0.0008723346009031328\n",
            "train loss:0.009531930902703241\n",
            "train loss:0.0011962337520935596\n",
            "train loss:0.0002756352486636177\n",
            "train loss:0.000247235880743049\n",
            "train loss:0.001834797677053677\n",
            "train loss:0.001292658728079776\n",
            "train loss:0.0011305654664543758\n",
            "train loss:0.0021285823838350666\n",
            "train loss:9.520575942189899e-05\n",
            "train loss:0.0004171227619137927\n",
            "train loss:0.003700184883540888\n",
            "train loss:0.0030517950736127846\n",
            "train loss:0.0019009031175514135\n",
            "train loss:0.0016649911356329669\n",
            "=== epoch:17, train acc:0.999, test acc:0.987 ===\n",
            "train loss:0.0015494007536109144\n",
            "train loss:0.001136852588158428\n",
            "train loss:0.004375134482248562\n",
            "train loss:0.0011439066391627879\n",
            "train loss:1.3668980774794326e-05\n",
            "train loss:3.611513132455928e-05\n",
            "train loss:0.0010972339957692463\n",
            "train loss:0.003117366018906163\n",
            "train loss:0.0005401565650425011\n",
            "train loss:0.00024280253447312743\n",
            "train loss:0.0008804683696585997\n",
            "train loss:0.00427689904465423\n",
            "train loss:0.001863413403346983\n",
            "train loss:2.8261281481953976e-05\n",
            "train loss:0.0002427220653895902\n",
            "train loss:0.0021405428575268304\n",
            "train loss:0.002746425204537134\n",
            "train loss:6.007818319725745e-05\n",
            "train loss:0.00015582718255661678\n",
            "train loss:0.007090773418206984\n",
            "train loss:0.0015186618122026232\n",
            "train loss:0.0010391455865116895\n",
            "train loss:8.779916460590122e-05\n",
            "train loss:0.008050101398491338\n",
            "train loss:0.00021852948739718643\n",
            "train loss:5.4732124348560566e-05\n",
            "train loss:0.0006668567017858396\n",
            "train loss:0.0005298014488995805\n",
            "train loss:0.002435954023347411\n",
            "train loss:0.00043092854276165357\n",
            "train loss:0.0009940225143264896\n",
            "train loss:0.0005083659295585943\n",
            "train loss:0.0007096028769528256\n",
            "train loss:0.0004935592858871804\n",
            "train loss:0.0051544100007001985\n",
            "train loss:0.0017135678556009497\n",
            "train loss:0.00020211453824309862\n",
            "train loss:0.0006634154323461231\n",
            "train loss:0.0026469909348933807\n",
            "train loss:0.0004415044137980869\n",
            "train loss:0.0031894380636012204\n",
            "train loss:0.0016635485967232489\n",
            "train loss:0.0004383544599935546\n",
            "train loss:0.0010452031436285925\n",
            "train loss:0.011306097052002046\n",
            "train loss:2.3451105636048733e-05\n",
            "train loss:0.0037820431937014565\n",
            "train loss:0.00020905803200697146\n",
            "train loss:0.0006948360509493045\n",
            "train loss:0.000991491033031286\n",
            "train loss:0.001220482538239374\n",
            "train loss:0.0007696392056243057\n",
            "train loss:0.0004984926743951231\n",
            "train loss:0.0010823363752614586\n",
            "train loss:0.0010048243640805388\n",
            "train loss:0.0014254482064389531\n",
            "train loss:0.0016822635171700765\n",
            "train loss:4.104123074228106e-05\n",
            "train loss:0.0010926111083771558\n",
            "train loss:0.0006177905772858041\n",
            "train loss:0.0029228810814421065\n",
            "train loss:0.004390426207822788\n",
            "train loss:0.0008027729409896598\n",
            "train loss:0.01325364329746943\n",
            "train loss:0.0007213564210060093\n",
            "train loss:0.00019531849977425932\n",
            "train loss:0.002148009200361788\n",
            "train loss:0.0002493233094993469\n",
            "train loss:0.0004210147186308641\n",
            "train loss:0.00533521275960116\n",
            "train loss:8.788159951540745e-05\n",
            "train loss:0.0014185448702097462\n",
            "train loss:0.006732270773633167\n",
            "train loss:0.0010798436536005019\n",
            "train loss:0.0037312045135971966\n",
            "train loss:0.003080252925786655\n",
            "train loss:0.002687793291454634\n",
            "train loss:0.0005666367457289031\n",
            "train loss:0.0026454718598870573\n",
            "train loss:9.96741447587606e-05\n",
            "train loss:0.00025982440468536494\n",
            "train loss:0.0018951557431261309\n",
            "train loss:0.03160873012376787\n",
            "train loss:0.0021021849993682153\n",
            "train loss:0.00014583718995790626\n",
            "train loss:0.010186187307422007\n",
            "train loss:0.00011837217380246686\n",
            "train loss:0.001815395977040756\n",
            "train loss:0.0008821049056599677\n",
            "train loss:0.002807433125822846\n",
            "train loss:0.0014827723392543154\n",
            "train loss:0.0006171659282696566\n",
            "train loss:0.0008658316563577125\n",
            "train loss:0.0004416026454356873\n",
            "train loss:0.0260611128173936\n",
            "train loss:0.0003654325757424459\n",
            "train loss:0.0008605389271204469\n",
            "train loss:0.0014612665288462228\n",
            "train loss:0.0011844556905766268\n",
            "train loss:0.00043252462283536824\n",
            "train loss:0.01674531925484024\n",
            "train loss:0.0012007613783114016\n",
            "train loss:4.159098874981995e-05\n",
            "train loss:0.0028005221186847566\n",
            "train loss:0.000172708040062384\n",
            "train loss:0.003537625299554616\n",
            "train loss:0.00031868859982891144\n",
            "train loss:0.0022461611086018805\n",
            "train loss:0.00010691969304967328\n",
            "train loss:0.005139522945284669\n",
            "train loss:0.00021516132599817516\n",
            "train loss:7.751481830480463e-05\n",
            "train loss:0.0024418972632941696\n",
            "train loss:0.0054905281511286085\n",
            "train loss:0.0001422307192226647\n",
            "train loss:0.00104320271988628\n",
            "train loss:0.0009826432345169268\n",
            "train loss:0.0004105522601116144\n",
            "train loss:0.0006879809989144511\n",
            "train loss:0.0016269178366483913\n",
            "train loss:0.0008973523687208025\n",
            "train loss:0.0018581268699890123\n",
            "train loss:0.001282496526588576\n",
            "train loss:0.00494803895661079\n",
            "train loss:0.001795040652231325\n",
            "train loss:0.0026017547792681717\n",
            "train loss:0.00405829864232551\n",
            "train loss:5.282256117458062e-05\n",
            "train loss:0.004449411074612231\n",
            "train loss:0.0005942609697071207\n",
            "train loss:0.0012979111785739641\n",
            "train loss:0.0003839829353017775\n",
            "train loss:0.0030214988639916066\n",
            "train loss:0.0015404041339926264\n",
            "train loss:0.0019465981064833132\n",
            "train loss:0.0002915439250082612\n",
            "train loss:0.0026533997935947757\n",
            "train loss:0.002874397460765983\n",
            "train loss:0.0007250792535907592\n",
            "train loss:0.00011386164514161247\n",
            "train loss:0.0022662119615555055\n",
            "train loss:0.0024841410295896836\n",
            "train loss:0.0006642168460025789\n",
            "train loss:0.03400684422089772\n",
            "train loss:0.00016166928303475465\n",
            "train loss:0.003999739572537901\n",
            "train loss:0.001846662072091993\n",
            "train loss:0.0007005871287144969\n",
            "train loss:0.002983180346481893\n",
            "train loss:0.0020852792647216454\n",
            "train loss:0.0005751166682861755\n",
            "train loss:0.008187289118995624\n",
            "train loss:0.0026291635538937953\n",
            "train loss:4.774355812954099e-05\n",
            "train loss:0.0008633229016074229\n",
            "train loss:0.0023563762655833325\n",
            "train loss:0.0031319234293554943\n",
            "train loss:0.0007003941904077591\n",
            "train loss:0.0014691826429089766\n",
            "train loss:0.0005667509774598896\n",
            "train loss:0.0005520586353444678\n",
            "train loss:0.0013962760394711255\n",
            "train loss:6.700636395282399e-05\n",
            "train loss:0.005370890284179355\n",
            "train loss:0.00593606710205075\n",
            "train loss:5.566232868973645e-05\n",
            "train loss:0.0023575177821944264\n",
            "train loss:0.0007570926990480678\n",
            "train loss:0.0005557083236684067\n",
            "train loss:0.0018277407003541516\n",
            "train loss:0.000305671353380149\n",
            "train loss:8.701200173810337e-05\n",
            "train loss:0.002109812308782996\n",
            "train loss:0.0009642648779312691\n",
            "train loss:0.00219064644399452\n",
            "train loss:0.00011432392329735022\n",
            "train loss:0.0011121791175417992\n",
            "train loss:0.0013174989072219862\n",
            "train loss:0.0003401196300244975\n",
            "train loss:0.002254868791697289\n",
            "train loss:0.0012432726753124402\n",
            "train loss:0.00047462323343927\n",
            "train loss:0.0016121606043299202\n",
            "train loss:0.0025482777268166\n",
            "train loss:0.00013054154596642482\n",
            "train loss:0.0002928774704959972\n",
            "train loss:0.005488210609646997\n",
            "train loss:0.0001876709190253565\n",
            "train loss:0.0009501613277826458\n",
            "train loss:0.0005841166236556509\n",
            "train loss:0.0014663133026975744\n",
            "train loss:0.0005721065859943586\n",
            "train loss:0.00015063369297424044\n",
            "train loss:0.0028617245056651724\n",
            "train loss:0.00491921765802359\n",
            "train loss:0.00030012643771099896\n",
            "train loss:8.1024971378536e-05\n",
            "train loss:5.992028049890152e-05\n",
            "train loss:0.0005575300245445826\n",
            "train loss:0.000121804435682374\n",
            "train loss:0.0032983924601831523\n",
            "train loss:0.00033365064774113363\n",
            "train loss:0.0024509816135693373\n",
            "train loss:0.001937625744619931\n",
            "train loss:7.076822472643979e-05\n",
            "train loss:0.001825565125100338\n",
            "train loss:0.00020018092877350713\n",
            "train loss:0.001042530585341075\n",
            "train loss:0.0026726530259464083\n",
            "train loss:0.00024513023197280845\n",
            "train loss:0.0011243945395951328\n",
            "train loss:0.0012041997136713043\n",
            "train loss:0.001349120414047837\n",
            "train loss:0.0005014230207971355\n",
            "train loss:0.009647695079649271\n",
            "train loss:0.0008721082426302443\n",
            "train loss:0.002292671022441135\n",
            "train loss:0.0006367857915051136\n",
            "train loss:0.0007406727957318408\n",
            "train loss:0.0006574114893353047\n",
            "train loss:9.162201307932331e-05\n",
            "train loss:0.0012951531443633469\n",
            "train loss:3.9467039845838583e-05\n",
            "train loss:0.0013162323395400593\n",
            "train loss:0.0009648358941670401\n",
            "train loss:0.003552247137158861\n",
            "train loss:0.0008964752603189178\n",
            "train loss:0.0009587624118034155\n",
            "train loss:0.0005715006158940604\n",
            "train loss:0.0027315412437579727\n",
            "train loss:0.0044730428309267\n",
            "train loss:0.0009595399605873238\n",
            "train loss:0.0005754720223704793\n",
            "train loss:0.004568396190452493\n",
            "train loss:0.06513422220126412\n",
            "train loss:0.0014139263480880675\n",
            "train loss:0.0018441604199871944\n",
            "train loss:0.000813919903658449\n",
            "train loss:0.008143230293455022\n",
            "train loss:0.0004521721009841256\n",
            "train loss:0.0011112898515732194\n",
            "train loss:0.002019759534662531\n",
            "train loss:0.0028804986302955727\n",
            "train loss:0.0012284861564591859\n",
            "train loss:0.04643363515851266\n",
            "train loss:0.006322137074097389\n",
            "train loss:0.000944047883108856\n",
            "train loss:0.001289133777153532\n",
            "train loss:0.004475738206238437\n",
            "train loss:0.0002320658084737916\n",
            "train loss:0.007166631028213978\n",
            "train loss:8.107528619650784e-06\n",
            "train loss:0.0011697992006128365\n",
            "train loss:0.0023101724645155698\n",
            "train loss:0.0013513767280936494\n",
            "train loss:6.747128345186878e-05\n",
            "train loss:0.0034753337892478382\n",
            "train loss:0.0020724948596888386\n",
            "train loss:0.0011502814615496698\n",
            "train loss:0.0033435386032026502\n",
            "train loss:0.00307993091930267\n",
            "train loss:0.0006410459514201572\n",
            "train loss:0.00025668449203894005\n",
            "train loss:0.03840059456088762\n",
            "train loss:0.0015640636829273192\n",
            "train loss:0.0013267190131741916\n",
            "train loss:0.00021966660740414646\n",
            "train loss:0.008286183278615071\n",
            "train loss:0.0027533308694405885\n",
            "train loss:0.0009835708785412838\n",
            "train loss:0.0004210184723720752\n",
            "train loss:0.0037854323011401615\n",
            "train loss:0.00020455310065761278\n",
            "train loss:0.012428541753699394\n",
            "train loss:0.00017479793911561774\n",
            "train loss:0.0016275704841246\n",
            "train loss:0.0009117746897063344\n",
            "train loss:0.003305818147462396\n",
            "train loss:0.0005147650224345219\n",
            "train loss:0.0012092948645308582\n",
            "train loss:0.0015672362165545823\n",
            "train loss:0.0005494404243695785\n",
            "train loss:0.014435624562454185\n",
            "train loss:0.002302318738112301\n",
            "train loss:0.00036056263301744335\n",
            "train loss:0.00033089756427224294\n",
            "train loss:0.0011160620120923062\n",
            "train loss:0.008567510904925205\n",
            "train loss:0.0026762335650436673\n",
            "train loss:0.000692512271554769\n",
            "train loss:0.013118049030251567\n",
            "train loss:0.0006217885215929153\n",
            "train loss:0.002084476350118748\n",
            "train loss:0.0016946619810688695\n",
            "train loss:0.0002139596642743702\n",
            "train loss:0.004542660870373485\n",
            "train loss:0.0014795089821931446\n",
            "train loss:0.0007537294620215987\n",
            "train loss:0.0014808987806090364\n",
            "train loss:0.00026559257002713534\n",
            "train loss:0.0014610599623994434\n",
            "train loss:0.0035043224441395227\n",
            "train loss:0.0006433590450909762\n",
            "train loss:0.0007009841133814167\n",
            "train loss:0.005694280104673711\n",
            "train loss:0.016734432365747887\n",
            "train loss:0.0025743819991508716\n",
            "train loss:0.0009095899633945649\n",
            "train loss:0.0005440065463658072\n",
            "train loss:0.0063962193124349085\n",
            "train loss:0.0002686093970375729\n",
            "train loss:0.00035153696246685\n",
            "train loss:0.00014507545557107535\n",
            "train loss:0.0009192761502095019\n",
            "train loss:0.00028803608772422207\n",
            "train loss:0.0020354360885919067\n",
            "train loss:0.0008093200096539014\n",
            "train loss:0.0039192628965262636\n",
            "train loss:0.005728335881502625\n",
            "train loss:0.00103365515632631\n",
            "train loss:7.782444008374544e-05\n",
            "train loss:0.0002469516032660047\n",
            "train loss:0.00020293765031200825\n",
            "train loss:0.00018562277357318018\n",
            "train loss:0.00168684240612289\n",
            "train loss:7.413176219160063e-05\n",
            "train loss:0.0037769274746506237\n",
            "train loss:0.00043368886105807884\n",
            "train loss:0.0007931777293552178\n",
            "train loss:0.0017118642871642806\n",
            "train loss:0.007285988326601587\n",
            "train loss:0.0004635437833651888\n",
            "train loss:0.0028193790859311557\n",
            "train loss:0.0001483031011731114\n",
            "train loss:0.009747462083530146\n",
            "train loss:0.0023594294435089497\n",
            "train loss:0.0004213255088966764\n",
            "train loss:0.000884222532788378\n",
            "train loss:0.0006570813561331625\n",
            "train loss:0.0018187824045487744\n",
            "train loss:0.004055320064703883\n",
            "train loss:0.0011494088362226653\n",
            "train loss:0.004016841938388597\n",
            "train loss:0.0006712453814469308\n",
            "train loss:0.0012746399768968604\n",
            "train loss:0.007696263384643329\n",
            "train loss:0.0008547528896442315\n",
            "train loss:0.04269851647103487\n",
            "train loss:0.00027027335759173494\n",
            "train loss:0.00023293809703601895\n",
            "train loss:0.0013888016528544286\n",
            "train loss:0.004988854470139061\n",
            "train loss:0.0016216869983696016\n",
            "train loss:0.000267963619857645\n",
            "train loss:0.0008661197199676543\n",
            "train loss:0.002063388622154545\n",
            "train loss:0.001177124433799626\n",
            "train loss:0.0024224377983012926\n",
            "train loss:8.732371233176758e-05\n",
            "train loss:0.0010540243084786007\n",
            "train loss:0.0014165626670805068\n",
            "train loss:0.003568386740928879\n",
            "train loss:0.0022670902670831284\n",
            "train loss:0.00361188494315022\n",
            "train loss:0.0004504115976209066\n",
            "train loss:0.0007323014092791157\n",
            "train loss:0.0002349972390702416\n",
            "train loss:0.00035926679160952284\n",
            "train loss:0.0004693232705417533\n",
            "train loss:0.0043432632862559885\n",
            "train loss:0.0022848679134074273\n",
            "train loss:0.001615182206839096\n",
            "train loss:0.0011313827583284079\n",
            "train loss:0.0020630228776231233\n",
            "train loss:0.0201691180265862\n",
            "train loss:0.0011821990780342734\n",
            "train loss:0.004172485507672266\n",
            "train loss:0.0019272079735154215\n",
            "train loss:0.0027356490297422886\n",
            "train loss:0.0005430131522309159\n",
            "train loss:0.0030687706428180568\n",
            "train loss:0.0009796613113942075\n",
            "train loss:6.028723171035916e-05\n",
            "train loss:0.006633179258097739\n",
            "train loss:0.00100935074036034\n",
            "train loss:0.000421595867168046\n",
            "train loss:0.0001305195387373988\n",
            "train loss:0.002894395065027821\n",
            "train loss:6.595665797958682e-05\n",
            "train loss:0.00048485958305507436\n",
            "train loss:0.0018955849079436685\n",
            "train loss:0.005249353710034657\n",
            "train loss:0.00026093427365647\n",
            "train loss:0.0022239896833027746\n",
            "train loss:0.0027038828559769897\n",
            "train loss:0.0010288326636435766\n",
            "train loss:7.734997101098675e-05\n",
            "train loss:0.005450405008765538\n",
            "train loss:0.0003297104103561672\n",
            "train loss:0.00012210794070433617\n",
            "train loss:0.00014803795548772787\n",
            "train loss:0.0009824846853763128\n",
            "train loss:0.0030136169321936974\n",
            "train loss:0.00031679818028045004\n",
            "train loss:0.0008980995921525657\n",
            "train loss:0.0011074784526073312\n",
            "train loss:0.004320803180154083\n",
            "train loss:0.003199298844020236\n",
            "train loss:0.014859833022605271\n",
            "train loss:0.0006220344597812143\n",
            "train loss:0.0006017010570046845\n",
            "train loss:0.0032025301840252324\n",
            "train loss:0.005393562029235163\n",
            "train loss:0.001584994309893773\n",
            "train loss:0.0009631142581622936\n",
            "train loss:0.003036091145515753\n",
            "train loss:0.00383468696635203\n",
            "train loss:0.0015791524456531486\n",
            "train loss:0.003685201706670147\n",
            "train loss:5.465855425976918e-05\n",
            "train loss:5.216120544921917e-05\n",
            "train loss:0.002326640666657795\n",
            "train loss:0.0006233684815426829\n",
            "train loss:0.004307516444036332\n",
            "train loss:0.0005052646871059536\n",
            "train loss:0.001513683055374414\n",
            "train loss:0.002431895188276517\n",
            "train loss:2.5879978438560954e-05\n",
            "train loss:0.0007835608477508221\n",
            "train loss:0.002154506213625873\n",
            "train loss:0.011275600483574826\n",
            "train loss:0.0004917943189495847\n",
            "train loss:0.0010677260836278767\n",
            "train loss:0.00028654170662951377\n",
            "train loss:0.0013551325592736862\n",
            "train loss:0.0021099907407006674\n",
            "train loss:0.002228261813240015\n",
            "train loss:0.003956948067470381\n",
            "train loss:0.0014211732050865402\n",
            "train loss:0.0014253689180876838\n",
            "train loss:0.0005188059644694729\n",
            "train loss:0.0002678925422085447\n",
            "train loss:0.0001944311665002918\n",
            "train loss:0.001887338454668035\n",
            "train loss:0.004084019304271643\n",
            "train loss:9.198615410152742e-05\n",
            "train loss:0.00021495951300207178\n",
            "train loss:0.0035786687461264836\n",
            "train loss:0.00409601920710119\n",
            "train loss:0.0017981920243625327\n",
            "train loss:0.0037840946308377245\n",
            "train loss:0.0036685038123314352\n",
            "train loss:5.2526335376113475e-05\n",
            "train loss:0.0020709118711364043\n",
            "train loss:0.00014264917370319992\n",
            "train loss:0.0001924713490074586\n",
            "train loss:0.00044819249328136745\n",
            "train loss:0.0004574420065681959\n",
            "train loss:0.0013684002717717747\n",
            "train loss:0.00217058749527099\n",
            "train loss:0.0001451517209500629\n",
            "train loss:0.00018951769963832958\n",
            "train loss:0.0007791674410480244\n",
            "train loss:0.00044850795904407935\n",
            "train loss:0.0002455508432399755\n",
            "train loss:0.0007551450060720715\n",
            "train loss:0.00030258801565484316\n",
            "train loss:0.007677984645627659\n",
            "train loss:0.0007963179937071648\n",
            "train loss:0.022577028463416235\n",
            "train loss:0.001404754383152679\n",
            "train loss:0.003565561750737985\n",
            "train loss:0.0006093028158173021\n",
            "train loss:0.00047856978402091214\n",
            "train loss:0.022504300717047995\n",
            "train loss:0.0004524428948845229\n",
            "train loss:0.001732259734435323\n",
            "train loss:4.9695644816950314e-05\n",
            "train loss:0.00036764087897396276\n",
            "train loss:0.00016350107464750409\n",
            "train loss:0.00043948537035103417\n",
            "train loss:0.005874251877245992\n",
            "train loss:0.001733407135370092\n",
            "train loss:0.001966615714200452\n",
            "train loss:0.0005236501596785921\n",
            "train loss:0.0018731199180633815\n",
            "train loss:0.0008478072552564703\n",
            "train loss:0.000798594457213986\n",
            "train loss:0.001094221011376173\n",
            "train loss:7.460058367986214e-05\n",
            "train loss:0.0010592320423634853\n",
            "train loss:0.0019519571907448973\n",
            "train loss:0.00024227146403421883\n",
            "train loss:7.02954443870171e-05\n",
            "train loss:0.0001521883341302328\n",
            "train loss:0.0005002078797320437\n",
            "train loss:0.046613257580761285\n",
            "train loss:0.0010267761617063678\n",
            "train loss:0.002716791580132238\n",
            "train loss:1.7985079376113533e-05\n",
            "train loss:0.0003820319359235309\n",
            "train loss:0.0038486054034009053\n",
            "train loss:0.00020056303006157517\n",
            "train loss:0.00045462020366022327\n",
            "train loss:0.004481526710936853\n",
            "train loss:0.0047029044516751\n",
            "train loss:0.000997229456619534\n",
            "train loss:0.0013717113523421037\n",
            "train loss:0.004956752567169587\n",
            "train loss:0.004514173276626104\n",
            "train loss:0.002150048773719335\n",
            "train loss:0.0009227170876856952\n",
            "train loss:0.00030340287278568886\n",
            "train loss:0.00031970641621301593\n",
            "train loss:0.0007078464501276942\n",
            "train loss:0.0025401634103648053\n",
            "train loss:0.002020902558186158\n",
            "train loss:0.000445567739205299\n",
            "train loss:0.0018883282084707713\n",
            "train loss:0.00182855063785822\n",
            "train loss:0.0020095728419772347\n",
            "train loss:0.0007411943037490031\n",
            "train loss:0.0004483910057936727\n",
            "train loss:0.0025961482440971717\n",
            "train loss:0.001237421470216253\n",
            "train loss:3.3285686385906964e-05\n",
            "train loss:0.01970526712832142\n",
            "train loss:0.0008459799127576396\n",
            "train loss:0.0001891702995921967\n",
            "train loss:0.0035166677336123473\n",
            "train loss:0.000356291486364777\n",
            "train loss:0.009618873553105738\n",
            "train loss:0.0010210759822291023\n",
            "train loss:7.10634587338922e-05\n",
            "train loss:0.0014415640115072487\n",
            "train loss:0.0018185901953034081\n",
            "train loss:0.00044102722412601395\n",
            "train loss:0.0013433104872000105\n",
            "train loss:0.00021152407306089313\n",
            "train loss:0.00015333574591456747\n",
            "train loss:0.0006704200240883413\n",
            "train loss:0.00011257932440262231\n",
            "train loss:0.014388949126611397\n",
            "train loss:0.0028256369673031806\n",
            "train loss:0.00044220222354103127\n",
            "train loss:0.00014285952321821656\n",
            "train loss:0.00045064012736113323\n",
            "train loss:0.0009041934800056047\n",
            "train loss:0.0029120422270987514\n",
            "train loss:0.006627991177309655\n",
            "train loss:0.00026573422978341197\n",
            "train loss:0.0016670075031854717\n",
            "train loss:6.170494215006186e-05\n",
            "train loss:0.0013686389061306052\n",
            "train loss:0.00012816223178724803\n",
            "train loss:0.0010169113296269925\n",
            "train loss:0.0005580841430463817\n",
            "train loss:0.0007497708824265275\n",
            "train loss:0.0004283249721552008\n",
            "train loss:0.0004443740331383857\n",
            "train loss:0.022054573508443677\n",
            "train loss:0.00017940783357412522\n",
            "train loss:0.0008057428054668659\n",
            "train loss:0.001277392677380329\n",
            "train loss:0.0008532827027797525\n",
            "train loss:0.013491557110454824\n",
            "train loss:0.0007384978567187273\n",
            "train loss:0.0015500751636334737\n",
            "train loss:0.0020391815013938177\n",
            "train loss:0.0013159286634867864\n",
            "train loss:0.00037288997368990534\n",
            "train loss:0.00047413438355155984\n",
            "train loss:0.001320266357401259\n",
            "train loss:0.0013047862714251216\n",
            "train loss:0.0014811846062991803\n",
            "train loss:0.0004673508788159041\n",
            "train loss:6.81252513586215e-05\n",
            "train loss:0.0012762723161503283\n",
            "train loss:0.0043866220300980314\n",
            "train loss:9.324989558581994e-05\n",
            "train loss:0.00014094984352068098\n",
            "train loss:0.00012168927833304602\n",
            "train loss:0.0004216810108594285\n",
            "train loss:0.006782599243781768\n",
            "train loss:8.185936179375373e-05\n",
            "train loss:0.00035782355255624295\n",
            "train loss:0.0040594813243789414\n",
            "train loss:0.0011354920204383445\n",
            "train loss:0.0007089058984945472\n",
            "train loss:0.00034667021145229696\n",
            "train loss:0.00021268592573710657\n",
            "train loss:0.001705848857425087\n",
            "train loss:0.00015592078418467963\n",
            "train loss:0.0012242310520242433\n",
            "train loss:0.0010180539967829852\n",
            "train loss:0.0002567776191370498\n",
            "train loss:9.944674222509635e-05\n",
            "train loss:0.0019109681431258468\n",
            "train loss:0.0023589636543426547\n",
            "train loss:0.00032667540430297945\n",
            "=== epoch:18, train acc:1.0, test acc:0.99 ===\n",
            "train loss:0.00026368408052193057\n",
            "train loss:0.003620511731183876\n",
            "train loss:0.0001601537886955552\n",
            "train loss:0.0010625833526289165\n",
            "train loss:0.007872440865972342\n",
            "train loss:0.0030266873414770607\n",
            "train loss:0.0002422119060091701\n",
            "train loss:0.00018316292774824092\n",
            "train loss:0.0030693380278195175\n",
            "train loss:0.0042929290090192316\n",
            "train loss:0.0008626792075359683\n",
            "train loss:0.0007586631764931305\n",
            "train loss:0.005734028625101854\n",
            "train loss:0.0017065803743328992\n",
            "train loss:0.0025781961061371243\n",
            "train loss:0.0053647452683258144\n",
            "train loss:0.00025602685295486697\n",
            "train loss:0.0005414682727123145\n",
            "train loss:8.207154614541769e-05\n",
            "train loss:1.9322086832363913e-05\n",
            "train loss:0.0051050167590417864\n",
            "train loss:0.011060608753206538\n",
            "train loss:0.0015908258463293045\n",
            "train loss:0.002312456029561887\n",
            "train loss:0.00043391030975306193\n",
            "train loss:0.003452855909897166\n",
            "train loss:0.0003418327385946773\n",
            "train loss:0.0018083508824653727\n",
            "train loss:0.006015649776359428\n",
            "train loss:0.00035974436971629496\n",
            "train loss:0.0008993074571461486\n",
            "train loss:0.002302273639346035\n",
            "train loss:0.004031918974158175\n",
            "train loss:0.01088876193816535\n",
            "train loss:0.0035353397798103515\n",
            "train loss:0.0022599666968095653\n",
            "train loss:0.0003655192760835201\n",
            "train loss:0.001429146255659801\n",
            "train loss:0.0003310668784098169\n",
            "train loss:0.010042888624183352\n",
            "train loss:0.009554227383805786\n",
            "train loss:0.013535309507517327\n",
            "train loss:0.009510594742792826\n",
            "train loss:0.002256230351886054\n",
            "train loss:0.004121508332753836\n",
            "train loss:0.001765543341528572\n",
            "train loss:0.007111000012606849\n",
            "train loss:0.004556105610499237\n",
            "train loss:0.0004228221483250071\n",
            "train loss:0.012198090511046583\n",
            "train loss:0.006412722854741136\n",
            "train loss:0.0014337995121540376\n",
            "train loss:0.03852063381088393\n",
            "train loss:0.00044712610179251696\n",
            "train loss:0.005920223669681207\n",
            "train loss:0.0016680873935521786\n",
            "train loss:0.005264988138428982\n",
            "train loss:0.0053777383022714145\n",
            "train loss:0.001139147750654535\n",
            "train loss:0.00046099449443551537\n",
            "train loss:0.029493937689710027\n",
            "train loss:0.01274906697937584\n",
            "train loss:0.0018826685342970741\n",
            "train loss:0.0063502253874149804\n",
            "train loss:0.002229515262690212\n",
            "train loss:0.002784064905127713\n",
            "train loss:0.013217998064759767\n",
            "train loss:0.0012858693126609672\n",
            "train loss:0.0028853605479576185\n",
            "train loss:0.01053570923808067\n",
            "train loss:0.0015415794199833647\n",
            "train loss:0.013200637825072352\n",
            "train loss:0.006412575212842657\n",
            "train loss:0.0020226579122609688\n",
            "train loss:0.00030213644521446913\n",
            "train loss:0.0008963942652708158\n",
            "train loss:0.0007943442662214251\n",
            "train loss:0.00011012698458517611\n",
            "train loss:0.002959766803375194\n",
            "train loss:0.0038680446474112546\n",
            "train loss:0.0016374124558243546\n",
            "train loss:0.003760734283266761\n",
            "train loss:0.0010155466421362894\n",
            "train loss:0.003962967017009072\n",
            "train loss:0.00010246956395124863\n",
            "train loss:0.0363337894891357\n",
            "train loss:0.0009397111983119571\n",
            "train loss:0.001664396230324827\n",
            "train loss:0.0045708031340131\n",
            "train loss:0.0017363049747430503\n",
            "train loss:0.0015348267707008091\n",
            "train loss:0.0036939284467275555\n",
            "train loss:0.0006773252682182675\n",
            "train loss:0.0016373392698651812\n",
            "train loss:0.00016753403307847775\n",
            "train loss:0.0007270588495539278\n",
            "train loss:0.005361893676669154\n",
            "train loss:0.0006018878176071138\n",
            "train loss:0.0015251884796125441\n",
            "train loss:0.0005442522419629167\n",
            "train loss:0.002020784606018384\n",
            "train loss:0.00047930629467875566\n",
            "train loss:0.0018717182697077642\n",
            "train loss:0.004965769665230567\n",
            "train loss:0.002257323245315519\n",
            "train loss:0.002066236436454143\n",
            "train loss:0.0017769191844229095\n",
            "train loss:0.00711667317224942\n",
            "train loss:0.0002891866210902411\n",
            "train loss:5.2810854703281e-05\n",
            "train loss:0.0009648066431541623\n",
            "train loss:0.006533332181128143\n",
            "train loss:0.0015254022551518304\n",
            "train loss:0.0033426355568867677\n",
            "train loss:9.52930573580909e-05\n",
            "train loss:0.0002715803140120533\n",
            "train loss:0.003196109641311852\n",
            "train loss:0.002198552337097529\n",
            "train loss:0.012270942558732154\n",
            "train loss:0.0005917351464130306\n",
            "train loss:0.002041637932932863\n",
            "train loss:0.003423183214779574\n",
            "train loss:0.0006774511230407697\n",
            "train loss:0.004244555865715785\n",
            "train loss:0.0009105100934868535\n",
            "train loss:0.0029226432223798077\n",
            "train loss:0.018925528777625558\n",
            "train loss:0.00025928122315219034\n",
            "train loss:0.0015748323752360729\n",
            "train loss:0.0002478075068804046\n",
            "train loss:0.003465048501094367\n",
            "train loss:0.003557187548626624\n",
            "train loss:0.04366204658447106\n",
            "train loss:0.004201084072579397\n",
            "train loss:0.0008784824311650521\n",
            "train loss:0.00020077005189373238\n",
            "train loss:5.417841078767374e-05\n",
            "train loss:0.0005860211234262464\n",
            "train loss:5.335232993838605e-05\n",
            "train loss:0.00039692564709373977\n",
            "train loss:0.00015454293100150814\n",
            "train loss:0.0016361231765877312\n",
            "train loss:0.044841639246486625\n",
            "train loss:0.0006064087395518543\n",
            "train loss:0.004349401140490662\n",
            "train loss:0.0001450480930397773\n",
            "train loss:0.002618961728572084\n",
            "train loss:0.0002542144740306354\n",
            "train loss:0.013141711143004765\n",
            "train loss:0.015429394244621388\n",
            "train loss:0.0001382159040835094\n",
            "train loss:0.0015420421606176277\n",
            "train loss:0.0005193419303282298\n",
            "train loss:0.0006834545458901963\n",
            "train loss:0.0008384255636019539\n",
            "train loss:0.0013611859412090749\n",
            "train loss:0.002518816519763275\n",
            "train loss:0.00785033072134557\n",
            "train loss:0.002355322820914449\n",
            "train loss:0.000328929280710814\n",
            "train loss:0.00024164822763170438\n",
            "train loss:0.0015390309134107477\n",
            "train loss:0.005381225312384004\n",
            "train loss:0.0003565027412736526\n",
            "train loss:0.0010629579171371278\n",
            "train loss:0.010415819833885722\n",
            "train loss:0.00384652140192667\n",
            "train loss:0.00010313165886630704\n",
            "train loss:0.00021096216255055302\n",
            "train loss:0.001923337073014673\n",
            "train loss:0.0035243606171134066\n",
            "train loss:0.0005417935169978006\n",
            "train loss:0.0015468066768236604\n",
            "train loss:0.0013731988912865538\n",
            "train loss:0.005455559905963536\n",
            "train loss:0.003487106778048141\n",
            "train loss:0.004795270976332921\n",
            "train loss:0.002018671575144064\n",
            "train loss:0.0026459728609476135\n",
            "train loss:0.00026041984904381953\n",
            "train loss:0.0022650934950783535\n",
            "train loss:0.003807937071956754\n",
            "train loss:0.0005706635062156981\n",
            "train loss:0.003682956051265307\n",
            "train loss:0.0003291039510855245\n",
            "train loss:0.003343875102208407\n",
            "train loss:0.0017051482678553285\n",
            "train loss:0.006034950636255262\n",
            "train loss:0.023495165057510006\n",
            "train loss:0.004971681009614374\n",
            "train loss:0.00337113211569345\n",
            "train loss:0.004259832644371756\n",
            "train loss:0.0022547129884086027\n",
            "train loss:0.0007103060492226722\n",
            "train loss:0.000992785741006271\n",
            "train loss:0.0015863960274505508\n",
            "train loss:0.00021181946225225457\n",
            "train loss:0.0019514414567119067\n",
            "train loss:0.0015923320102173742\n",
            "train loss:0.0010666130615454921\n",
            "train loss:0.0008375480166807266\n",
            "train loss:0.0013061858329956751\n",
            "train loss:0.00014796419527268148\n",
            "train loss:0.002189645279243823\n",
            "train loss:0.0017756995547746146\n",
            "train loss:0.001312409981247021\n",
            "train loss:0.0004993393058700209\n",
            "train loss:0.0008843294196801602\n",
            "train loss:0.0012067691160785304\n",
            "train loss:8.768348516252668e-05\n",
            "train loss:0.002758396897188066\n",
            "train loss:0.0007949922883516289\n",
            "train loss:0.00033142597994718947\n",
            "train loss:0.0034811363517773925\n",
            "train loss:0.004783194080040864\n",
            "train loss:0.000220296168240489\n",
            "train loss:0.0020937258975337976\n",
            "train loss:0.001323460752162554\n",
            "train loss:0.0010318593686536007\n",
            "train loss:0.005032842650654928\n",
            "train loss:0.006434037719136059\n",
            "train loss:0.0015355766825350837\n",
            "train loss:0.004584408346649552\n",
            "train loss:0.00046921498718456903\n",
            "train loss:0.0005159628817754591\n",
            "train loss:0.0026000486735199236\n",
            "train loss:0.006917293603146941\n",
            "train loss:0.003830913564488393\n",
            "train loss:0.0003633163563762829\n",
            "train loss:6.024261452165999e-05\n",
            "train loss:5.775687520695097e-05\n",
            "train loss:0.00027936295373319687\n",
            "train loss:0.001402716512140183\n",
            "train loss:0.008982703688647602\n",
            "train loss:0.0021449620301587195\n",
            "train loss:0.0032937683303863774\n",
            "train loss:0.003984564662814484\n",
            "train loss:0.0008448892996256632\n",
            "train loss:0.0009718019563133715\n",
            "train loss:0.0005391599635301709\n",
            "train loss:0.0011332091198023712\n",
            "train loss:0.0006355141722194486\n",
            "train loss:0.0012753114201344692\n",
            "train loss:0.0014811975832461316\n",
            "train loss:0.0005249850523026949\n",
            "train loss:0.002946724523621776\n",
            "train loss:0.005223088858354522\n",
            "train loss:0.0007685891653912965\n",
            "train loss:0.0009629858881014898\n",
            "train loss:8.84297896993817e-05\n",
            "train loss:0.0008166989049503267\n",
            "train loss:0.0010526678915571712\n",
            "train loss:0.0011826576117528848\n",
            "train loss:0.0010787112595260656\n",
            "train loss:0.00013309533495400882\n",
            "train loss:0.0015564409548146053\n",
            "train loss:0.0010749227833595327\n",
            "train loss:0.0014232089383181413\n",
            "train loss:0.0015338622553545444\n",
            "train loss:0.0016918756748689023\n",
            "train loss:0.00017703054799458992\n",
            "train loss:0.0015397547209343215\n",
            "train loss:0.001233763152097893\n",
            "train loss:0.0046828095236903546\n",
            "train loss:0.0002252099924622334\n",
            "train loss:0.00017213006997386106\n",
            "train loss:0.0015073895350576863\n",
            "train loss:0.0015882187511847693\n",
            "train loss:0.004284425362614827\n",
            "train loss:0.0005458947251994253\n",
            "train loss:0.0038100771434465713\n",
            "train loss:9.166184318106403e-05\n",
            "train loss:0.0024428694110444986\n",
            "train loss:0.002917621505558883\n",
            "train loss:0.00014331475595422217\n",
            "train loss:0.0017140097083992096\n",
            "train loss:0.0008075956924781579\n",
            "train loss:2.7407448065239144e-05\n",
            "train loss:0.00029122081492573564\n",
            "train loss:0.0077325980135477335\n",
            "train loss:0.002100546754959058\n",
            "train loss:0.003418116682430755\n",
            "train loss:0.005622964521070037\n",
            "train loss:0.0002859465561959916\n",
            "train loss:0.0033806297414857842\n",
            "train loss:0.031418660802019466\n",
            "train loss:0.0012070891963007905\n",
            "train loss:6.865761987745234e-05\n",
            "train loss:0.002388748514915012\n",
            "train loss:0.0016315860382247994\n",
            "train loss:0.000966229277914899\n",
            "train loss:0.0014700598460583656\n",
            "train loss:0.003910014745568388\n",
            "train loss:0.0013971678192024697\n",
            "train loss:0.002119661487498843\n",
            "train loss:0.0003436050890349352\n",
            "train loss:0.014819852050665262\n",
            "train loss:0.0001483579569731331\n",
            "train loss:0.00396997222681191\n",
            "train loss:0.0032681650369051577\n",
            "train loss:0.0012645375518445898\n",
            "train loss:0.0028490022680342053\n",
            "train loss:0.000601230446599779\n",
            "train loss:0.0020469014208911677\n",
            "train loss:0.0008974170251135234\n",
            "train loss:0.0003189296560737209\n",
            "train loss:0.0012763826861368515\n",
            "train loss:0.0009948177658546309\n",
            "train loss:0.0001974492224706141\n",
            "train loss:0.00186435932807042\n",
            "train loss:0.0016094643023753215\n",
            "train loss:0.003954345777611607\n",
            "train loss:3.28681255258752e-05\n",
            "train loss:0.00022742645261538037\n",
            "train loss:0.0034418751751046045\n",
            "train loss:0.002796631631400612\n",
            "train loss:0.0005349010458297648\n",
            "train loss:0.0027179542552077775\n",
            "train loss:0.00043212910838951574\n",
            "train loss:0.001139778610728345\n",
            "train loss:0.00011714092913242991\n",
            "train loss:0.0032008529032982484\n",
            "train loss:0.0012246602577833776\n",
            "train loss:0.002782796549242168\n",
            "train loss:0.0004038083267259221\n",
            "train loss:0.0033484013755878785\n",
            "train loss:0.002664917504805766\n",
            "train loss:0.007568320545571119\n",
            "train loss:0.0006710318080716274\n",
            "train loss:0.0011992173328874197\n",
            "train loss:0.0038445808116047897\n",
            "train loss:0.00041358201088269764\n",
            "train loss:0.0015235349844306047\n",
            "train loss:0.00017499174312838805\n",
            "train loss:8.874094777916562e-05\n",
            "train loss:0.002719738239116086\n",
            "train loss:0.0002841689723258953\n",
            "train loss:0.00017412545542335172\n",
            "train loss:0.0014627667661346253\n",
            "train loss:0.0003089196750166806\n",
            "train loss:0.0007417094428976181\n",
            "train loss:0.002753879143702622\n",
            "train loss:0.0010241898224164865\n",
            "train loss:0.0016321951526198798\n",
            "train loss:0.0022424775991427516\n",
            "train loss:0.0005283918537441592\n",
            "train loss:0.0012645962890111885\n",
            "train loss:0.007487773665548833\n",
            "train loss:6.0636544718953395e-05\n",
            "train loss:0.0006893393206692704\n",
            "train loss:0.0002868864690270149\n",
            "train loss:0.0010481495893325244\n",
            "train loss:0.009424336573209959\n",
            "train loss:0.0014739698984635634\n",
            "train loss:0.0012362809880675204\n",
            "train loss:0.0019315980073859344\n",
            "train loss:5.6347726419982476e-05\n",
            "train loss:0.000940738972207384\n",
            "train loss:0.00208902265591981\n",
            "train loss:0.0023415194125076025\n",
            "train loss:0.0014219942697559454\n",
            "train loss:0.0005698252566260204\n",
            "train loss:0.002963659871661072\n",
            "train loss:0.000751882677801375\n",
            "train loss:0.0007986445876089431\n",
            "train loss:0.0009081986618537607\n",
            "train loss:0.0010113629109583632\n",
            "train loss:0.0027015779087088015\n",
            "train loss:0.0017363366987713392\n",
            "train loss:6.641417830480388e-05\n",
            "train loss:5.973864520838641e-05\n",
            "train loss:0.0006193571766843583\n",
            "train loss:0.008717790614713958\n",
            "train loss:0.0006686439197938086\n",
            "train loss:0.00012129063298810049\n",
            "train loss:0.0010129933966251325\n",
            "train loss:0.001956771229856701\n",
            "train loss:0.0014180379642560676\n",
            "train loss:0.0006195623826341163\n",
            "train loss:0.00020560697314338727\n",
            "train loss:9.995076083252672e-05\n",
            "train loss:0.020756962003476386\n",
            "train loss:0.000676530291999455\n",
            "train loss:0.00024404585737466708\n",
            "train loss:0.005083641876760742\n",
            "train loss:0.0015964406999338834\n",
            "train loss:0.0006909986946870839\n",
            "train loss:0.0007146220133636313\n",
            "train loss:0.002177393039005969\n",
            "train loss:0.00017177260399799468\n",
            "train loss:0.0011874606251153284\n",
            "train loss:0.005297602724331957\n",
            "train loss:0.0025305212949871046\n",
            "train loss:0.0008282508346697626\n",
            "train loss:0.0004964280737222536\n",
            "train loss:0.011145583779449826\n",
            "train loss:0.00018571266435213964\n",
            "train loss:0.00026488276390450706\n",
            "train loss:0.00410051403412288\n",
            "train loss:0.0002720872420375313\n",
            "train loss:0.00010967862801986841\n",
            "train loss:0.0006185997796268761\n",
            "train loss:0.012072270055167983\n",
            "train loss:0.00022055615108999436\n",
            "train loss:5.8984574564282155e-05\n",
            "train loss:0.0004418484821077712\n",
            "train loss:0.0011644515117864463\n",
            "train loss:0.006752523631960116\n",
            "train loss:0.000899122506939116\n",
            "train loss:0.0011254822318733924\n",
            "train loss:0.0001283408420239013\n",
            "train loss:0.00019187235218145075\n",
            "train loss:0.0014348247786618528\n",
            "train loss:0.0003025703529291001\n",
            "train loss:0.00010080331675071161\n",
            "train loss:0.00046624665180143923\n",
            "train loss:0.0165979092869868\n",
            "train loss:0.0008297577316653561\n",
            "train loss:0.008372436681997658\n",
            "train loss:0.0007392891073104155\n",
            "train loss:0.0009681475731937539\n",
            "train loss:0.002574139312922708\n",
            "train loss:0.00016631107150264665\n",
            "train loss:0.0005095269599333554\n",
            "train loss:0.0018110003971321\n",
            "train loss:0.0001950836687052299\n",
            "train loss:0.004583329506116787\n",
            "train loss:0.00014051351273691175\n",
            "train loss:0.000653110891375587\n",
            "train loss:0.006037569026878943\n",
            "train loss:0.0011799323419938448\n",
            "train loss:0.00018691158150376558\n",
            "train loss:0.0005547422961894479\n",
            "train loss:0.00015447940747136382\n",
            "train loss:0.0005864259227598705\n",
            "train loss:0.00023552545050332396\n",
            "train loss:0.001724996257910283\n",
            "train loss:0.0004958768106776462\n",
            "train loss:0.0003598649173223768\n",
            "train loss:0.003089685808972093\n",
            "train loss:0.002814391957316681\n",
            "train loss:0.0011486411108140556\n",
            "train loss:6.583180739553904e-05\n",
            "train loss:0.0001676246070271856\n",
            "train loss:0.0006771041834993995\n",
            "train loss:0.001665386292387524\n",
            "train loss:0.002239316013765444\n",
            "train loss:4.768500929648668e-05\n",
            "train loss:0.0006838820174016715\n",
            "train loss:0.00042208013774632387\n",
            "train loss:0.0012618383412057851\n",
            "train loss:0.0014307548447070604\n",
            "train loss:0.0019964596765298364\n",
            "train loss:0.004884024361888799\n",
            "train loss:0.0013951921903737196\n",
            "train loss:0.0004935397613019737\n",
            "train loss:0.00044312388930633275\n",
            "train loss:0.000612381082150004\n",
            "train loss:0.00014534966763991766\n",
            "train loss:0.000562473427705321\n",
            "train loss:0.0010884679891656438\n",
            "train loss:3.6690080948865655e-05\n",
            "train loss:0.0003257239288325646\n",
            "train loss:0.0002924714032897792\n",
            "train loss:0.0004828058083695613\n",
            "train loss:0.0002275289166130541\n",
            "train loss:0.0005273098349266303\n",
            "train loss:0.003860159388411917\n",
            "train loss:0.0005596932609552136\n",
            "train loss:0.0016126081216293136\n",
            "train loss:0.0020858675971227995\n",
            "train loss:0.00014540745254264906\n",
            "train loss:0.00047639180231981627\n",
            "train loss:0.001869934747369306\n",
            "train loss:0.002535820957909375\n",
            "train loss:0.0010157419216651553\n",
            "train loss:0.002120712058328273\n",
            "train loss:0.000795866477969438\n",
            "train loss:0.0018584779270866744\n",
            "train loss:0.0013619992452734934\n",
            "train loss:0.00046906842305529056\n",
            "train loss:0.00044155288347264774\n",
            "train loss:0.00012669432258528713\n",
            "train loss:0.00181955027683972\n",
            "train loss:0.0019783952952778776\n",
            "train loss:0.002261080515292739\n",
            "train loss:0.0011287051190466196\n",
            "train loss:0.0009521022939706716\n",
            "train loss:0.0027746318196981007\n",
            "train loss:0.0004736480697319683\n",
            "train loss:0.000727052559974121\n",
            "train loss:0.0017134923631435153\n",
            "train loss:0.0023362064584767305\n",
            "train loss:0.0025218483129178845\n",
            "train loss:0.0004230093203266627\n",
            "train loss:0.000795367235355237\n",
            "train loss:0.00016541631571121836\n",
            "train loss:0.00174807445088676\n",
            "train loss:0.003084221643352825\n",
            "train loss:0.0028437869838141205\n",
            "train loss:0.0028341811427871283\n",
            "train loss:0.00034327437902527706\n",
            "train loss:0.0018017261599492405\n",
            "train loss:0.000273844910266078\n",
            "train loss:0.0003652748129195284\n",
            "train loss:9.083675699943961e-05\n",
            "train loss:0.0016267925839355466\n",
            "train loss:0.0009573175551069801\n",
            "train loss:0.0009010957624781572\n",
            "train loss:0.00029046944779644805\n",
            "train loss:0.00019883734737295982\n",
            "train loss:0.0009818044533631447\n",
            "train loss:0.0001829539674043893\n",
            "train loss:0.0021524384956083087\n",
            "train loss:0.00014077420234696436\n",
            "train loss:0.00017317740559505006\n",
            "train loss:0.0008173674602221692\n",
            "train loss:0.0006998915930497538\n",
            "train loss:0.00027785987239821843\n",
            "train loss:0.0006968922438090884\n",
            "train loss:0.0003379625812857571\n",
            "train loss:0.0005183320772168063\n",
            "train loss:0.039754369052802926\n",
            "train loss:0.0002143924627494652\n",
            "train loss:0.001992014537774233\n",
            "train loss:0.0018427761615123527\n",
            "train loss:0.0008194912915068361\n",
            "train loss:0.0013819451585195223\n",
            "train loss:0.00010988925553976072\n",
            "train loss:0.00029733969410230014\n",
            "train loss:0.0002706772824884276\n",
            "train loss:0.0016788708441202544\n",
            "train loss:0.008664433378077846\n",
            "train loss:7.414036371836111e-05\n",
            "train loss:0.0002449198162935191\n",
            "train loss:0.0007828301556497655\n",
            "train loss:0.0004166557347503285\n",
            "train loss:0.0015832048232578269\n",
            "train loss:0.000973815417518602\n",
            "train loss:0.0002519507478867006\n",
            "train loss:0.0011538043323221178\n",
            "train loss:0.00016363642555770753\n",
            "train loss:0.0007159085151040652\n",
            "train loss:0.0007393159673940903\n",
            "train loss:0.004697668628290426\n",
            "train loss:0.002588866011526839\n",
            "train loss:0.0010417575954622706\n",
            "train loss:0.016794200889283472\n",
            "train loss:0.0005704193779254805\n",
            "train loss:0.0007900365521847819\n",
            "train loss:0.0018577708010131743\n",
            "train loss:0.0018179728400291798\n",
            "train loss:0.00028804955389566357\n",
            "train loss:0.00014419849213978433\n",
            "train loss:0.00011652105770353287\n",
            "train loss:0.0007989697004996675\n",
            "train loss:0.0001277813762262754\n",
            "train loss:0.0010717026370093482\n",
            "train loss:0.00014275265003682006\n",
            "train loss:0.0004582704040880224\n",
            "train loss:0.0007697224908675037\n",
            "train loss:0.001286502440492681\n",
            "train loss:0.02256442751205098\n",
            "train loss:0.00020390584655317995\n",
            "train loss:7.826534473596838e-05\n",
            "train loss:0.0011327407124517696\n",
            "train loss:0.003365547779083761\n",
            "train loss:0.0010859829578780207\n",
            "train loss:0.0007851717142231221\n",
            "train loss:0.00013005321407632417\n",
            "train loss:0.00010203284915571071\n",
            "train loss:0.00026733514826854577\n",
            "train loss:0.0025137829301219734\n",
            "train loss:0.0007717813688428836\n",
            "train loss:0.0004143152145627638\n",
            "train loss:0.0004776086856073329\n",
            "train loss:0.0018515310234928132\n",
            "train loss:0.004394903375464325\n",
            "train loss:0.000615267461115167\n",
            "train loss:0.00020426358822815818\n",
            "train loss:0.000268062995041152\n",
            "train loss:0.0005167124401563002\n",
            "train loss:0.002220131866263434\n",
            "train loss:0.002042617629896876\n",
            "train loss:0.0004446597276695232\n",
            "train loss:8.016024487890949e-05\n",
            "train loss:8.788979874735886e-05\n",
            "train loss:0.0009247786878815111\n",
            "train loss:0.0017108472803093228\n",
            "train loss:0.00026470051877392224\n",
            "train loss:0.0025491115007618735\n",
            "train loss:0.00034305661256752026\n",
            "train loss:3.423294933901458e-05\n",
            "train loss:0.0004915218724822829\n",
            "train loss:0.0011235565973108025\n",
            "train loss:0.0001122258364704307\n",
            "train loss:8.598250088788648e-05\n",
            "train loss:0.0001515220067371547\n",
            "train loss:0.00018790267322217177\n",
            "train loss:0.0014186553769022139\n",
            "=== epoch:19, train acc:0.999, test acc:0.986 ===\n",
            "train loss:5.7649500085879726e-05\n",
            "train loss:0.00020115953540253135\n",
            "train loss:0.0002653257582134716\n",
            "train loss:0.00013804841263740473\n",
            "train loss:0.0006233141053884236\n",
            "train loss:0.0004255470103681323\n",
            "train loss:0.00024794970301325776\n",
            "train loss:0.00024593485215414077\n",
            "train loss:0.00080480211098529\n",
            "train loss:8.121380588522782e-05\n",
            "train loss:0.0022159903099260325\n",
            "train loss:0.0006272932384658504\n",
            "train loss:0.002390769535703721\n",
            "train loss:0.0004228018295579952\n",
            "train loss:0.00107183552951281\n",
            "train loss:4.4099976313104146e-05\n",
            "train loss:0.0025101125984347333\n",
            "train loss:1.3838778428739054e-05\n",
            "train loss:0.000660257104214724\n",
            "train loss:0.0007521227798714135\n",
            "train loss:5.642616590329171e-05\n",
            "train loss:0.009597729864024945\n",
            "train loss:0.00024087050903746595\n",
            "train loss:0.0003713808435643771\n",
            "train loss:0.0001412919463326366\n",
            "train loss:0.00017244030588382806\n",
            "train loss:7.061552405287499e-05\n",
            "train loss:0.0006836019457756714\n",
            "train loss:0.0007397946965614138\n",
            "train loss:0.0004160398656421378\n",
            "train loss:0.001409653595450092\n",
            "train loss:0.00040425124359776154\n",
            "train loss:0.00024042789780146388\n",
            "train loss:0.0009507904054273031\n",
            "train loss:0.0024184856190558714\n",
            "train loss:4.193637455297044e-05\n",
            "train loss:0.0002556689572251308\n",
            "train loss:0.003474346319625316\n",
            "train loss:0.000371147795795359\n",
            "train loss:0.005038670622992087\n",
            "train loss:0.0051915554751829065\n",
            "train loss:0.002812529750550685\n",
            "train loss:0.000614079860270323\n",
            "train loss:0.003701930279102998\n",
            "train loss:0.0004436750503278655\n",
            "train loss:0.00027606034183652714\n",
            "train loss:0.0008230753253894257\n",
            "train loss:0.001566116702688064\n",
            "train loss:0.001374596883665246\n",
            "train loss:0.0012990698757135253\n",
            "train loss:0.00028580762613192567\n",
            "train loss:0.001260758639355242\n",
            "train loss:0.0002757803039091845\n",
            "train loss:0.0007682944869551351\n",
            "train loss:0.0010524111188515886\n",
            "train loss:0.005659501663434212\n",
            "train loss:0.0005956397817095094\n",
            "train loss:0.0007282182408975929\n",
            "train loss:0.0019011924419886147\n",
            "train loss:0.0001912844117854537\n",
            "train loss:0.0012752997299774987\n",
            "train loss:0.0010333824056742303\n",
            "train loss:0.0005096140033885933\n",
            "train loss:0.00021706135938071554\n",
            "train loss:0.0001350575491060409\n",
            "train loss:0.00037781400193101435\n",
            "train loss:3.46307424873136e-05\n",
            "train loss:0.0005224150721719302\n",
            "train loss:0.00209001000281647\n",
            "train loss:0.001100053128472379\n",
            "train loss:0.00029160502034926336\n",
            "train loss:0.0012505294231913361\n",
            "train loss:9.638972348685804e-05\n",
            "train loss:0.0016373039399544026\n",
            "train loss:0.002521849107788992\n",
            "train loss:0.00019591902040203105\n",
            "train loss:1.9593683897050814e-05\n",
            "train loss:0.0006942033592600551\n",
            "train loss:0.005843675067213455\n",
            "train loss:0.0030928906689110876\n",
            "train loss:0.001369463789562997\n",
            "train loss:0.0034862293804488933\n",
            "train loss:0.00010790740122984315\n",
            "train loss:0.0001484413266299186\n",
            "train loss:0.0001803143888657092\n",
            "train loss:0.00019877735475970682\n",
            "train loss:0.0006646108067594094\n",
            "train loss:8.149786716459913e-05\n",
            "train loss:0.00022991094718219447\n",
            "train loss:0.0008668388203601239\n",
            "train loss:0.0032313242202834363\n",
            "train loss:0.0003243489244463095\n",
            "train loss:0.0003571054817112007\n",
            "train loss:0.0027214240955359703\n",
            "train loss:0.004860059278450375\n",
            "train loss:0.0007091734398295989\n",
            "train loss:0.000296288396497025\n",
            "train loss:4.512196569285191e-05\n",
            "train loss:8.039353474177479e-05\n",
            "train loss:0.001291302458676499\n",
            "train loss:0.0002265397601073827\n",
            "train loss:0.0006584375297555415\n",
            "train loss:0.0005594864879694821\n",
            "train loss:0.0012132792290088618\n",
            "train loss:3.371725116047305e-05\n",
            "train loss:0.00012139815009252384\n",
            "train loss:0.0015405807924293831\n",
            "train loss:0.001103543946261089\n",
            "train loss:0.0012373973474280404\n",
            "train loss:0.00013206499550864355\n",
            "train loss:0.001413287207265336\n",
            "train loss:0.0036290632251732174\n",
            "train loss:0.0006645254539500464\n",
            "train loss:0.0003765256513073985\n",
            "train loss:0.0064626832957876525\n",
            "train loss:0.0010190401080521416\n",
            "train loss:0.010486910165591163\n",
            "train loss:0.003089036679196522\n",
            "train loss:0.00024972020505793107\n",
            "train loss:0.00227404579000183\n",
            "train loss:0.00031355650913859277\n",
            "train loss:0.0007952369374465068\n",
            "train loss:0.0008532873457617682\n",
            "train loss:0.00024869521785005544\n",
            "train loss:0.0007576499889595098\n",
            "train loss:0.015085817625101075\n",
            "train loss:0.0012889163245911755\n",
            "train loss:0.0002488300791061893\n",
            "train loss:3.745269760274525e-06\n",
            "train loss:0.00014843033756212307\n",
            "train loss:0.0009780816647939746\n",
            "train loss:0.0013444569784650307\n",
            "train loss:0.004200428168060574\n",
            "train loss:0.006753964793862794\n",
            "train loss:0.0024210932635472344\n",
            "train loss:0.0007590303145088522\n",
            "train loss:0.00023402773653048616\n",
            "train loss:0.00024185821295576927\n",
            "train loss:0.0001756816341154553\n",
            "train loss:0.00021450890740229892\n",
            "train loss:0.00031721135630636307\n",
            "train loss:8.203871615265845e-05\n",
            "train loss:0.00015333636766866595\n",
            "train loss:0.02775493578251691\n",
            "train loss:8.576956445817016e-05\n",
            "train loss:0.0005432474023915137\n",
            "train loss:0.000483555469544144\n",
            "train loss:0.0007732071978995221\n",
            "train loss:0.0012288466105067229\n",
            "train loss:0.001959229472174534\n",
            "train loss:0.0029856106551142013\n",
            "train loss:0.005204599987387339\n",
            "train loss:3.644699128078317e-05\n",
            "train loss:0.0005401830042684885\n",
            "train loss:7.622509310139012e-05\n",
            "train loss:9.632324646799064e-05\n",
            "train loss:0.0023375955205199267\n",
            "train loss:0.005608738667375137\n",
            "train loss:0.00029945672583998255\n",
            "train loss:0.00024286743608153112\n",
            "train loss:0.0011548340538723193\n",
            "train loss:0.0008666119623425099\n",
            "train loss:0.00582589921186447\n",
            "train loss:0.0014660521478322968\n",
            "train loss:0.0003556331497129583\n",
            "train loss:0.0007489323600928868\n",
            "train loss:0.016656337050509954\n",
            "train loss:0.0011442688739942443\n",
            "train loss:0.0022848942594544458\n",
            "train loss:0.005553340079064286\n",
            "train loss:0.003237141284571433\n",
            "train loss:0.0009286122802379843\n",
            "train loss:0.0031653294524251075\n",
            "train loss:0.00038742155708046893\n",
            "train loss:0.00118733826366761\n",
            "train loss:0.004132607695044362\n",
            "train loss:0.0004207025158011143\n",
            "train loss:0.0030133158450984542\n",
            "train loss:8.075389761638305e-05\n",
            "train loss:0.0005378662911134327\n",
            "train loss:0.0018617701437684921\n",
            "train loss:0.006123453960086226\n",
            "train loss:0.0013493921960392946\n",
            "train loss:0.0027910359052137975\n",
            "train loss:0.0005904634380599427\n",
            "train loss:0.0003219252437789342\n",
            "train loss:7.899763466768244e-05\n",
            "train loss:0.0001180416289436931\n",
            "train loss:0.009280873444175518\n",
            "train loss:0.00015627406923784167\n",
            "train loss:0.0009886299081066474\n",
            "train loss:0.0005581158274773701\n",
            "train loss:0.0015410494639306912\n",
            "train loss:0.0010151547047340653\n",
            "train loss:0.0022902184151509437\n",
            "train loss:0.00018758540039488756\n",
            "train loss:0.0001493258255719998\n",
            "train loss:0.0011989399944795386\n",
            "train loss:0.0011345594784891743\n",
            "train loss:0.0021901130898744816\n",
            "train loss:0.013130737586235026\n",
            "train loss:0.0009059426144330454\n",
            "train loss:0.0338530106317263\n",
            "train loss:0.0001488506188642537\n",
            "train loss:0.0009729292284765438\n",
            "train loss:0.0010114882132712104\n",
            "train loss:0.005276890026632064\n",
            "train loss:0.0006255678479344647\n",
            "train loss:0.00033998453536446575\n",
            "train loss:0.0010002418164280476\n",
            "train loss:0.0005601707528820747\n",
            "train loss:0.00014328491289264926\n",
            "train loss:0.0029987410960032406\n",
            "train loss:0.0006433020074829145\n",
            "train loss:0.0010785562821803742\n",
            "train loss:0.00046762069733407755\n",
            "train loss:0.0004280562244611261\n",
            "train loss:0.002932313436142772\n",
            "train loss:0.001497010729006242\n",
            "train loss:0.0024169038123630438\n",
            "train loss:0.0012775656837367196\n",
            "train loss:0.002572916418374173\n",
            "train loss:0.001936052951955842\n",
            "train loss:0.009873576603336896\n",
            "train loss:0.0012116104421636402\n",
            "train loss:0.001295799141004593\n",
            "train loss:0.0005355116584512922\n",
            "train loss:0.0005312590310879175\n",
            "train loss:0.00011107734691122106\n",
            "train loss:0.00030058181681255137\n",
            "train loss:0.0007801841853138825\n",
            "train loss:0.0006920583442950655\n",
            "train loss:0.0004936532718506976\n",
            "train loss:0.00021701718851177353\n",
            "train loss:0.0015884097429107432\n",
            "train loss:0.003791410657441334\n",
            "train loss:0.0007003357395347405\n",
            "train loss:0.00041315731067149747\n",
            "train loss:0.0005775946559436068\n",
            "train loss:0.0030021607001740397\n",
            "train loss:0.0008530450459002059\n",
            "train loss:0.0020032300022459998\n",
            "train loss:0.000792183777858848\n",
            "train loss:0.0014058008731537713\n",
            "train loss:0.0002732466681490351\n",
            "train loss:0.00012393229498007261\n",
            "train loss:0.00016571850034125739\n",
            "train loss:0.002209911401859122\n",
            "train loss:0.0021485311369367696\n",
            "train loss:0.0001912002879084756\n",
            "train loss:0.0004904473004552456\n",
            "train loss:0.0001839298089921685\n",
            "train loss:0.0009704832380057466\n",
            "train loss:0.0004927041223771998\n",
            "train loss:0.004215305499507841\n",
            "train loss:0.00012312814765346505\n",
            "train loss:0.00036570741439667984\n",
            "train loss:0.0003544593119537861\n",
            "train loss:3.0589553841634583e-05\n",
            "train loss:0.0018793445972266562\n",
            "train loss:0.0016390440484761407\n",
            "train loss:0.0003585065877594991\n",
            "train loss:0.0013258135667850567\n",
            "train loss:0.0012389169245478606\n",
            "train loss:0.0006569698342319299\n",
            "train loss:0.0017976032253032661\n",
            "train loss:0.0005344992465707649\n",
            "train loss:0.00011359014084264136\n",
            "train loss:0.002388107982425829\n",
            "train loss:0.0004946966869218396\n",
            "train loss:0.00011696271451984928\n",
            "train loss:0.0001665655378857916\n",
            "train loss:0.0008602274074798159\n",
            "train loss:0.001793724966713\n",
            "train loss:0.00017508702210242765\n",
            "train loss:0.0018290199268398938\n",
            "train loss:0.00021272079239490683\n",
            "train loss:0.0025235793534698295\n",
            "train loss:8.672277601540134e-05\n",
            "train loss:0.001190274336479993\n",
            "train loss:0.000583314343644508\n",
            "train loss:7.856223607146661e-05\n",
            "train loss:0.000674147939861918\n",
            "train loss:0.0004623331286855964\n",
            "train loss:0.0011870928972882858\n",
            "train loss:2.3027599157329418e-05\n",
            "train loss:0.001343168787374547\n",
            "train loss:7.668466253724467e-05\n",
            "train loss:0.00036227917614962035\n",
            "train loss:0.00034702518584518597\n",
            "train loss:0.0002194975360602607\n",
            "train loss:0.0005730065982548609\n",
            "train loss:0.00011547651919627013\n",
            "train loss:0.0019411093767817552\n",
            "train loss:0.00040323368007722703\n",
            "train loss:0.00029657382184665156\n",
            "train loss:0.002475543499875924\n",
            "train loss:0.00038854853173887717\n",
            "train loss:0.00010897199694293154\n",
            "train loss:0.0003714590090158517\n",
            "train loss:0.0015929442910112795\n",
            "train loss:0.0011724742570601676\n",
            "train loss:0.0009041049469794985\n",
            "train loss:9.404499054930469e-05\n",
            "train loss:0.00033829871024673356\n",
            "train loss:0.00022424016925801527\n",
            "train loss:0.0015850332108849228\n",
            "train loss:0.0014903135301775\n",
            "train loss:0.0030052945899692878\n",
            "train loss:0.0007155588335273009\n",
            "train loss:0.0026026401790375606\n",
            "train loss:0.00012490761558259513\n",
            "train loss:0.0021778073228557343\n",
            "train loss:0.0022090015776968343\n",
            "train loss:0.0002794751306005025\n",
            "train loss:0.001710050394943615\n",
            "train loss:0.0008475240137125438\n",
            "train loss:2.896863096494372e-05\n",
            "train loss:0.0022981232076113643\n",
            "train loss:6.286044282602939e-05\n",
            "train loss:0.00010080648790110329\n",
            "train loss:0.0033741796173027013\n",
            "train loss:0.0004606691815214515\n",
            "train loss:9.379906730709974e-05\n",
            "train loss:0.0001029792631763973\n",
            "train loss:0.0008275058388641724\n",
            "train loss:0.0006874310441068235\n",
            "train loss:0.0004310662374909573\n",
            "train loss:0.0019046812452032046\n",
            "train loss:0.0008024508942990897\n",
            "train loss:0.002190007997510862\n",
            "train loss:6.455704193210945e-05\n",
            "train loss:0.0008524268347311557\n",
            "train loss:0.000254744407207901\n",
            "train loss:0.004792886779535788\n",
            "train loss:0.0002835067534815519\n",
            "train loss:0.0003109806919773864\n",
            "train loss:8.837984439749083e-05\n",
            "train loss:0.0011459703685878688\n",
            "train loss:0.00024369719169537314\n",
            "train loss:0.0011588902605575747\n",
            "train loss:0.0009893205231315321\n",
            "train loss:0.0010557760516574188\n",
            "train loss:0.0011176063187392019\n",
            "train loss:0.0002851679995100209\n",
            "train loss:0.0010614970858714178\n",
            "train loss:0.0001209571653120057\n",
            "train loss:0.00535300581843287\n",
            "train loss:0.0005971137343486108\n",
            "train loss:0.000991120898646919\n",
            "train loss:0.00017827829813132104\n",
            "train loss:0.0009676280961192202\n",
            "train loss:0.00014586062824570927\n",
            "train loss:0.00015091271255895626\n",
            "train loss:0.003236687658476512\n",
            "train loss:0.00016010068786710224\n",
            "train loss:0.0012737033622355557\n",
            "train loss:0.004783309022074341\n",
            "train loss:3.870078093228443e-05\n",
            "train loss:0.00011531969420185358\n",
            "train loss:0.0004975086184726214\n",
            "train loss:0.0018828624353328747\n",
            "train loss:0.00023250736478048334\n",
            "train loss:0.00019428496561766807\n",
            "train loss:0.00010798858974103518\n",
            "train loss:0.0016095937036415342\n",
            "train loss:0.0007374921174901732\n",
            "train loss:0.0013716023922392636\n",
            "train loss:0.00030878933273792783\n",
            "train loss:0.0013776154201116469\n",
            "train loss:0.0004071274969009587\n",
            "train loss:0.0004407350338470362\n",
            "train loss:0.0003525598863434518\n",
            "train loss:0.00011422106142132782\n",
            "train loss:0.0012994692885883968\n",
            "train loss:0.0004213159508345414\n",
            "train loss:0.0007327509169768866\n",
            "train loss:4.365657434558689e-05\n",
            "train loss:2.172979296843455e-05\n",
            "train loss:0.0025722391863873626\n",
            "train loss:9.240908686682141e-05\n",
            "train loss:0.0011945242455887414\n",
            "train loss:0.003046305190433743\n",
            "train loss:0.0026202566022109856\n",
            "train loss:4.181765746289045e-05\n",
            "train loss:0.0005890814572999156\n",
            "train loss:0.0002527169946271327\n",
            "train loss:0.0012193190147889237\n",
            "train loss:0.0007156722026149105\n",
            "train loss:0.0004927925123048718\n",
            "train loss:0.00013998831798284762\n",
            "train loss:0.0029037000675564773\n",
            "train loss:0.0015138680531631794\n",
            "train loss:0.00019049262388961925\n",
            "train loss:0.00027818390158186287\n",
            "train loss:0.00018368817689009084\n",
            "train loss:0.0005031136457960225\n",
            "train loss:0.003549492651309553\n",
            "train loss:9.494554233261189e-05\n",
            "train loss:5.501766699234396e-05\n",
            "train loss:7.328508350128366e-05\n",
            "train loss:0.0008102721656911116\n",
            "train loss:2.2478511874724787e-05\n",
            "train loss:0.0001013184252264691\n",
            "train loss:0.0037760779909447673\n",
            "train loss:0.00047850549903338265\n",
            "train loss:0.0069679097071114185\n",
            "train loss:0.00025456844179611444\n",
            "train loss:0.0027567725940607813\n",
            "train loss:0.0006322937929265851\n",
            "train loss:0.0001481794935557138\n",
            "train loss:0.015149152981257887\n",
            "train loss:0.0031444273209003154\n",
            "train loss:0.00025064362189682233\n",
            "train loss:0.0025642177224183614\n",
            "train loss:0.010863560848639946\n",
            "train loss:0.00025668604538285053\n",
            "train loss:0.0013961684985973782\n",
            "train loss:0.0007553112848997129\n",
            "train loss:0.0001256767839083603\n",
            "train loss:0.005653294712713335\n",
            "train loss:0.0016389153444345216\n",
            "train loss:5.143933567936521e-05\n",
            "train loss:0.00028917637533197094\n",
            "train loss:0.0005516181181445946\n",
            "train loss:0.0005820091385871501\n",
            "train loss:0.0008089174560797433\n",
            "train loss:0.00026904595628906983\n",
            "train loss:0.0008587084577736971\n",
            "train loss:0.0009720938436698315\n",
            "train loss:0.0025427774839813927\n",
            "train loss:0.0014626444232406824\n",
            "train loss:0.00039682408100473693\n",
            "train loss:0.0008141386041440907\n",
            "train loss:0.00378655446454894\n",
            "train loss:0.0004602609899974949\n",
            "train loss:0.0001319340668962949\n",
            "train loss:0.00303886122083578\n",
            "train loss:0.0013493825202102975\n",
            "train loss:0.0008248799172624347\n",
            "train loss:0.0007279003789828313\n",
            "train loss:0.00014322707759072478\n",
            "train loss:0.0020320322481700757\n",
            "train loss:0.00037656240663289786\n",
            "train loss:0.0009479087021939723\n",
            "train loss:0.0001681001927080882\n",
            "train loss:0.0012444263905704925\n",
            "train loss:0.0009655546610419439\n",
            "train loss:0.00011593801600863165\n",
            "train loss:0.0001946208690441998\n",
            "train loss:0.0025774671590415234\n",
            "train loss:0.00034603265593175094\n",
            "train loss:0.0012914133582357628\n",
            "train loss:0.0005875181373784941\n",
            "train loss:0.0006260261716268638\n",
            "train loss:0.0010013203895681598\n",
            "train loss:0.001430103100417728\n",
            "train loss:0.0001330558487824366\n",
            "train loss:0.00018094756315823707\n",
            "train loss:7.240101813887906e-05\n",
            "train loss:0.00036199013005196873\n",
            "train loss:0.0019292981567964607\n",
            "train loss:0.0021149838645073537\n",
            "train loss:0.003034011932724795\n",
            "train loss:0.002517290653764475\n",
            "train loss:0.00021092267410966978\n",
            "train loss:8.066991879044387e-05\n",
            "train loss:0.00048715545917181696\n",
            "train loss:2.363905061173757e-05\n",
            "train loss:2.1401731255393004e-05\n",
            "train loss:0.0002036016432011615\n",
            "train loss:8.357175411357781e-05\n",
            "train loss:0.0011139264571387877\n",
            "train loss:0.00022030683169329732\n",
            "train loss:0.00017363327032761736\n",
            "train loss:0.00018914634965191848\n",
            "train loss:0.00041986040220155964\n",
            "train loss:0.0011646223688254905\n",
            "train loss:0.0014880396545845177\n",
            "train loss:0.00023581283080936291\n",
            "train loss:4.226828016560488e-05\n",
            "train loss:0.00012346576073318972\n",
            "train loss:0.00015121475487058658\n",
            "train loss:1.8453400288207157e-05\n",
            "train loss:0.0006575463212607936\n",
            "train loss:0.00042070640097815944\n",
            "train loss:0.00017668491888947246\n",
            "train loss:0.0017825177857564466\n",
            "train loss:0.0004366771719738837\n",
            "train loss:8.00863793495652e-05\n",
            "train loss:8.074724160701965e-05\n",
            "train loss:0.00047408926643275945\n",
            "train loss:7.267892488962426e-05\n",
            "train loss:0.00014785108418046062\n",
            "train loss:0.0013068312852895247\n",
            "train loss:0.0010164999952529126\n",
            "train loss:0.0010293054195164417\n",
            "train loss:0.0003399962583825107\n",
            "train loss:0.00011128775133031211\n",
            "train loss:9.997719453384523e-05\n",
            "train loss:0.0009398831857654502\n",
            "train loss:0.0007753077909418177\n",
            "train loss:0.002177829372932151\n",
            "train loss:0.000856640941977544\n",
            "train loss:0.003159451354386918\n",
            "train loss:0.0030314600936423987\n",
            "train loss:0.0012611299390890907\n",
            "train loss:7.283837999676651e-05\n",
            "train loss:0.0006746813991166279\n",
            "train loss:1.5748829679751702e-05\n",
            "train loss:3.1895192717307314e-05\n",
            "train loss:0.0016887912769419137\n",
            "train loss:0.00017121329708506984\n",
            "train loss:0.0012506536354657924\n",
            "train loss:0.004266151117140873\n",
            "train loss:0.0027352042353542268\n",
            "train loss:0.0005492237219198187\n",
            "train loss:0.0004094178350566294\n",
            "train loss:0.0015788839853990689\n",
            "train loss:0.00017765943045615926\n",
            "train loss:0.0048814440239727894\n",
            "train loss:0.0026585176095253256\n",
            "train loss:0.005308675800526365\n",
            "train loss:0.0010036211319348837\n",
            "train loss:0.001620222447700388\n",
            "train loss:0.0009093923673367736\n",
            "train loss:0.004453658696180511\n",
            "train loss:0.006549645169167664\n",
            "train loss:0.002880950288014235\n",
            "train loss:0.0002416422088393252\n",
            "train loss:0.000662507287115709\n",
            "train loss:0.0004213487227353943\n",
            "train loss:0.0002009611821469892\n",
            "train loss:0.002022120088580151\n",
            "train loss:0.0015047324247992888\n",
            "train loss:0.026585266183554675\n",
            "train loss:0.0021313763440707206\n",
            "train loss:0.00032975809897598413\n",
            "train loss:1.5032071678220613e-05\n",
            "train loss:0.0004240753173358907\n",
            "train loss:0.00043015487728443865\n",
            "train loss:0.0006239003258928711\n",
            "train loss:0.0001504361998022812\n",
            "train loss:0.006112145834621775\n",
            "train loss:0.0005240179740315592\n",
            "train loss:0.009676593427274643\n",
            "train loss:0.0008542003466705247\n",
            "train loss:0.002271282879940702\n",
            "train loss:0.002143720976512022\n",
            "train loss:0.0003030575360437317\n",
            "train loss:0.004272037242892018\n",
            "train loss:0.00020206305415478235\n",
            "train loss:0.0017437795190462335\n",
            "train loss:0.002183210861094329\n",
            "train loss:0.0012892317058492263\n",
            "train loss:0.0005420753203239291\n",
            "train loss:0.0022030519330567175\n",
            "train loss:0.0005073554147423682\n",
            "train loss:0.0003229455848132125\n",
            "train loss:0.0011193541061261963\n",
            "train loss:0.00011230916690379513\n",
            "train loss:0.0047334720353947755\n",
            "train loss:0.0009271360627699873\n",
            "train loss:0.0002535841916990482\n",
            "train loss:0.003824881544633081\n",
            "train loss:0.0016602021378076576\n",
            "train loss:0.016813071954535967\n",
            "train loss:0.0003657725835066125\n",
            "train loss:0.0012737869693117223\n",
            "train loss:0.0019226883111088415\n",
            "train loss:0.001718744216617377\n",
            "train loss:0.0031700493863876817\n",
            "train loss:0.00131708264301006\n",
            "train loss:0.0036759826942246265\n",
            "train loss:0.004391031285174515\n",
            "train loss:0.0014618915630814572\n",
            "train loss:0.00040309915973665085\n",
            "train loss:3.704278777176715e-05\n",
            "train loss:3.3471044791478156e-05\n",
            "train loss:0.00023692430240407475\n",
            "train loss:0.00048624453293739163\n",
            "train loss:0.0011955227949757199\n",
            "train loss:9.9535786135102e-05\n",
            "train loss:0.0017807214969079476\n",
            "train loss:0.0017454501318178665\n",
            "train loss:0.020474489164477486\n",
            "train loss:0.002765895280343399\n",
            "train loss:0.0003214250497597726\n",
            "train loss:0.001409927440494102\n",
            "train loss:0.003449207357956821\n",
            "train loss:0.0001869442902117794\n",
            "train loss:0.0013200533483986548\n",
            "train loss:0.0007419696385144543\n",
            "train loss:0.001771467101325679\n",
            "train loss:0.0022767572446879382\n",
            "train loss:0.000721730485485197\n",
            "train loss:0.003581157375540546\n",
            "train loss:0.0002276100478501747\n",
            "train loss:0.002932460226601464\n",
            "train loss:0.0018594834124863843\n",
            "=== epoch:20, train acc:1.0, test acc:0.985 ===\n",
            "train loss:0.0007748934494831737\n",
            "train loss:0.0003829132447045508\n",
            "train loss:6.570190363314195e-05\n",
            "train loss:0.002438820565256602\n",
            "train loss:0.0006771365136228985\n",
            "train loss:0.0014821152365049686\n",
            "train loss:0.0021875427766155885\n",
            "train loss:0.00021163402463682693\n",
            "train loss:0.0009519708234573354\n",
            "train loss:0.002458692229681511\n",
            "train loss:0.0010534106601206939\n",
            "train loss:0.0026657593197669293\n",
            "train loss:0.0063082098700171455\n",
            "train loss:0.0012348311508036181\n",
            "train loss:0.0028451395007084777\n",
            "train loss:0.0031867452564556197\n",
            "train loss:4.535129671768662e-05\n",
            "train loss:0.0027612855392997655\n",
            "train loss:0.0001879063824179731\n",
            "train loss:0.0057715281770849445\n",
            "train loss:0.01827969292294726\n",
            "train loss:0.0005632913831273435\n",
            "train loss:0.0003056717370313428\n",
            "train loss:0.000645974476336826\n",
            "train loss:0.0005666001593956022\n",
            "train loss:0.00015139798034095195\n",
            "train loss:0.0002041370853590333\n",
            "train loss:0.0031486197587027765\n",
            "train loss:0.0032395560351309543\n",
            "train loss:0.0015981971293402269\n",
            "train loss:0.0009720299586516658\n",
            "train loss:0.00041338759399216896\n",
            "train loss:0.006872310262577643\n",
            "train loss:0.00018998455769624544\n",
            "train loss:0.0006254279035358948\n",
            "train loss:0.004436569503154102\n",
            "train loss:0.044454862285711695\n",
            "train loss:0.02287805057822537\n",
            "train loss:0.0007809481918809929\n",
            "train loss:0.0018588679139813344\n",
            "train loss:0.0011696765817548341\n",
            "train loss:0.0005009348854010287\n",
            "train loss:0.0013472990482928072\n",
            "train loss:0.00449126316523193\n",
            "train loss:0.0026167472209537683\n",
            "train loss:0.0012173844098812795\n",
            "train loss:0.0007562702727689606\n",
            "train loss:0.0011402436901251447\n",
            "train loss:0.0028273563735942923\n",
            "train loss:0.0040122272309186255\n",
            "train loss:0.0006271888357306973\n",
            "train loss:0.002143976932986441\n",
            "train loss:0.0005975491161552418\n",
            "train loss:0.0017872574238687905\n",
            "train loss:0.0012719127134471287\n",
            "train loss:0.001319294720991169\n",
            "train loss:0.003032459840140322\n",
            "train loss:0.0010118700668063614\n",
            "train loss:0.0035678888853731615\n",
            "train loss:0.0021157879884408475\n",
            "train loss:0.0003523634814181132\n",
            "train loss:0.001344850838948081\n",
            "train loss:0.0023377888593845895\n",
            "train loss:0.0004347800215676418\n",
            "train loss:0.002666865585329115\n",
            "train loss:0.0013071865756084258\n",
            "train loss:0.0012461786567319326\n",
            "train loss:0.005767411486737219\n",
            "train loss:0.0005544763029098537\n",
            "train loss:0.0006880734643660525\n",
            "train loss:0.0024041777499592422\n",
            "train loss:0.001209803497122518\n",
            "train loss:0.0017939384146605595\n",
            "train loss:4.382760432758631e-05\n",
            "train loss:0.0003074375180575162\n",
            "train loss:0.0004251115555420645\n",
            "train loss:0.0014133069146297756\n",
            "train loss:0.01195625279617924\n",
            "train loss:8.077585528935479e-05\n",
            "train loss:0.00039099749425236253\n",
            "train loss:0.0012884849835658274\n",
            "train loss:0.001486266268772498\n",
            "train loss:0.00324831086141987\n",
            "train loss:0.0006290990346108044\n",
            "train loss:0.0011861104240140138\n",
            "train loss:0.00031961400804764276\n",
            "train loss:7.586170333850447e-05\n",
            "train loss:0.00031479791501957233\n",
            "train loss:0.016358648658054063\n",
            "train loss:0.0006566481513392143\n",
            "train loss:0.001919304717956405\n",
            "train loss:0.0031631886973118866\n",
            "train loss:0.00037439459136698723\n",
            "train loss:0.00020563592998611426\n",
            "train loss:0.0033602369223783906\n",
            "train loss:0.0016465679172830746\n",
            "train loss:0.00019054124030157697\n",
            "train loss:0.0005485304860931184\n",
            "train loss:0.002287334091174955\n",
            "train loss:0.004614664592921779\n",
            "train loss:0.00024333045814540452\n",
            "train loss:0.0017365081350968442\n",
            "train loss:0.0008964660741237301\n",
            "train loss:0.0014845068202809428\n",
            "train loss:2.7549647893624218e-05\n",
            "train loss:0.004033937507216938\n",
            "train loss:0.0012698536446882791\n",
            "train loss:0.0006177430400882713\n",
            "train loss:0.0016498419740736867\n",
            "train loss:0.00255238777991741\n",
            "train loss:0.00010210477919770786\n",
            "train loss:0.00021250678655056993\n",
            "train loss:0.00032015612798233896\n",
            "train loss:0.0005077742438053662\n",
            "train loss:0.0011166568381919507\n",
            "train loss:0.0013954932735700213\n",
            "train loss:0.00011792320460926555\n",
            "train loss:0.0014741576830421166\n",
            "train loss:0.008256902088659465\n",
            "train loss:0.00012028139602024182\n",
            "train loss:0.03526875225429628\n",
            "train loss:0.0014092795428388648\n",
            "train loss:0.00296875734331269\n",
            "train loss:0.003673367335040788\n",
            "train loss:0.001586931456596214\n",
            "train loss:0.0005327195899467996\n",
            "train loss:0.0014280832764205292\n",
            "train loss:0.0014608146592022103\n",
            "train loss:0.0024879419900709103\n",
            "train loss:0.0012001639614992706\n",
            "train loss:0.0020164456481893537\n",
            "train loss:0.003158305796737357\n",
            "train loss:0.0032614372953725236\n",
            "train loss:0.0014127133930449875\n",
            "train loss:0.00014057283626245208\n",
            "train loss:0.0003045134773171289\n",
            "train loss:0.00019913529421326087\n",
            "train loss:0.0006566267159091344\n",
            "train loss:0.0017932574070775487\n",
            "train loss:0.0003349386394140404\n",
            "train loss:0.0002884280847712741\n",
            "train loss:3.735502731606008e-05\n",
            "train loss:0.0014207585675122985\n",
            "train loss:0.0017661958829782542\n",
            "train loss:0.0006651634766899658\n",
            "train loss:0.0006413763371392046\n",
            "train loss:0.00011058260303739005\n",
            "train loss:0.0001608924254844062\n",
            "train loss:0.03772082293274575\n",
            "train loss:0.007515278009379673\n",
            "train loss:0.004939689227359837\n",
            "train loss:5.2147504407288426e-05\n",
            "train loss:0.000930890597919775\n",
            "train loss:0.0005296405773259869\n",
            "train loss:0.0016186349115501308\n",
            "train loss:0.000823906284706202\n",
            "train loss:7.13900929389697e-05\n",
            "train loss:0.00011244461208962716\n",
            "train loss:0.0037075058745841493\n",
            "train loss:0.0004344081325230198\n",
            "train loss:0.0010595259412020806\n",
            "train loss:0.0004858249330807423\n",
            "train loss:0.000857527659038021\n",
            "train loss:0.0006589269532818859\n",
            "train loss:0.0014549969646716165\n",
            "train loss:0.0034798737458483853\n",
            "train loss:0.0017942594856851207\n",
            "train loss:0.0012576304249814127\n",
            "train loss:0.0001304145221120612\n",
            "train loss:0.0034463934805478635\n",
            "train loss:6.47381509181306e-05\n",
            "train loss:4.718495847401142e-05\n",
            "train loss:0.0007981587267044881\n",
            "train loss:0.0020006687543809194\n",
            "train loss:0.0003750744933412795\n",
            "train loss:0.0007916445747897901\n",
            "train loss:0.0020979331723023358\n",
            "train loss:0.00013881000183317137\n",
            "train loss:0.00253865551746222\n",
            "train loss:3.826197735136577e-05\n",
            "train loss:0.00014885559605645212\n",
            "train loss:0.0005651193690377676\n",
            "train loss:0.0011030325567472849\n",
            "train loss:0.0005059357323789416\n",
            "train loss:0.0034379636367457955\n",
            "train loss:0.001930818828014686\n",
            "train loss:0.00027250111244855267\n",
            "train loss:0.0013317217598395928\n",
            "train loss:0.0020194344852483104\n",
            "train loss:2.4020167685476863e-05\n",
            "train loss:0.00041559137285132684\n",
            "train loss:0.002225639638486377\n",
            "train loss:0.00046489190505653844\n",
            "train loss:0.0004262132820603542\n",
            "train loss:0.0014640689222475795\n",
            "train loss:0.0013717282055214198\n",
            "train loss:6.34950775728234e-05\n",
            "train loss:0.0016504649785852943\n",
            "train loss:3.297860694550971e-05\n",
            "train loss:0.0015593948393528357\n",
            "train loss:0.0011193807351951547\n",
            "train loss:0.00012838539677398675\n",
            "train loss:2.5419053996304723e-05\n",
            "train loss:0.00030649648187662493\n",
            "train loss:0.0010448791974931675\n",
            "train loss:4.623182135474602e-05\n",
            "train loss:0.0002473010281812385\n",
            "train loss:0.003168642172802893\n",
            "train loss:0.00015019014182145746\n",
            "train loss:0.0003618513698272391\n",
            "train loss:0.0005273893489835651\n",
            "train loss:0.0002957321379393075\n",
            "train loss:0.0034501584058394884\n",
            "train loss:9.68027532473988e-06\n",
            "train loss:0.0011788850032566959\n",
            "train loss:2.4585582156386187e-05\n",
            "train loss:0.0003091646509476149\n",
            "train loss:0.0001843960399705757\n",
            "train loss:0.00017787318482199047\n",
            "train loss:0.0007384819608227708\n",
            "train loss:0.0006498870071917148\n",
            "train loss:0.0013778800997074675\n",
            "train loss:0.0007804950726878848\n",
            "train loss:0.00017619600606572707\n",
            "train loss:0.00019862170513854665\n",
            "train loss:0.002004405263199303\n",
            "train loss:0.003117157437852548\n",
            "train loss:0.0006707054722511578\n",
            "train loss:0.0017603859216233481\n",
            "train loss:0.00765282378675916\n",
            "train loss:0.0035930011339442903\n",
            "train loss:0.00023116924753659662\n",
            "train loss:0.002236353653583678\n",
            "train loss:0.000340528083930703\n",
            "train loss:7.645430801326227e-05\n",
            "train loss:0.001419356445683815\n",
            "train loss:0.0009437825249131549\n",
            "train loss:0.0046918481979324215\n",
            "train loss:0.0003469509880712732\n",
            "train loss:0.0017725238430345916\n",
            "train loss:0.011238798327701248\n",
            "train loss:0.00032214645562816187\n",
            "train loss:0.0014212971491859433\n",
            "train loss:0.0006684059394989271\n",
            "train loss:0.0038037255406233477\n",
            "train loss:0.00087971781424593\n",
            "train loss:0.0018791987043488638\n",
            "train loss:0.0019638032723048055\n",
            "train loss:0.001490860280619919\n",
            "train loss:0.00023098186084813906\n",
            "train loss:0.018181778440244552\n",
            "train loss:0.0006397572651542691\n",
            "train loss:0.0012763712864593225\n",
            "train loss:0.0005975415940425445\n",
            "train loss:0.0010517272146323726\n",
            "train loss:0.0022365121000284425\n",
            "train loss:0.004407783794423672\n",
            "train loss:0.002988331641747523\n",
            "train loss:4.182759534194181e-05\n",
            "train loss:0.001557444201806745\n",
            "train loss:0.0005998471518136407\n",
            "train loss:0.0002556218434572722\n",
            "train loss:0.0054939689828333894\n",
            "train loss:0.0024749245871189766\n",
            "train loss:0.0013880021431527426\n",
            "train loss:0.003969707588508924\n",
            "train loss:0.0029179089476286803\n",
            "train loss:0.0013186251525836151\n",
            "train loss:0.0007885938055121922\n",
            "train loss:0.002615008271655289\n",
            "train loss:6.844833707766784e-06\n",
            "train loss:0.00015758297742666825\n",
            "train loss:0.000252972043106901\n",
            "train loss:0.00118279542280934\n",
            "train loss:0.004491343405769\n",
            "train loss:0.00018646610640125493\n",
            "train loss:0.006290406621615408\n",
            "train loss:0.00011902947351873188\n",
            "train loss:0.001857236758088472\n",
            "train loss:0.002878568972542538\n",
            "train loss:0.00038345775620508935\n",
            "train loss:0.0006655937982115583\n",
            "train loss:0.001395119162422324\n",
            "train loss:0.0006994855161021191\n",
            "train loss:7.559059867259266e-05\n",
            "train loss:0.0017135560539739963\n",
            "train loss:0.001028225991319298\n",
            "train loss:0.00023826052978453076\n",
            "train loss:5.173693661019759e-05\n",
            "train loss:0.0008869000622209862\n",
            "train loss:0.00023636962972899082\n",
            "train loss:0.0003125834574982439\n",
            "train loss:0.0003216291471982788\n",
            "train loss:0.0015259108044006593\n",
            "train loss:0.0011905129194547256\n",
            "train loss:0.00034536339517468275\n",
            "train loss:0.00026509485737209265\n",
            "train loss:9.165135031674872e-05\n",
            "train loss:0.00016061253169533742\n",
            "train loss:0.00028975591832827073\n",
            "train loss:0.00148193499156724\n",
            "train loss:0.00034998516598867504\n",
            "train loss:0.0013124734142255078\n",
            "train loss:0.0016924431295130967\n",
            "train loss:0.0007620972930309053\n",
            "train loss:0.0018880331000313624\n",
            "train loss:0.0009610892538568165\n",
            "train loss:0.0022808582363495512\n",
            "train loss:0.005871120020583562\n",
            "train loss:0.00012654433379596464\n",
            "train loss:0.00019991345013128413\n",
            "train loss:0.0007928898609026556\n",
            "train loss:0.001142750475587003\n",
            "train loss:0.0009323291345591333\n",
            "train loss:0.00020552648416295418\n",
            "train loss:0.0010945326189758737\n",
            "train loss:0.0005489599718655454\n",
            "train loss:0.00042532015014613763\n",
            "train loss:0.0007610195445652978\n",
            "train loss:0.00013778841549449787\n",
            "train loss:0.005569919178021498\n",
            "train loss:0.0012850632685046381\n",
            "train loss:0.00028858015497702105\n",
            "train loss:6.987030038528789e-05\n",
            "train loss:0.0002216218901623256\n",
            "train loss:0.0008696001808609444\n",
            "train loss:0.0018901108214970006\n",
            "train loss:0.0005288600664030907\n",
            "train loss:0.008225305432204831\n",
            "train loss:0.00010259367907888088\n",
            "train loss:0.0007191847836682484\n",
            "train loss:0.00031794150554654747\n",
            "train loss:0.0006805313626446535\n",
            "train loss:0.0005174977598986498\n",
            "train loss:0.0022237160049441555\n",
            "train loss:0.0008233925482469477\n",
            "train loss:0.0002748853928349596\n",
            "train loss:0.01208913456511499\n",
            "train loss:0.0004940820815402566\n",
            "train loss:0.0017140014429765397\n",
            "train loss:6.948109058876442e-05\n",
            "train loss:0.004708225660651597\n",
            "train loss:0.00037128785763269965\n",
            "train loss:0.0003369525826943984\n",
            "train loss:0.00020580457966916732\n",
            "train loss:0.006882906145522737\n",
            "train loss:0.0013751192387382305\n",
            "train loss:0.00044315778121086744\n",
            "train loss:1.5260187429905343e-05\n",
            "train loss:0.02696071079080943\n",
            "train loss:4.268056246266614e-05\n",
            "train loss:0.0013788546055940995\n",
            "train loss:0.0018560456352321193\n",
            "train loss:0.0008415280279170021\n",
            "train loss:0.004046930112146938\n",
            "train loss:0.0003570867706258828\n",
            "train loss:0.003002697795915905\n",
            "train loss:0.0007648365583328715\n",
            "train loss:0.0003160068758528598\n",
            "train loss:0.0026270389190927446\n",
            "train loss:0.0002193842619220179\n",
            "train loss:0.00013878937658126116\n",
            "train loss:0.00034094278468456993\n",
            "train loss:0.00010243936299233263\n",
            "train loss:0.0007398430977404457\n",
            "train loss:0.002119019003611439\n",
            "train loss:0.0019931911771914517\n",
            "train loss:0.0003656207964669619\n",
            "train loss:0.0006101331354675198\n",
            "train loss:0.0029208009948393683\n",
            "train loss:0.0016644004434321427\n",
            "train loss:0.002033433763302802\n",
            "train loss:0.00035977444400884617\n",
            "train loss:0.0002416328629848664\n",
            "train loss:7.820274054607597e-05\n",
            "train loss:0.0002084599595090298\n",
            "train loss:0.00019041093642551408\n",
            "train loss:0.0007203261976835489\n",
            "train loss:0.002400140386773248\n",
            "train loss:0.0020019922037231865\n",
            "train loss:0.00013735515113865856\n",
            "train loss:0.0006198147141310026\n",
            "train loss:0.00037453568452609754\n",
            "train loss:2.977867495590166e-05\n",
            "train loss:0.0003795184364416973\n",
            "train loss:0.0016573612075171842\n",
            "train loss:9.489034905009279e-05\n",
            "train loss:4.8236932843363255e-05\n",
            "train loss:0.0004736465566509019\n",
            "train loss:0.0001332729545625733\n",
            "train loss:0.000479696601943701\n",
            "train loss:0.0002750733112670299\n",
            "train loss:0.0002001386421253067\n",
            "train loss:0.000855225252585305\n",
            "train loss:0.0003425157061943478\n",
            "train loss:0.00044115074971190564\n",
            "train loss:9.320550261601569e-06\n",
            "train loss:0.008381684304861284\n",
            "train loss:0.0025238464635277836\n",
            "train loss:0.00045766979362053345\n",
            "train loss:0.00026242758707166773\n",
            "train loss:0.0012711563361572098\n",
            "train loss:0.0004221721480967139\n",
            "train loss:0.0004649508817230244\n",
            "train loss:0.0008014900167092441\n",
            "train loss:0.0023018117950095634\n",
            "train loss:4.5103140109467395e-05\n",
            "train loss:0.001298517761803067\n",
            "train loss:0.0013230707696554093\n",
            "train loss:0.000644604674543782\n",
            "train loss:0.0021329635061789233\n",
            "train loss:0.0005455024675915629\n",
            "train loss:1.2919166116510986e-05\n",
            "train loss:5.5887817482291816e-05\n",
            "train loss:0.00018242199347282315\n",
            "train loss:5.834109119505174e-05\n",
            "train loss:0.0029598157504438886\n",
            "train loss:8.361657661753518e-05\n",
            "train loss:2.9536678373180966e-05\n",
            "train loss:0.0003999355835507353\n",
            "train loss:4.932013582700452e-05\n",
            "train loss:0.0013610986912302291\n",
            "train loss:0.0032990089925768475\n",
            "train loss:0.00034882749850897424\n",
            "train loss:0.00021624601245364477\n",
            "train loss:0.000994440993226489\n",
            "train loss:0.0002518562150595748\n",
            "train loss:0.00043887159373195235\n",
            "train loss:0.00021937678917086415\n",
            "train loss:0.0023036195912116424\n",
            "train loss:9.4508056084886e-05\n",
            "train loss:8.108647012687915e-05\n",
            "train loss:0.00214356305874124\n",
            "train loss:0.00010704804071317318\n",
            "train loss:0.0005312308442650365\n",
            "train loss:0.0007345719500396455\n",
            "train loss:0.0002494306079678597\n",
            "train loss:0.0006656992982465334\n",
            "train loss:5.690737529360403e-05\n",
            "train loss:0.0007974352791476254\n",
            "train loss:0.00019870996077319376\n",
            "train loss:0.0032583030712140513\n",
            "train loss:0.0029324876173509717\n",
            "train loss:0.00011433021467158178\n",
            "train loss:4.388769463042225e-05\n",
            "train loss:3.533225648474886e-05\n",
            "train loss:0.0016770953979133437\n",
            "train loss:0.00045713700137510097\n",
            "train loss:1.1583253909312526e-05\n",
            "train loss:0.00013024286530154171\n",
            "train loss:0.0005091358702372098\n",
            "train loss:4.956425229794214e-05\n",
            "train loss:0.0022217705938650874\n",
            "train loss:0.0009643791673287084\n",
            "train loss:0.0002881775755965082\n",
            "train loss:0.0007690282918924956\n",
            "train loss:0.0002760332394647868\n",
            "train loss:0.00030199314053326056\n",
            "train loss:0.0028209103791268686\n",
            "train loss:0.00037856377882190107\n",
            "train loss:8.500289861666762e-05\n",
            "train loss:5.103389216610564e-05\n",
            "train loss:0.0006402945798968815\n",
            "train loss:0.0011759392497748427\n",
            "train loss:3.220924381889226e-05\n",
            "train loss:0.0016434330252468646\n",
            "train loss:0.0012083611729347885\n",
            "train loss:0.013529804614520994\n",
            "train loss:0.001683351491804994\n",
            "train loss:4.260796983644521e-05\n",
            "train loss:0.00037521736572850657\n",
            "train loss:0.0006230700013928999\n",
            "train loss:0.0012441094742405893\n",
            "train loss:9.055399197550084e-05\n",
            "train loss:0.0006245053648906404\n",
            "train loss:0.0035358356078022757\n",
            "train loss:0.0004206399223966243\n",
            "train loss:0.0001820290542721858\n",
            "train loss:0.0008663583165210806\n",
            "train loss:5.6044308138809154e-05\n",
            "train loss:0.0012598564475226478\n",
            "train loss:9.371327309593394e-05\n",
            "train loss:0.00023851567692503826\n",
            "train loss:0.004955838196437521\n",
            "train loss:0.0008815701395232901\n",
            "train loss:0.0005124645720910532\n",
            "train loss:6.278650189869703e-05\n",
            "train loss:0.0009659803837195815\n",
            "train loss:0.000102844500057838\n",
            "train loss:5.57520682974355e-05\n",
            "train loss:0.0010036323558215057\n",
            "train loss:0.0004765106157343482\n",
            "train loss:0.0012410049536498537\n",
            "train loss:0.0002944122189261098\n",
            "train loss:0.0002724041312821933\n",
            "train loss:0.0008149640020833238\n",
            "train loss:0.0005794520057986547\n",
            "train loss:0.0006049633067217442\n",
            "train loss:5.508676370676265e-05\n",
            "train loss:0.0026433345252339924\n",
            "train loss:0.0001756925082126877\n",
            "train loss:0.0006394892046899931\n",
            "train loss:0.0005382641794975305\n",
            "train loss:0.00044953713148226666\n",
            "train loss:7.894928765831673e-05\n",
            "train loss:0.00012893162763355753\n",
            "train loss:0.0004697822271500494\n",
            "train loss:0.0003742950860945269\n",
            "train loss:4.584602055341164e-05\n",
            "train loss:0.0013890174651760053\n",
            "train loss:0.0009471995365340834\n",
            "train loss:0.00034982567757207164\n",
            "train loss:0.002061670285367157\n",
            "train loss:0.0002762653881877045\n",
            "train loss:0.010821549893662503\n",
            "train loss:0.0016066696410690264\n",
            "train loss:0.003537016901109821\n",
            "train loss:0.0003518788982630519\n",
            "train loss:0.00014559699240284562\n",
            "train loss:3.0240968891811554e-05\n",
            "train loss:0.00024273703543194005\n",
            "train loss:0.0019719499048148187\n",
            "train loss:0.0005546361987918574\n",
            "train loss:2.83416195680516e-05\n",
            "train loss:7.348734676287949e-05\n",
            "train loss:0.028160065721534663\n",
            "train loss:0.0013633475489738596\n",
            "train loss:0.0026660361510058793\n",
            "train loss:0.0003993239003003274\n",
            "train loss:0.0007120491602157629\n",
            "train loss:9.358762446839648e-05\n",
            "train loss:0.0005754256339672262\n",
            "train loss:0.0007787057166950187\n",
            "train loss:0.0011166095344177789\n",
            "train loss:0.000774593376026635\n",
            "train loss:0.0011656811257137038\n",
            "train loss:0.0033079330739136724\n",
            "train loss:0.0026137095660533198\n",
            "train loss:0.0002815196162774543\n",
            "train loss:0.00011432912381436578\n",
            "train loss:0.0038149986187308808\n",
            "train loss:0.0023614090598592496\n",
            "train loss:0.0006725230448168089\n",
            "train loss:0.0006029011601833757\n",
            "train loss:0.0005170317229947131\n",
            "train loss:0.0017266422115378192\n",
            "train loss:0.000191711166612533\n",
            "train loss:2.9063646503758632e-05\n",
            "train loss:0.003226084170974281\n",
            "train loss:0.002107363507265305\n",
            "train loss:0.0003097715082798432\n",
            "train loss:0.0017779881254872132\n",
            "train loss:0.0002979875449802992\n",
            "train loss:0.000333014499371862\n",
            "train loss:0.00040927731750062084\n",
            "train loss:0.0015806499253592289\n",
            "train loss:0.0030696741582234943\n",
            "train loss:0.0023692604942950777\n",
            "train loss:0.0006410931367767541\n",
            "train loss:0.0025170100542404434\n",
            "train loss:0.003345993057665203\n",
            "train loss:0.00017759856128959944\n",
            "train loss:0.0015490936709193813\n",
            "train loss:0.0069416495721102076\n",
            "train loss:0.0020132068959808225\n",
            "train loss:0.0006818586134842107\n",
            "train loss:0.00029461688343137996\n",
            "train loss:1.2203245690055372e-05\n",
            "train loss:0.004205193506764058\n",
            "train loss:0.00024795958576882663\n",
            "train loss:8.851779964850104e-06\n",
            "train loss:0.0359165446560366\n",
            "train loss:0.0026043326977457197\n",
            "train loss:0.0003812535723338783\n",
            "train loss:0.0035964099590168313\n",
            "train loss:0.00017173465664904579\n",
            "train loss:0.000557542802966091\n",
            "train loss:0.00015450036378650349\n",
            "train loss:0.004720120459329686\n",
            "train loss:0.0029616364284932254\n",
            "train loss:9.081467031966717e-05\n",
            "train loss:9.544824270262053e-05\n",
            "train loss:0.0008361954614304211\n",
            "train loss:0.0023905078050711474\n",
            "train loss:0.0074020910836894915\n",
            "train loss:0.0028251000865048485\n",
            "train loss:0.00015839299625957357\n",
            "train loss:0.005817259878170439\n",
            "train loss:0.0008419512647764182\n",
            "train loss:0.000391197950639267\n",
            "train loss:0.00045075627540941627\n",
            "train loss:2.1178780305591595e-05\n",
            "train loss:0.006632178337001724\n",
            "train loss:0.0017785817184203436\n",
            "train loss:0.0026230838038068405\n",
            "train loss:0.0013655917187387042\n",
            "train loss:0.0008146234088305015\n",
            "train loss:0.0035330944579381666\n",
            "train loss:0.00013026988904404525\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9892\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1시간 26분 걸렸습니다..."
      ],
      "metadata": {
        "id": "pDFg7jy9SGK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
